var documenterSearchIndex = {"docs":
[{"location":"generated/demos/12/outlier1/#outlier1","page":"RMT and outliers","title":"RMT and outliers","text":"This example examines the effects of outliers on SVD performance for estimating a low-rank matrix from noisy data, from the perspective of random matrix theory, using the Julia language.\n\nThis page comes from a single Julia file: outlier1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: outlier1.ipynb, or open it in binder here: outlier1.ipynb.\n\nAdd the Julia packages that are need for this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages for this example. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, norm, rank, svd, svdvals\nusing MIRTjim: jim, prompt\nusing Plots.PlotMeasures: px\nusing Plots: default, gui, plot, plot!, scatter!, savefig\nusing Random: seed!\nusing StatsBase: mean\ndefault(markerstrokecolor=:auto, label=\"\", widen=true, markersize = 6,\n labelfontsize = 24, legendfontsize = 18, tickfontsize = 14, linewidth = 3,\n)\nseed!(0)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/12/outlier1/#Image-example","page":"RMT and outliers","title":"Image example","text":"Apply an SVD-based low-rank approximation approach to some data with outliers.","category":"section"},{"location":"generated/demos/12/outlier1/#Latent-matrix","page":"RMT and outliers","title":"Latent matrix","text":"Make a matrix that has low rank:\n\ntmp = [\n    zeros(1,20);\n    0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0;\n    0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0;\n    0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0;\n    0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0;\n    zeros(1,20)\n]';\nrank(tmp)\n\nTurn it into an image:\n\nXtrue = kron(1 .+ 8*tmp, ones(9,9))\nrtrue = rank(Xtrue)\n\nplots with consistent size\n\njim1 = (X ; kwargs...) -> jim(X; size = (700,300),\n leftmargin = 10px, rightmargin = 10px, kwargs...);\nnothing #hide\n\nand consistent display range\n\njimc = (X ; kwargs...) -> jim1(X; clim=(0,9), kwargs...);\nnothing #hide\n\nand with NRMSE label\n\nnrmse = (Xh) -> round(norm(Xh - Xtrue) / norm(Xtrue) * 100, digits=1)\nargs = (xaxis = false, yaxis = false, colorbar = :none) # book\nargs = (;) # web\njime = (X; kwargs...) -> jimc(X; xlabel = \"NRMSE = $(nrmse(X)) %\",\n args..., kwargs...,\n)\nbm = s -> \"\\\\mathbf{\\\\mathit{$s}}\"\ntitle = latexstring(\"\\$$(bm(:X))\\$ : Latent image\")\npt = jimc(Xtrue; title, xlabel = \" \", args...)","category":"section"},{"location":"generated/demos/12/outlier1/#Helper-functions","page":"RMT and outliers","title":"Helper functions","text":"Bernoulli outliers with magnitude Ï„ and probability p:\n\nfunction outliers(dims::Dims, Ï„::Real = 6, p::Real = 0.05)\n    Z = Ï„ * sign.(randn(dims)) .* (rand(dims...) .< p)\n    return Z\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/12/outlier1/#Noisy-data","page":"RMT and outliers","title":"Noisy data","text":"seed!(0)\n(M, N) = size(Xtrue)\nZ = outliers((M,N))\nY = Xtrue + Z\n\ntitle = latexstring(\"\\$$(bm(:Y))\\$ : Corrupted image matrix\\n(with outliers)\")\npy = jime(Y ; title)","category":"section"},{"location":"generated/demos/12/outlier1/#Singular-values","page":"RMT and outliers","title":"Singular values","text":"The first 3 singular values of Y are well above the \"noise floor\" caused by outliers.\n\nBut Ïƒâ‚„(X) is just barely above the threshold, and Ïƒâ‚…(X) is below the threshold, so we cannot expect a simple SVD approach to recover them well.\n\nps1 = plot(\n title = \"Singular values\",\n xaxis = (L\"k\", (1, N), [1, 3, 6, N]),\n yaxis = (L\"Ïƒ_k\",),\n leftmargin = 15px, bottommargin = 20px, size = (600,350), widen = true,\n)\nsv_x = svdvals(Xtrue)\nsv_y = svdvals(Y)\nscatter!(sv_y, color=:red, label=\"Y (data)\", marker=:dtriangle)\nscatter!(sv_x, color=:blue, label=\"Xtrue\", marker=:utriangle)\n\nprompt()","category":"section"},{"location":"generated/demos/12/outlier1/#Low-rank-estimate","page":"RMT and outliers","title":"Low-rank estimate","text":"A simple low-rank estimate of X from the first few SVD components of Y works just so-so here. A simple SVD approach recovers the first 3 components well, but cannot estimate the 4th and 5th components.\n\nr = 5\nU,s,V = svd(Y)\nXr = U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of data \\$$(bm(:Y))\\$\")\npr = jime(Xr ; title)\n\nExamine singular vector estimates. The first 3 are quite good; the next two are poor.\n\nsv1 = [\n sum(svd(Xr).U[:,1:r] .* svd(Xtrue).U[:,1:r], dims=1).^2\n sum(svd(Xr).V[:,1:r] .* svd(Xtrue).V[:,1:r], dims=1).^2\n]","category":"section"},{"location":"generated/demos/12/outlier1/#Non-iterative-\"robust\"-PCA","page":"RMT and outliers","title":"Non-iterative \"robust\" PCA","text":"Try simple outlier removal method. Look at the residual between hatX and Y:\n\nresidual = Xr - Y\n\npd = jim1(residual; clim = (-1,1) .* 7, cticks = (-1:1:1) * 8,\n title = latexstring(\"Residual \\$$(bm(:Y)) - \\\\hat{$(bm(:X))}\\$\"),\n)\n\nIdentify \"bad\" pixels with large residual errors\n\nbadpixel = @. abs(residual) > 3\njim1(badpixel)\n\nReplace \"bad\" pixels with typical image values\n\nYmod = copy(Y)\nYmod[badpixel] .= mean(Y[.!badpixel])\njime(Ymod) # already reduces NRMSE by a lot compared to Y itself!\n\nExamine singular values of modified Y. The noise floor is lower.\n\nps2 = plot(\n title = \"Singular values\",\n xaxis = (L\"k\", (1, N), [1, 3, 6, N]),\n yaxis = (L\"Ïƒ_k\",),\n leftmargin = 15px, bottommargin = 20px, size = (600,350), widen = true,\n)\nsv_f = svdvals(Ymod)\nscatter!(sv_f, color=:green, label=\"Y (modified)\", marker=:hex)\nscatter!(sv_x, color=:blue, label=\"Xtrue\", marker=:utriangle)\n\nprompt()\n\nApplying low-rank matrix approximation to modified Y leads to lower NRMSE.\n\nUm,sm,Vm = svd(Ymod)\nXh = Um[:,1:r] * Diagonal(sm[1:r]) * Vm[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of modified data \\$$(bm(:Y))\\$\")\nph = jime(Xh ; title)\n\nAll of the singular components are better recovered, including the ones that were near or below the noise threshold.\n\nsv2 = [\n sum(svd(Xh).U[:,1:r] .* svd(Xtrue).U[:,1:r], dims=1).^2\n sum(svd(Xh).V[:,1:r] .* svd(Xtrue).V[:,1:r], dims=1).^2\n]\n\nSummary\n\npa = jim(stack((Xtrue, abs.(Z), Y, Xr, 6*badpixel, Xh));\n ncol=1, size=(600, 900), clim=(0,9))","category":"section"},{"location":"generated/demos/12/outlier1/#More-outliers","page":"RMT and outliers","title":"More outliers","text":"Now examine a case where the outliers are stronger and more prevalent.\n\npout2 = 0.1\nZ = outliers((M,N), 50, pout2)\nY = Xtrue + Z\n\ntitle = latexstring(\"\\$$(bm(:Y))\\$ : Corrupted image matrix\\n(with $(100*pout2)% outliers)\")\npy2 = jime(Y ; title)","category":"section"},{"location":"generated/demos/12/outlier1/#Singular-values-2","page":"RMT and outliers","title":"Singular values","text":"Now all of the singular values of X are below the \"noise floor\" caused by outliers.\n\nps3 = plot(\n title = \"Singular values\",\n xaxis = (L\"k\", (1, N), [1, 3, 6, N]),\n yaxis = (L\"Ïƒ_k\",),\n leftmargin = 15px, bottommargin = 20px, size = (600,350), widen = true,\n)\nsv_x = svdvals(Xtrue)\nsv_y = svdvals(Y)\nscatter!(sv_y, color=:red, label=\"Y (data)\", marker=:dtriangle)\nscatter!(sv_x, color=:blue, label=\"Xtrue\", marker=:utriangle)\n\nprompt()","category":"section"},{"location":"generated/demos/12/outlier1/#Low-rank-estimate-2","page":"RMT and outliers","title":"Low-rank estimate","text":"A simple low-rank estimate of X from the first few SVD components of Y does not work at all now for such heavily corrupted data.\n\nr = 5\nU,s,V = svd(Y)\nXr = U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of data \\$$(bm(:Y))\\$\")\npr2 = jime(Xr ; title)\n\nExamine singular vector estimates. The first one is so-so, the rest are useless.\n\nsv3 = [\n sum(svd(Xr).U[:,1:r] .* svd(Xtrue).U[:,1:r], dims=1).^2\n sum(svd(Xr).V[:,1:r] .* svd(Xtrue).V[:,1:r], dims=1).^2\n]","category":"section"},{"location":"generated/demos/12/outlier1/#Non-iterative-\"robust\"-PCA-2","page":"RMT and outliers","title":"Non-iterative \"robust\" PCA","text":"Try simple outlier removal method. Look at the residual between hatX and Y:\n\nresidual = Xr - Y\n\npd2 = jim1(residual; clim = (-1,1) .* 70, cticks = (-1:1:1) * 8,\n title = latexstring(\"Residual \\$$(bm(:Y)) - \\\\hat{$(bm(:X))}\\$\"),\n)\n\nIdentify \"bad\" pixels with large residual errors. This is a nonlinear operation:\n\nbadpixel = @. abs(residual) > 10\njim1(badpixel)\n\nReplace \"bad\" pixels with typical image values\n\nYmod = copy(Y)\nYmod[badpixel] .= mean(Y[.!badpixel])\njime(Ymod) # already reduces NRMSE by a lot compared to Y itself!\n\nExamine singular values of modified Y. The noise floor is lower.\n\nps4 = plot(\n title = \"Singular values\",\n xaxis = (L\"k\", (1, N), [1, 3, 6, N]),\n yaxis = (L\"Ïƒ_k\",),\n leftmargin = 15px, bottommargin = 20px, size = (600,350), widen = true,\n)\nsv_f = svdvals(Ymod)\nscatter!(sv_f, color=:green, label=\"Y (modified)\", marker=:hex)\nscatter!(sv_x, color=:blue, label=\"Xtrue\", marker=:utriangle)\n\nprompt()\n\nApplying low-rank matrix approximation to modified Y leads to lower NRMSE.\n\nUm,sm,Vm = svd(Ymod)\nXh = Um[:,1:r] * Diagonal(sm[1:r]) * Vm[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of modified data \\$$(bm(:Y))\\$\")\nph2 = jime(Xh ; title)\n\nNow the first three singular components are better recovered.\n\nsv4 = [\n sum(svd(Xh).U[:,1:r] .* svd(Xtrue).U[:,1:r], dims=1).^2\n sum(svd(Xh).V[:,1:r] .* svd(Xtrue).V[:,1:r], dims=1).^2\n]\n\nLet's try iterating to see if we can refine it. Indeed it does refine it, but at this point it starts to become an ad hoc iterative method. If we going to iterate, then it seems preferable to use a cost function like the one used in robust PCA, with a proper optimization algorithm.\n\nresidual = Xh - Y\nbadpixel = @. abs(residual) > 10\njim1(badpixel)\nYmod = copy(Y)\nYmod[badpixel] .= mean(Y[.!badpixel])\nUm,sm,Vm = svd(Ymod)\nXh3 = Um[:,1:r] * Diagonal(sm[1:r]) * Vm[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of modified data \\$$(bm(:Y))\\$\")\nph3 = jime(Xh3 ; title)","category":"section"},{"location":"generated/demos/12/outlier1/#Reproducibility","page":"RMT and outliers","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/09/class01/#class01","page":"Binary classification","title":"Binary classification","text":"Binary classification of hand-written digits in Julia.\n\nThis page comes from a single Julia file: class01.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: class01.ipynb, or open it in binder here: class01.ipynb.","category":"section"},{"location":"generated/demos/09/class01/#Setup","page":"Binary classification","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"MLDatasets\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings # nice plot labels\nusing LinearAlgebra: dot\nusing MIRTjim: jim, prompt\nusing MLDatasets: MNIST\nusing Plots: default, gui, savefig\nusing Plots: histogram, histogram!, plot\nusing Plots: RGB, cgrad\nusing Plots.PlotMeasures: px\nusing Random: seed!, randperm\nusing StatsBase: mean\ndefault(); default(markersize=5, markerstrokecolor=:auto, label=\"\",\n tickfontsize=14, labelfontsize=18, legendfontsize=18, titlefontsize=18)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/demos/09/class01/#Load-data","page":"Binary classification","title":"Load data","text":"Read the MNIST data for some handwritten digits. This code will automatically download the data from web if needed and put it in a folder like: ~/.julia/datadeps/MNIST/.\n\nif !@isdefined(data)\n    digitn = (0, 1) # which digits to use\n    isinteractive() || (ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true) # avoid prompt\n    dataset = MNIST(Float32, :train)\n    nrep = 100 # how many of each digit\n    # function to extract the 1st `nrep` examples of digit n:\n    data = n -> dataset.features[:,:,findall(==(n), dataset.targets)[1:nrep]]\n    data = cat(dims=4, data.(digitn)...)\n    labels = vcat([fill(d, nrep) for d in digitn]...) # to check later\n    nx, ny, nrep, ndigit = size(data)\n    data = data[:,2:ny,:,:] # make images non-square to force debug\n    ny = size(data,2)\n    data = reshape(data, nx, ny, :)\n    seed!(0)\n    tmp = randperm(nrep * ndigit)\n    data = data[:,:,tmp]\n    labels = labels[tmp]\n    size(data) # (nx, ny, nrep*ndigit)\nend\n\nLook at \"unlabeled\" image data\n\npd = jim(data, \"Data\"; size=(600,300), tickfontsize=8,)\n\nExtract training data\n\ndata0 = data[:,:,labels .== 0]\ndata1 = data[:,:,labels .== 1];\n\npd0 = jim(data0[:,:,1:36]; nrow=6, colorbar=nothing, size=(400,400))\npd1 = jim(data1[:,:,1:36]; nrow=6, colorbar=nothing, size=(400,400))\n# savefig(pd0, \"class01-0.pdf\")\n# savefig(pd1, \"class01-1.pdf\")\n\nred-black-blue colorbar:\n\nRGB255(args...) = RGB((args ./ 255)...)\ncolor = cgrad([RGB255(230, 80, 65), :black, RGB255(23, 120, 232)]);\nnothing #hide","category":"section"},{"location":"generated/demos/09/class01/#Weights","page":"Binary classification","title":"Weights","text":"Compute sample average of each training class and define classifier weights as differences of the means.\n\nÎ¼0 = mean(data0, dims=3)\nÎ¼1 = mean(data1, dims=3)\nw = Î¼1 - Î¼0; # hand-crafted weights\nnothing #hide\n\nimages of means and weights\n\nsiz = (540,400)\nargs = (xaxis = false, yaxis = false) # book\np0 = jim(Î¼0; clim=(0,1), size=siz, cticks=[0,1], args...)\np1 = jim(Î¼1; clim=(0,1), size=siz, cticks=[0,1], args...)\npw = jim(w; color, clim=(-1,1).*0.8, size=siz, cticks=(-1:1)*0.75, args...)\npm = plot( p0, p1, pw;\n  size = (1400, 350),\n  layout = (1,3),\n  rightmargin = 20px,\n)\n# savefig(p0, \"class01-0.pdf\")\n# savefig(p1, \"class01-1.pdf\")\n# savefig(pw, \"class01-w.pdf\")\n# savefig(pm, \"class01-mean.pdf\")","category":"section"},{"location":"generated/demos/09/class01/#Inner-products","page":"Binary classification","title":"Inner products","text":"Examine performance of simple linear classifier. (Should be done with test data, not training data...)\n\ni0 = [dot(w, x) for x in eachslice(data0, dims=3)]\ni1 = [dot(w, x) for x in eachslice(data1, dims=3)];\n\nbins = -80:20\nph = plot(\n xaxis = (L\"âŸ¨\\mathbf{\\mathit{v}},\\mathbf{\\mathit{x}}âŸ©\", (-80, 20), -80:20:20),\n yaxis = (\"\", (0, 25), 0:10:20),\n size = (600, 250), bottommargin = 20px,\n)\nhistogram!(i0; bins, color=:red , label=\"0\")\nhistogram!(i1; bins, color=:blue, label=\"1\")\n\n# savefig(ph, \"class01-hist.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/09/class01/#Reproducibility","page":"Binary classification","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/02/gauss2d/#gauss2d","page":"2d heatmap","title":"2d heatmap","text":"This example illustrates matrix operations by making a 2D Gaussian plot and computing area under a curve and volume under a surface using the Julia language.\n\n2017-09-07, Jeff Fessler, University of Michigan\n2023-06-06 Julia 1.9.0\n\nThis page comes from a single Julia file: gauss2d.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: gauss2d.ipynb, or open it in binder here: gauss2d.ipynb.","category":"section"},{"location":"generated/demos/02/gauss2d/#Setup","page":"2d heatmap","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing MIRTjim: jim\nusing Plots: default, heatmap, savefig\ndefault(labelfontsize=18, tickfontsize=12, titlefontsize=18)","category":"section"},{"location":"generated/demos/02/gauss2d/#Broadcast","page":"2d heatmap","title":"Broadcast","text":"x = range(-2, 2, 101)\ny = range(-1.1, 1.1, 103) # deliberately non-square\nA = abs2.(x) .+ 30 * abs2.(y)' # a lot is happening here!\nF = exp.(-A)\np1 = heatmap(x, y, F', color=:grays, aspect_ratio=:equal)","category":"section"},{"location":"generated/demos/02/gauss2d/#Heatmap","page":"2d heatmap","title":"Heatmap","text":"Here is a fancy Julia way, now with labels:\n\np2 = heatmap(range(-2, 2, 101), range(-1.1, 1.1, 103),\n    (x,y) -> exp(-(abs2(x) + 3*abs2(y))), color=:grays, clim=(0,1),\n    aspect_ratio=:equal, xlabel=L\"x\", ylabel=L\"y\", title=L\"f(x,y)\")","category":"section"},{"location":"generated/demos/02/gauss2d/#jim","page":"2d heatmap","title":"jim","text":"The jim function from MIRTjim.jl has natural defaults.\n\np3 = jim(x, y, F; xlabel=L\"x\", ylabel=L\"y\", title=L\"f(x,y)\", clim=(0,1))\n# savefig(p3, \"plot_exp4.pdf\")","category":"section"},{"location":"generated/demos/02/gauss2d/#Area","page":"2d heatmap","title":"Area","text":"Compute 1D integral int_0^3 x^2  mathrmdx numerically.\n\nf(x) = x^2 # parabola\nx = range(0,3,2000) # sample points\nw = diff(x) # \"widths\" of rectangles\nArea = w' * f.(x[2:end])","category":"section"},{"location":"generated/demos/02/gauss2d/#Volume","page":"2d heatmap","title":"Volume","text":"2D integral int_0^3 int_0^2 exp(-x^2 - 3 y^2)  mathrmdx  mathrmdy\n\nf(x,y) = exp(-(x^2 + 3*y^2)) # gaussian bump function\nx = range(0,3,2000) # sample points\ny = range(0,2,1000) # sample points\nw = diff(x) # \"widths\" of rectangles in x\nu = diff(y) # \"widths\" of rectangles in y\nF = f.(x[2:end], y[2:end]') # automatic broadcasting again!\nVolume = w' * F * u","category":"section"},{"location":"generated/demos/02/gauss2d/#Reproducibility","page":"2d heatmap","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/05/sat-regress/#sat-regress","page":"Linear regression and SAT scores","title":"Linear regression and SAT scores","text":"This example is just a way to provide some template code for a homework problem on linear regression.\n\nThe data here comes from the 2007 paper by L. M. Lesser titled Critical Values and Transforming Data: Teaching Statistics with Social Justice and is based on data collected by the College Board, the organization that runs the SAT exam for high-school students. This data includes average SAT Math scores for 10 different family annual income brackets. The homework problem uses this data to explore the relationship between income and SAT scores via linear regression.\n\nThis page comes from a single Julia file: sat-regress.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: sat-regress.ipynb, or open it in binder here: sat-regress.ipynb.","category":"section"},{"location":"generated/demos/05/sat-regress/#Setup","page":"Linear regression and SAT scores","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing Plots: default, gui, scatter, savefig\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true, linewidth=2,\n markersize = 6, tickfontsize=14, labelfontsize = 18, legendfontsize=16)","category":"section"},{"location":"generated/demos/05/sat-regress/#Data","page":"Linear regression and SAT scores","title":"Data","text":"Normally we would read such data from a data file such as a .csv file using CSV.jl. This data is small enough to just paste here directly.\n\ndata = [\n \"Income Bracket (in \\$1000s)\" \"0 â€“ 10\" \"10 â€“ 20\" \"20 â€“ 30\" \"30 â€“ 40\" \"40 â€“ 50\" \"50 â€“ 60\" \"60 â€“ 70\" \"70 â€“ 80\" \"80 â€“ 100\" \"100+\"\n \"Math\" 457 465 474 488 501 509 515 521 534 564\n \"Verbal\" 429 445 462 478 493 500 505 511 523 549\n \"Writing\" 427 440 454 470 483 490 496 502 514 543\n];\n\nmath = Int.(data[2,2:end]) # math scores\n\nincome = [5:10:75; 90; 120] # middle of each range\n\nscatter(income, math; label=\"Data\", legend = :bottomright,\n xaxis = (\"Family Annual Income (1000\\$)\",),\n yaxis = (\"SAT Average Math Score\", (425,575), 425:50:575),\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/10/lrmc-m/#lrmc-m","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Low-rank matrix completion: AltMin, ISTA, FISTA","text":"This example illustrates low-rank matrix completion via alternating projection, ISTA (PGM), and FISTA (FPGM), using the Julia language.\n\n(This approach is related to \"projection onto convex sets\" (POCS) methods, but the term \"POCS\" would be a misnomer here because the rank constraint is not a convex set.)\n\nHistory:\n\n2021-08-23 Julia 1.6.2\n2021-12-09 Julia 1.6.4 and use M not Î©\n2023-06-04 Julia 1.9.0 in Literate\n\nThis page comes from a single Julia file: lrmc-m.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: lrmc-m.ipynb, or open it in binder here: lrmc-m.ipynb.","category":"section"},{"location":"generated/demos/10/lrmc-m/#Setup","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"Statistics\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LinearAlgebra: svd, svdvals, rank, norm, Diagonal\nusing LaTeXStrings\nusing MIRTjim: jim, prompt\nusing Plots: default, gui, plot, savefig, scatter, scatter!, xlabel!, xticks!\nusing Plots: RGB, cgrad\nusing Plots.PlotMeasures: px\nusing Random: seed!\nusing Statistics: mean\ndefault(\n markersize=7, markerstrokecolor=:auto, label = \"\",\n tickfontsize = 10, legendfontsize = 18, labelfontsize = 16, titlefontsize = 18,\n)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\njim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/demos/10/lrmc-m/#Latent-matrix","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Latent matrix","text":"Make a matrix that has low rank\n\ntmp = [\n    zeros(1,20);\n    0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0;\n    0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0;\n    0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0;\n    0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0;\n    zeros(1,20)\n]';\nrank(tmp)\n\nXtrue = kron(10 .+ 80*tmp, ones(9,9))\nrtrue = rank(Xtrue)\n\nHelper functions for plots with consistent size\n\njim1 = (X ; kwargs...) -> jim(X; size = (600,300),\n leftmargin = 10px, rightmargin = 10px, kwargs...);\nnothing #hide\n\nand consistent display range\n\njimc = (X ; kwargs...) -> jim1(X; clim=(0,100), kwargs...);\nnothing #hide\n\nand with NRMSE label\n\nnrmse = (Xh) -> round(norm(Xh - Xtrue) / norm(Xtrue) * 100, digits=1)\nargs = (xaxis = false, yaxis = false, colorbar = :none) # book\nargs = (;) # web\njime = (X; kwargs...) -> jimc(X; xlabel = \"NRMSE = $(nrmse(X)) %\",\n args..., kwargs...,\n)\ntitle = latexstring(\"\\$\\\\mathbf{\\\\mathit{X}}\\$ : Latent image\")\npt = jimc(Xtrue; title, xlabel = \" \", args...)\n# savefig(pt, \"mc_ap_x.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#Noisy-/-incomplete-data","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Noisy / incomplete data","text":"seed!(0)\nM = rand(Float32, size(Xtrue)) .>= 0.75 # 75% missing\nY = M .* (Xtrue + randn(size(Xtrue)));\n\ntitle = latexstring(\"\\$\\\\mathbf{\\\\mathit{Y}}\\$ : Corrupted image matrix\\n(missing pixels set to 0)\")\npy = jime(Y ; title)\n# savefig(py, \"mc_ap_y.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#What-is-rank(Y)-??","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"What is rank(Y) ??","text":"A 5-9\nB 10-49\nC 50-59\nD 60-70\nE 71-200\n\nrank(Y) svdvals(Y)\n\nShow mask, count proportion of missing entries\n\nfrac_nonzero = count(M) / length(M)\ntitle = latexstring(\"\\$\\\\mathbf{\\\\mathit{M}}\\$ : Locations of observed entries\")\npm = jim1(M; title, args...,\n    xlabel = \"sampled fraction = $(round(frac_nonzero * 100, digits=1))%\")\n# savefig(pm, \"mc_ap_m.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#Low-rank-approximation","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Low-rank approximation","text":"A simple low-rank approximation works poorly for missing data.\n\nr = 5\nU,s,V = svd(Y)\nXr = U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of data \\$\\\\mathbf{\\\\mathit{Y}}\\$\")\npr = jime(Xr ; title)\n# savefig(pr, \"mc_ap_lr.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#Alternating-projection","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Alternating projection","text":"Alternating projection is an iterative method that alternates between projecting onto the set ð’ž of rank-5 matrices and onto the set ð’Ÿ of matrices that match the data.\n\nfunction projC(X, r::Int)\n    U,s,V = svd(X)\n    return U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]' # project onto \"ð’ž\" &Cscr; U+1D49E\nend;\n\nfunction lrmc_alt(Y, r::Int, niter::Int)\n    Xr = copy(Y)\n    Xr[.!M] .= mean(Y[M]) # init: fill missing values with mean of other values\n    @show nrmse(Xr)\n    for iter in 1:niter\n        Xr = projC(Xr, r) # project onto \"ð’ž\" &Cscr; U+1D49E\n        Xr[M] .= Y[M] # project onto \"ð’Ÿ\" &Dscr; U+1D49F\n        if 0 == iter % 40\n            @show nrmse(Xr)\n        end\n    end\n    return Xr\nend;\n\nniter_alt = 400\nr = 5\nXr = lrmc_alt(Y, r, niter_alt)\ntitle = \"Alternating Projection at $niter_alt iterations\"\npa = jime(Xr ; title)\n# savefig(pa, \"mc_ap_400.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#What-is-rank(Xr)-here-??","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"What is rank(Xr) here ??","text":"A 5-9\nB 10-49\nC 50-59\nD 60-70\nE 71-200\n\nrank(Xr) svdvals(Xr)\n\nRun one more projection step onto the set of rank-r matrices\n\nXfinal = projC(Xr, r)\npf = jime(Xfinal ; title=\"Alternating Projection at $niter_alt iterations\")\n# savefig(pf, \"mc_ap_xh.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#What-is-rank(Xfinal)-here-??","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"What is rank(Xfinal) here ??","text":"A 5-9\nB 10-49\nC 50-59\nD 60-70\nE 71-200\n\nrank(Xfinal)\n\nPlot singular values\n\nsr = svdvals(Xr)\nrankeff = s -> count(>(0.01*s[1]), s); # effective rank\nnothing #hide\n\nps = plot(title=\"singular values\",\n xaxis=(L\"k\", (1, minimum(size(Y))), [1, rankeff(sr), minimum(size(Y))]),\n yaxis=(L\"Ïƒ\",), labelfontsize = 18,\n leftmargin = 15px, bottommargin = 20px, size = (600,350), widen = true,\n)\nscatter!(ps, svdvals(Y), color=:red, label=\"Y (data)\", marker=:dtriangle)\nscatter!(ps, svdvals(Xtrue), color=:blue, label=\"Xtrue\", marker=:utriangle)\npa = deepcopy(ps)\nscatter!(pa, sr, color=:green, label=\"Alt. Proj. output\")\n\n# savefig(pa, \"mc_ap_sv.pdf\")","category":"section"},{"location":"generated/demos/10/lrmc-m/#Think-about-why-Ïƒ(Y)-Ïƒ(X_{\\mathrm{true}})","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Think about why Ïƒâ‚(Y)  Ïƒâ‚(X_mathrmtrue)","text":"prompt()","category":"section"},{"location":"generated/demos/10/lrmc-m/#Nuclear-norm-approach","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Nuclear norm approach","text":"Now we will try to recover the matrix using low-rank matrix completion with a nuclear-norm regularizer.\n\nThe optimization problem we will solve is:\n\nargmin_mathbfmathitX frac12\n mathbfmathitM \n (mathbfmathitX - mathbfmathitY) _mathrmF^2\n+ beta  mathbfmathitX _*\nquadquad (textNN-min)\n\nmathbfmathitY is the zero-filled input data matrix\nmathbfmathitM is the binary sampling mask.\n\nDefine cost function for optimization problem\n\nnucnorm = (X) -> sum(svdvals(X)) # nuclear norm\ncostfun1 = (X,beta) -> 0.5 * norm(M .* (X - Y))^2 + beta * nucnorm(X); # regularized cost\nnothing #hide","category":"section"},{"location":"generated/demos/10/lrmc-m/#Q.-The-cost-function-above-is-(convex,-strictly-convex):","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Q. The cost function above is (convex, strictly convex):","text":"A: F,F\nB: F,T\nC: T,F\nD: T,T\n\nDefine singular value soft thresholding (SVST) function\n\nfunction SVST(X::AbstractMatrix, beta::Real)\n    U,s,V = svd(X) # see below\n    sthresh = @. max(s - beta, 0)\n    index = findall(>(0), sthresh)\n    return (@view U[:,index]) * Diagonal(sthresh[index]) * (@view V[:,index])'\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/10/lrmc-m/#Q.-Which-SVD-is-that?","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Q. Which SVD is that?","text":"A compact\nB economy\nC full\nD SUV\nE none of these\nU,s,V = svd(Y)\n@show size(s), size(U), size(V)","category":"section"},{"location":"generated/demos/10/lrmc-m/#ISTA","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"ISTA","text":"The iterative soft-thresholding algorithm (ISTA) is an extension of gradient descent for (often convex) \"composite\" cost functions that look like min_x f(x) + g(x) where f(x) is smooth and g(x) is non-smooth.\n\nISTA is also known as the proximal gradient method (PGM).\n\nISTA algorithm for solving (NN-min):\n\nInitialize mathbfmathitX_0 = mathbfmathitY (zero-fill missing entries)\nfor k=0,1,2,...\nmathbfmathitX_k_ij = begincasesmathbfmathitX_k_ij  textif  (ij)  Î©  mathbfmathitY_ij  textif  (ij)  Î© endcases (Put back in known entries)\nmathbfmathitX_k+1 = textSVST(mathbfmathitX_k beta) (Singular value soft-thresholding)\nend\n\nISTA for matrix completion, using functions SVST and costfun1\n\nfunction lrmc_ista(Y, M, beta::Real, niter::Int)\n    X = copy(Y)\n    Xold = copy(X)\n    cost = zeros(niter+1)\n    cost[1] = costfun1(X, beta)\n    for k in 1:niter\n        @. X[M] = Y[M] # in place\n        X = SVST(X, beta)\n        cost[k+1] = costfun1(X, beta)\n    end\n    return X, cost\nend;\nnothing #hide\n\nApply ISTA\n\nniter = 1000\nbeta = 0.8 # chosen by trial-and-error here\nxh_ista, cost_ista = lrmc_ista(Y, M, beta, niter)\npp = jime(xh_ista ; title=\"ISTA result at $niter iterations\")\n\n# savefig(pp, \"mc-nuc-ista.pdf\")\n\nThat result is not good. What went wrong? Let's investigate. First, check if the ISTA solution is actually low-rank.\n\nsp = svdvals(xh_ista)\npsi = deepcopy(ps)\nscatter!(psi, sp, color=:orange, label=L\"\\hat{X} \\mathrm{(ISTA)}\")\nxticks!(psi, [1, rtrue, rank(Diagonal(sp)), minimum(size(Y))])\n\nprompt()\n\nNow check the cost function. It is decreasing monotonically, but quite slowly.\n\nscatter(cost_ista, color=:orange,\n    title=\"cost vs. iteration\",\n    xlabel=\"iteration\",\n    ylabel=\"cost function value\",\n    label=\"ISTA\")\n\nprompt()","category":"section"},{"location":"generated/demos/10/lrmc-m/#FISTA","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"FISTA","text":"The fast iterative soft-thresholding algorithm (FISTA) is a modification of ISTA that includes Nesterov acceleration for much faster convergence. Also known as the fast proximal gradient method (FPGM).\n\nReference:\n\nBeck, A. and Teboulle, M., 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183-202.\n\nFISTA algorithm for solving (NN-min)\n\ninitialize matrices mathbf Z_0 = mathbf X_0 = mathbf Y\nfor k=0,1,2,...\nmathbfZ_k_ij = begincasesmathbf Z_k_ij  textif(ij)  Î©  mathbfY_ij  textif(ij)  Î© endcases (Put back in known entries)\nmathbfX_k+1 = textSVST(mathbfZ_k beta)\nt_k+1 = frac1 + sqrt1+4t_k^22 (Nesterov step-size)\nmathbfZ_k+1 = mathbfX_k+1 + fract_k-1t_k+1 (mathbfX_k+1 - mathbfX_k) (Momentum update)\nend\n\nFISTA algorithm for low-rank matrix completion, using SVST and costfun1\n\nfunction lrmc_fista(Y, M, beta::Real, niter::Int)\n    X = copy(Y)\n    Z = copy(X)\n    Xold = copy(X)\n    told = 1\n    cost = zeros(niter+1)\n    cost[1] = costfun1(X, beta)\n    for k in 1:niter\n        @. Z[M] = Y[M]\n        X = SVST(Z, beta)\n        t = (1 + sqrt(1+4*told^2))/2\n        Z = X + ((told-1)/t) * (X - Xold)\n        Xold = copy(X)\n        told = t\n        cost[k+1] = costfun1(X, beta) # comment out to speed-up\n    end\n    return X, cost\nend;\nnothing #hide\n\nRun FISTA\n\nniter = 300\nxh_nn_fista, cost_fista = lrmc_fista(Y, M, beta, niter)\np1 = jime(xh_nn_fista ; title=\"FISTA with nuclear norm at $niter iterations\")\n\n# savefig(p1, \"lrmc-nn-fs300.pdf\")\n\nPlot showing that FISTA converges much faster! POGM would be even faster.\n\nplot(title=\"cost vs. iteration for NN regularizer\",\n    xlabel=\"iteration\", ylabel=\"cost function value\")\nscatter!(cost_ista, color=:orange, label=\"ISTA\")\nscatter!(cost_fista, color=:magenta, label=\"FISTA\")\n\nprompt()\n\nSee if the FISTA result is \"low rank\"\n\nsf = svdvals(xh_nn_fista)\nrfista = rank(Diagonal(sf))\nrfista, rankeff(sf)\n\npsf = deepcopy(ps)\nscatter!(psf, sf, color=:magenta, label=\"Xh (output of FISTA)\")\nxticks!(psf, [1, rtrue, rfista, minimum(size(Y))])\n\nprompt()\n\nOptional exercise: think about why Ïƒ_1(Y)  Ïƒ_1(hatX)  Ïƒ_1(X_mathrmtrue)\nOptional: try ADMM too","category":"section"},{"location":"generated/demos/10/lrmc-m/#Your-work-goes-below-here","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Your work goes below here","text":"The results below are place-holders that will be much improved when implemented properly.\n\nif true # replace these place-holder functions with your work\n    shrink_p_1_2(v, reg::Real) = v\n    lr_schatten(Y, reg::Real) = Y\n    fista_schatten(Y, M, reg::Real, niter::Int) = Y\n    name = \"placeholder \" # change to \"for Schatten p=1/2\"\nelse # instructor version\n    mydir = ENV[\"hw551test\"] # change path\n    include(mydir * \"shrink_p_1_2.jl\") # 1D shrinker for |x|^(1/2), previous HW\n    include(mydir * \"lr_schatten.jl\")\n    include(mydir * \"fista_schatten.jl\")\n    name = \"for Schatten p=1/2\"\nend;\nnothing #hide\n\nApply FISTA for Schatten p=1/2\n\nniter = 100\nreg_fs = 120\nxh_fs = fista_schatten(Y, M, reg_fs, niter)\n\np2 = jime(xh_fs; title=\"FISTA $name, $niter iterations\")\n# savefig(\"schatten_complete_fs150_sp.pdf\")\n\nSee if the Schatten FISTA result is \"low rank\"\n\nss = svdvals(xh_fs)\nrank_schatten_fista = rank(Diagonal(ss))\nrank_schatten_fista, rankeff(ss)\n\npss = deepcopy(ps)\nscatter!(pss, ss, color=:cyan, label=\"Xh (FISTA $name)\")\nxticks!(pss, [1, rank_schatten_fista, minimum(size(Y))])\n\nprompt()\n\nred-black-blue colormap\n\nRGB255(args...) = RGB((args ./ 255)...)\ncolor = cgrad([RGB255(230, 80, 65), :black, RGB255(23, 120, 232)]);\nnothing #hide\n\nError image for nuclear norm\n\np3 = jimc(xh_nn_fista - Xtrue; title = \"FISTA Nuclear Norm: Xh-X\",\n clim=(-80,80), color)\n# savefig(p3, \"schatten_complete_fs300_nn_err.pdf\")\n\nError image for schatten p=1/2\n\np4 = jimc(xh_fs - Xtrue; title = \"FISTA $name: Xh-X\",\n clim=(-80,80), color)\n# savefig(p4, \"schatten_complete_fs150_sp_err.pdf\")\n\nCost function plot\n\nif false # set to true for HW\n   costfun2 = (X,Î²) -> 0.5 * norm(M .* (X - Y))^2 + Î² * norm(svdvals(X), 1/2)\n   tmp = niter -> costfun2( fista_schatten(Y, M, reg_fs, niter), reg_fs )\n   niter2 = 100\n   cost_fista2 = tmp.(0:niter)\n   p5 = scatter(0:niter, cost_fista2,\n    title = \"cost vs. iteration for $name\",\n    xlabel = \"iteration\",\n    ylabel = \"cost function value\",\n    label = \"FISTA $name\",\n   )\n# savefig(p5, \"schatten_complete_fs100_cost.pdf\")\nend","category":"section"},{"location":"generated/demos/10/lrmc-m/#Reproducibility","page":"Low-rank matrix completion: AltMin, ISTA, FISTA","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/11/ring3/#ring3","page":"Classification with MLP","title":"Classification with MLP","text":"This demo illustrates basic artificial NN training for a simple synthetic classification example with cross-entropy loss using Julia's Flux library.\n\nJeff Fessler, University of Michigan\n2018-10-18 Julia 1.0.1 original\n2024-02-26 Julia 1.10.1 update\n\nThis page comes from a single Julia file: ring3.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: ring3.ipynb, or open it in binder here: ring3.ipynb.","category":"section"},{"location":"generated/demos/11/ring3/#Setup","page":"Classification with MLP","title":"Setup","text":"Packages needed here.\n\nimport Flux # Julia package for deep learning\nusing Flux: Dense, Chain, relu, params, Adam, throttle, mse\nusing Flux.Losses: logitcrossentropy\nusing OneHotArrays: onehotbatch\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings # pretty plot labels\nusing MIRTjim: jim, prompt\nusing Random: seed!, randperm\nusing Plots: Plot, plot, plot!, scatter!, default, gui, savefig\nusing Plots.PlotMeasures: px\n\ndefault(markersize=5, markerstrokecolor=:auto, label=\"\",\n legendfontsize=16, labelfontsize=16, tickfontsize=14)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/demos/11/ring3/#Generate-(synthetic)-data","page":"Classification with MLP","title":"Generate (synthetic) data","text":"This data is suitable for a \"hand crafted\" classifier.\n\n(The Xvm versions are vectors of matrices.)\n\nFunctions to simulate data that cannot be linearly separated\n\nfunction sim_ring(\n    n::Int, # number of points\n    r::Real, # radius of center of annulus center\n    Ïƒ::Real, # spread (annulus width)\n)\n    T = promote_type(eltype(r), eltype(Ïƒ))\n    rad = r .+ Ïƒ * randn(T, n)\n    ang = T(2Ï€) * rand(T, n)\n    return Matrix([rad .* cos.(ang)  rad .* sin.(ang)]') # (2,n)\nend;\n\nK = 3 # classes\nnsim = (40, 80, 120) # how many in each class\nrsim = (0, 3, 6) # mean radii of each class\ndsim = (1.5, 4.5) # ideal decision boundaries\nfunction simdata(;\n    n = nsim,\n    r = rsim,\n    Ïƒ = Float32.((0.7, 0.5, 0.5)),\n)\n    Xvm = [sim_ring(n[k], r[k], Ïƒ[k]) for k in 1:K] # [K][n]\n    Yvm = [fill(k, 1, n[k]) for k in 1:K] # [K][n]\n    return (Xvm, Yvm)\nend;\nnothing #hide\n\nScatter plot function\n\nfunction plot_data!(p, Xvm;\n    colors = (:blue, :red, :orange),\n    marks = (:circle, :star, :uptri),\n)\n    for k in 1:K\n        scatter!(p, Xvm[k][1,:], Xvm[k][2,:],\n           marker=marks[k], color=colors[k], label=\"class $k\")\n    end\n    tmp = range(0, 2Ï€, 101)\n    for d in dsim\n        plot!(p, d * cos.(tmp), d * sin.(tmp), color=:gray)\n    end\n    return p\nend\n\nfunction plot_data(Xvm; kwargs...)\n    p = plot(\n     xaxis = (L\"x_1\", (-1,1).*8, -9:3:9),\n     yaxis = (L\"x_2\", (-1,1).*8, -9:3:9),\n     aspect_ratio = 1, size = (500,500);\n     kwargs...,\n    )\n    plot_data!(p, Xvm)\nend;\nnothing #hide\n\nTraining data\n\nseed!(0)\nntrain = nsim\n(Xvm_train, Yvm_train) = simdata(; n=ntrain)\np0 = plot_data(Xvm_train)\n# savefig(p0, \"ring3-data.pdf\")\n\nprompt()\n\nValidation and testing data\n\n(Xvm_valid, Yvm_valid) = simdata()\n(Xvm_test, Yvm_test) = simdata()\n\np1 = plot_data(Xvm_valid; title=\"Validation\")\np2 = plot_data(Xvm_test; title=\"Test\")\n\np0t = plot!(deepcopy(p0); title=\"Train\")\np3 = plot(p0t, p1, p2;\n leftmargin = 30px, bottommargin = 35px,\n size = (1600,500), layout=(1,3),\n)\n\nprompt()","category":"section"},{"location":"generated/demos/11/ring3/#Hand-crafted-classifier","page":"Classification with MLP","title":"Hand-crafted classifier","text":"This data is not linearly separable, but a simple nonlinearity makes it so.\n\nlift1 = x -> [x; sqrt(sum(abs2, x))] # lift 1 feature vector\nlifter = xx -> mapslices(lift1, xx, dims=1) # apply to each data column\nXvm_train_lift = lifter.(Xvm_train)\nXm_train_lift = hcat(Xvm_train_lift...)\npl_train = plot(Xm_train_lift[3,:], marker=:circle, yticks=(1:5)*1.5,\n xticks = cumsum([1; collect(ntrain)]),\n)\n\nprompt()\n\nSo the levels 1.5 and 4.5 should work to separate classes. Try on the test data.\n\nXm_test_lift = hcat(lifter.(Xvm_test)...)\nlift_classifier = x -> x < dsim[1] ? 1 : x > dsim[2] ? 3 : 2\nclass_test_lift = lift_classifier.(Xm_test_lift[3,:])\ntest_lift_errors = count(class_test_lift .!= hcat(Yvm_test...)')\n@show test_lift_errors, size(Xm_test_lift, 2)\n\nPermute the training data (just to be cautious)\n\nXm_train = hcat(Xvm_train...)\nYm_train = hcat(Yvm_train...)\nperm = randperm(size(Xm_train,2))\nXm_train = Xm_train[:,perm] # (2, sum(nsim))\nYm_train = Ym_train[:,perm] # (1, sum(nsim))\nXm_valid = hcat(Xvm_valid...)\nYm_valid = hcat(Yvm_valid...)\nXm_test = hcat(Xvm_test...)\nYm_test = hcat(Yvm_test...);\nnothing #hide","category":"section"},{"location":"generated/demos/11/ring3/#Train-simple-MLP-model","page":"Classification with MLP","title":"Train simple MLP model","text":"A [multilayer perceptron model] (https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) consists of multiple fully connected layers.\n\nTrain a basic NN model with 1 hidden layer; here using MSE loss just for illustration.\n\nif !@isdefined(state1)\n    nhidden = 10 # neurons in hidden layer\n    model = Chain(Dense(2, nhidden, relu), Dense(nhidden, 1))\n    loss3ms(model, x, y) = mse(model(x), y) # admittedly silly choice\n    iters = 2000\n    dataset = Base.Iterators.repeated((Xm_train, Ym_train), iters)\n    state1 = Flux.setup(Adam(), model)\n    Flux.train!(loss3ms, model, dataset, state1)\nend;\n\nscalar = y -> y[1]\nmodel1 = x -> scalar(model(x))\n\nPlot results after training\n\nfunction display_decision_boundaries(\n    model;\n    x1range = range(-1f0,1f0,101)*8,\n    x2range = x1range,\n    kwargs...,\n)\n    D = [model1([x1;x2]) for x1 in x1range, x2 in x2range]\n    D = round.(D)\n    jim(x1range, x2range, D; color=:cividis,\n     xaxis = (L\"x_1\", (-1,1).*8, -9:3:9),\n     yaxis = (L\"x_2\", (-1,1).*8, -9:3:9),\n     aspect_ratio = 1, size = (500,500),\n    kwargs...)\nend;\n\nfunction display_decision_boundaries(model, Xvm; kwargs...)\n    p = display_decision_boundaries(model; kwargs...)\n    plot_data!(p, Xvm)\nend\n\nExamine classification accuracy\n\nclassacc(model, x, y::Number) = round(model1(x)) == y\nclassacc(model, x, y::AbstractArray) = classacc(model, x, y[1])\nfunction classacc(Xm, Ym)\n    tmp = zip(eachcol(Xm), eachcol(Ym))\n    tmp = count(xy -> classacc(model, xy...), tmp)\n    tmp = tmp / size(Ym,2) * 100\n    return round(tmp, digits=3)\nend\n\nlossXYtrain = loss3ms(model, Xm_train, Ym_train)\np4 = display_decision_boundaries(model, Xvm_train;\n title = \"Train: RMSE Loss = $(round(sqrt(lossXYtrain),digits=4)), \" *\n \"Class=$(classacc(Xm_train, Ym_train)) %\");\nlossXYtest = loss3ms(model, Xm_test, Ym_test)\np5 = display_decision_boundaries(model, Xvm_test;\n     title = \"Test: RMSE Loss = $(round(sqrt(lossXYtest),digits=4)), \" *\n    \"Class=$(classacc(Xm_test, Ym_test)) %\");\nplot(p4, p5; size=(1000,500))\n\nprompt()","category":"section"},{"location":"generated/demos/11/ring3/#Train-while-validating","page":"Classification with MLP","title":"Train while validating","text":"This time using cross-entropy loss, which makes more sense for a classification problem.\n\nCreate a basic NN model with 1 hidden layer. This version evaluates performance every epoch for both the training data and validation data.\n\nif !@isdefined(model2) # || true\n    layer2 = Dense(2, nhidden, relu)\n    layer3 = Dense(nhidden, K)\n    model2 = Chain(layer2, layer3)\n    onehot1 = y -> reshape(onehotbatch(y, 1:K), K, :)\n    loss3ce(model, x, y) = logitcrossentropy(model(x), onehot1(y))\n\n    nouter = 80 # of outer iterations, for showing loss\n    losstrain = zeros(nouter+1)\n    lossvalid = zeros(nouter+1)\n\n    iters = 100 # inner iterations\n    losstrain[1] = loss3ce(model2, Xm_train, Ym_train)\n    lossvalid[1] = loss3ce(model2, Xm_valid, Ym_valid)\n\n    model2s = similar(Vector{Any}, nouter) # to checkpoint every outer iteration\n    for io in 1:nouter\n        # @show io\n        idataset = Base.Iterators.repeated((Xm_train, Ym_train), iters)\n        istate = Flux.setup(Adam(), model2)\n        Flux.train!(loss3ce, model2, idataset, istate)\n        losstrain[io+1] = loss3ce(model2, Xm_train, Ym_train)\n        lossvalid[io+1] = loss3ce(model2, Xm_valid, Ym_valid)\n        if (io â‰¤ 6) && false # set to true to make images\n            display_decision_boundaries(model2, Xvm_train)\n            plot!(title=\"$(io*iters) epochs\")\n            gui(); sleep(0.3)\n        end\n        model2s[io] = deepcopy(model2)\n    end\nend;\nnothing #hide\n\nShow loss vs epoch\n\nivalid = findfirst(>(0), diff(lossvalid))\nplot(xlabel=\"epoch/$(iters)\", yaxis=(\"CE loss\", (0,1.05*maximum(losstrain))))\nplot!(0:nouter, lossvalid, label=\"validation\", marker=:+, color=:violet)\nplot!(0:nouter, losstrain, label=\"training\", marker=:o, color=:green)\nplot!(xticks = [0, ivalid, nouter])\n\nprompt()\n\nmodel2v = model2s[ivalid]; # model at validation epoch\nlossf = (Xm,Ym) -> round(loss3ce(model2v, Xm, Ym), digits=4)\nloss_train = lossf(Xm_train, Ym_train)\nloss_valid = lossf(Xm_valid, Ym_valid)\nloss_test = lossf(Xm_test, Ym_test);\n\np1 = display_decision_boundaries(model2v, Xvm_train;\n title=\"Train:\\nCE Loss = $loss_train\\n\" *\n    \"Class=$(classacc(Xm_train, Ym_train)) %\",\n)\np2 = display_decision_boundaries(model2v, Xvm_valid;\n title=\"Valid:\\nCE Loss = $loss_valid\\n\" *\n    \"Class=$(classacc(Xm_valid, Ym_valid)) %\",\n)\np3 = display_decision_boundaries(model2v, Xvm_test;\n title=\"Test:\\nCE Loss = $loss_test\\n\" *\n    \"Class=$(classacc(Xm_valid, Ym_valid)) %\",\n)\np123 = plot(p1, p2, p3; size=(1500,500), layout=(1,3))\n\nprompt()\n\nShow response of (trained) first hidden layer, at validation step\n\nx1range = range(-1f0,1f0,31) * 6\nx2range = range(-1f0,1f0,33) * 6\ntmp = model2v.layers[1]\nlayer2data = [tmp([x1;x2])[n] for x1 = x1range, x2 = x2range, n in 1:nhidden]\n\npl = Array{Plot}(undef, nhidden)\nfor n in 1:nhidden\n    ptmp = jim(x1range, x2range, layer2data[:,:,n], color=:cividis,\n        xtick=-6:6:6, ytick=-6:6:6,\n    )\n    if n == 7\n        plot!(ptmp, xlabel=L\"x_1\", ylabel=L\"x_2\")\n    end\n    pl[n] = ptmp\nend\nplot(pl[1:9]...)\n\nprompt()","category":"section"},{"location":"generated/demos/11/ring3/#Reproducibility","page":"Classification with MLP","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/10/foreback/#foreback","page":"Video foreground/background separation","title":"Video foreground/background separation","text":"This example illustrates video foreground/background separation via robust PCA using the Julia language. For simplicity, the method here assumes a static camera. For free-motion camera video, see Moore et al., 2019.\n\nThis page comes from a single Julia file: foreback.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: foreback.ipynb, or open it in binder here: foreback.ipynb.","category":"section"},{"location":"generated/demos/10/foreback/#Setup","page":"Video foreground/background separation","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"ColorTypes\"\n        \"ColorVectorSpace\"\n        \"Downloads\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"LinearMapsAA\"\n        \"MIRT\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Statistics\"\n        \"VideoIO\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ColorTypes: RGB, N0f8\nusing ColorVectorSpace\nusing Downloads: download\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, I, norm, svd, svdvals\nusing LinearMapsAA: LinearMapAA, redim\nusing MIRT: pogm_restart\nusing MIRTjim: jim, prompt\nusing Plots: default, gui, plot, savefig\nusing Plots: gif, @animate, Plots\nusing Statistics: mean\nusing VideoIO\ndefault(); default(markerstrokecolor=:auto, label = \"\", markersize=6,\nlegendfontsize = 8) # todo: https://github.com/JuliaPlots/Plots.jl/issues/4621\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/10/foreback/#Load-video-data","page":"Video foreground/background separation","title":"Load video data","text":"Load raw data\n\nif !@isdefined(y1)\n    url = \"https://github.com/JeffFessler/book-la-data/raw/main/data/bmc-12/111-240-320-100.mp4\"\n    tmp = download(url)\n    y1 = VideoIO.load(tmp) # 100 frames of size (240,320)\n    if !isinteractive() # downsample for github cloud\n        y1 = map(y -> y[1:2:end,1:2:end], (@view y1[2:2:end]))\n    end\nend;\nnothing #hide\n\nConvert to array\n\nyf = map(y -> 1f0*permutedims(y, (2,1)), y1) # 100 frames of size (320,240)\nY3 = stack(yf) # (nx,ny,nf)\n(nx, ny, nf) = size(Y3)\npy = jim([yf[1], yf[end], yf[end]-yf[1]];\n    nrow = 1, size = (600, 200),\n    title=\"Frame 001  |  Frame $nf  |  Difference\")","category":"section"},{"location":"generated/demos/10/foreback/#Cost-function","page":"Video foreground/background separation","title":"Cost function","text":"Encoding operator A = [I I] for L+S because we stack X = [L;;;S]\n\ntmp = LinearMapAA(I(nx*ny*nf);\n    odim=(nx,ny,nf), idim=(nx,ny,nf), T=Float32, prop=(;name=\"I\"))\ntmp = kron([1 1], tmp)\nA = redim(tmp; odim=(nx,ny,nf), idim=(nx,ny,nf,2)) # \"squeeze\" odim\n\nunstack(X, i) = selectdim(X, ndims(X), i)\nLpart = X -> unstack(X, 1) # extract \"L\" from X\nSpart = X -> unstack(X, 2) # extract \"S\" from X\nnucnorm(L::AbstractMatrix) = sum(svdvals(L)) # nuclear norm\nnucnorm(L::AbstractArray) = nucnorm(reshape(L, :, nf)); # (nx*ny, nf) for L\nnothing #hide\n\nThe robust PCA optimization cost function is:\n\nÎ¨(mathbfLmathbfS) = frac12\n  mathbfL + mathbfS - mathbfY _mathrmF^2\n + Î±  mathbfL _* + Î²  mathrmvec(mathbfS) _1\n\nor equivalently\n\nÎ¨(mathbfX) = frac12\n left mathbfI  mathbfI mathbfX - mathbfY right_mathrmF^2\n + Î±  mathbfX_1 _* + Î²  mathrmvec(mathbfX_2) _1\n\nwhere mathbfY is the original data matrix.\n\nrobust_pca_cost(Y, X, Î±::Real, Î²::Real) =\n    0.5 * norm( A * X - Y )^2 + Î± * nucnorm(Lpart(X)) + Î² * norm(Spart(X), 1);\nnothing #hide\n\nProximal algorithm helpers:\n\nsoft(z,t) = sign(z) * max(abs(z) - t, 0);\nnothing #hide\n\nSingular value soft thresholding (SVST) function:\n\nfunction SVST(X, beta)\n    shape = size(X)\n    X = reshape(X, :, shape[end]) # unfold\n    U,s,V = svd(X)\n    sthresh = @. soft(s, beta)\n    jj = findall(>(0), sthresh)\n    out = U[:,jj] * Diagonal(sthresh[jj]) * V[:,jj]'\n    return reshape(out, shape)\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/10/foreback/#Algorithm","page":"Video foreground/background separation","title":"Algorithm","text":"Proximal gradient methods for minimizing robust PCA cost function.\n\nThe proximal optimized gradient method (POGM) with adaptive restart is faster than FISTA with very similar computation per iteration. POGM does not require any algorithm tuning parameter, making it easier to use than ADMM in many practical composite optimization problems.\n\nfunction robust_pca(Y;\n    L = Y,\n    S = zeros(size(Y)),\n    Î± = 1,\n    Î² = 1,\n    mom = :pogm,\n    Fcost::Function = X -> robust_pca_cost(Y, X, Î±, Î²),\n#  fun = (iter, xk, yk, is_restart) -> (),\n    fun = mom === :fgm ?\n        (iter, xk, yk, is_restart) -> (yk, Fcost(yk), is_restart) :\n        (iter, xk, yk, is_restart) -> (xk, Fcost(xk), is_restart),\n    kwargs..., # for pogm_restart\n)\n\n    X0 = stack([L, S])\n    f_grad = X -> A' * (A * X - Y) # gradient of smooth term\n    f_L = 2 # Lipschitz constant of f_grad\n    g_prox = (X, c) -> stack([SVST(Lpart(X), c * Î±), soft.(Spart(X), c * Î²)])\n    Xhat, out = pogm_restart(X0, Fcost, f_grad, f_L; g_prox, fun, mom, kwargs...)\n    return Xhat, out\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/10/foreback/#Run-algorithm","page":"Video foreground/background separation","title":"Run algorithm","text":"Apply robust PCA to each RGB color channel separately for simplicity, then reassemble.\n\nchannels = [:r :g :b]\nif !@isdefined(Xpogm)\n    Î± = 30\n    Î² = 0.1\n    niter = 10\n    Xc = Array{Any}(undef, 3)\n    out = Array{Any}(undef, 3)\n    for (i, c) in enumerate(channels) # separate color channels\n        @info \"channel $c\"\n        Yc = map(y -> getfield(y, c), Y3);\n        Xc[i], out[i] = robust_pca(Yc; Î±, Î², mom = :pogm, niter)\n    end\n    Xpogm = map(RGB{Float32}, Xc...) # reassemble colors\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/10/foreback/#Results","page":"Video foreground/background separation","title":"Results","text":"Extract low-rank (background) and sparse (foreground) components:\n\nLpogm = Lpart(Xpogm)\nSpogm = Spart(Xpogm)\niz = nf * 81 Ã· 100\ntmp = stack([Y3[:,:,iz], Lpogm[:,:,iz], Spogm[:,:,iz]])\njim(:line3type, :white)\npf = jim(tmp; nrow=1, size=(700, 250),\n# xaxis = false, yaxis = false, # book\n title=\"Original frame $iz | low-rank background | sparse foreground\")\n# savefig(pf, \"foreback-81.pdf\")\n\nCost function plot overall cost for all 3 color channels\n\ntmp = sum(out -> [o[2] for o in out], out)\npc = plot(0:niter, tmp; marker = :circle, label=\"POGM\", widen = true,\n  xaxis=(\"Iteration\", (0,niter), 0:2:niter), ylabel=\"Cost function\")\n\nprompt()","category":"section"},{"location":"generated/demos/10/foreback/#Alternatives","page":"Video foreground/background separation","title":"Alternatives","text":"Explore simpler methods:\n\naverage of each color channel\nfirst SVD component of each color channel\n\nif !@isdefined(Xsvd3)\n    Xmean = Vector{Matrix{Float32}}(undef, 3)\n    Xsvd3 = Vector{Matrix{Float32}}(undef, 3)\n    refold = v -> reshape(v, nx, ny)\n    for (i, c) in enumerate(channels) # separate color channels\n        @info \"channel $c\"\n        tmp_ = map(y -> getfield(y, c), Y3) # (nx,ny,nf)\n        Xsvd3[i] = refold(svd(reshape(tmp_, :, nf)).U[:,1]) # first component\n        Xmean[i] = refold(mean(tmp_, dims=3))\n    end\n    Xmean = map(RGB{Float32}, Xmean...) # reassemble colors\n    L1 = Lpogm[:,:,1]\n    extrema(norm.(Lpogm .- Xmean))\nend\n\nIn this case the temporal average of the video sequence is pretty close to the low-rank component, because this video is so simple. The benefits of robust PCA would be more apparent with more complicated videos, e.g., with illumination changes. Even here one can see undesirable effects of the sparse component in the average when we scale the difference image.\n\npm = jim(\n jim(Xmean, \"Average\"),\n jim(L1, L\"L_1\"),\n jim(9 * abs.(L1 - Xmean), \"9 Ã— |L_1 - average|\"),\n# jim(L1 .== Xmean),\n)\n\nExamining the first SVD component of each color takes a bit more work. SVD components have unit norm, but RGB values should be in [0,1] range. And the SVD has a sign ambiguity. Even after correcting for those issues, the SVD version has a visibly different color tint.\n\nps1 = jim(Xsvd3; nrow=1, title=\"Xsvd before corrections\", size=(600,200))\n\nCorrect for SVD sign ambiguity\n\nXsvd = map(x -> x / sign(mean(x)), Xsvd3)\nps2 = jim(Xsvd; nrow=1, title=\"Xsvd after sign correction\", size=(600,200))\n\nCorrect for scaling\n\nsvdmax = maximum(maximum, Xsvd)\nXsvd = map(x -> x / svdmax, Xsvd)\nXsvd = map(RGB{Float32}, Xsvd...) # reassemble colors\nps = jim(\n jim(yf[1], \"First frame\"),\n jim(Lpogm[:,:,1], L\"L_1\"),\n jim(Xsvd, \"Xsvd\"),\n layout = (1,3),\n size = (600,200),\n)\n\nExplore rank-1 approximation of each color channel.\n\nfunction lr_channel(arr::Array, r::Int=1)\n    Xlr = Vector{Array{Float32}}(undef, 3)\n    for (i, c) in enumerate(channels) # separate color channels\n        @info \"lr_channel $c\"\n        tmp = map(y -> getfield(y, c), arr) # (nx,ny,nf)\n        tmp = svd(reshape(tmp, :, nf))\n        tmp = tmp.U[:,1:r] * Diagonal(tmp.S[1:r]) * tmp.Vt[1:r,:]\n        tmp = reshape(tmp, nx, ny, :)\n        Xlr[i] = tmp\n    end\n    return Xlr\nend\nif !@isdefined(Xr1)\n    Xr1 = lr_channel(Y3)\n    Xr1 = map(RGB{Float32}, Xr1...) # reassemble colors\n    jim(Xr1)\nend;\nnothing #hide\n\nExamine residual between rank-1 approximation and the original video sequence. The residual has a lot of sparse structure to it, rather than being random noise, suggesting that a robust PCA approach with a low-rank + sparse signal model could be more suitable.\n\npr = jim(Y3 - Xr1)\n\nAnimate videos\n\nanim1 = @animate for it in 1:nf\n    tmp = stack([Y3[:,:,it], Lpogm[:,:,it], Spogm[:,:,it]])\n    jim(tmp; nrow=1, title=\"Original | Low-rank | Sparse\",\n        xlabel = \"Frame $it\", size=(600, 250))\n#  gui()\nend\ngif(anim1; fps = 6)","category":"section"},{"location":"generated/demos/10/foreback/#Reproducibility","page":"Video foreground/background separation","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/lr-cv/#lr-cv","page":"Low-Rank Selection via Cross Validation","title":"Low-Rank Selection via Cross Validation","text":"This example illustrates low-rank matrix approximation using cross-validation methods for rank parameter selection, using the Julia language. As discussed by Owen & Perry, 2009, separate row or column hold-out is ineffective, whereas Bi-Cross-Validation (BCV) is more effective.\n\nThis page comes from a single Julia file: lr-cv.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: lr-cv.ipynb, or open it in binder here: lr-cv.ipynb.","category":"section"},{"location":"generated/demos/07/lr-cv/#Setup","page":"Low-Rank Selection via Cross Validation","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: svd, svdvals, Diagonal, norm, pinv\nusing MIRTjim: prompt\nusing Plots: default, gui, plot, plot!, scatter!, savefig\nusing Random: seed!, randperm\nusing Statistics: mean\ndefault(); default(label=\"\", markerstrokecolor=:auto, markersize=7,\n    labelfontsize=20, tickfontsize=16, legendfontsize=17, widen=true)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-cv/#Generate-data","page":"Low-Rank Selection via Cross Validation","title":"Generate data","text":"Noiseless low-rank matrix and noisy data matrix\n\nM, N = 100, 50 # problem size\nseed!(0)\nKtrue = 5 # true rank (planted model)\nX = svd(randn(M,Ktrue)).U * Diagonal(1:Ktrue) * svd(randn(Ktrue,N)).Vt\nsig0 = 0.03 # noise standard deviation\nY = X + sig0 * randn(size(X)) # noisy\nsy = svdvals(Y)\nsx = svdvals(X)\nsx[1:Ktrue]\n\nsy[1:Ktrue]","category":"section"},{"location":"generated/demos/07/lr-cv/#Plot-singular-values","page":"Low-Rank Selection via Cross Validation","title":"Plot singular values","text":"ps = plot(xaxis = (L\"k\", (1,N), [1, Ktrue, N]), yaxis = (L\"Ïƒ\", (0,5.5), 0:5))\nscatter!(1:N, sy, color=:red, marker=:hexagon,\n label=L\"\\sigma_k(Y) \\ \\mathrm{noisy}\")\nscatter!(1:N, sx, color=:blue, label=L\"\\sigma_k(X) \\ \\mathrm{noiseless}\")\n\nprompt()\n\n# savefig(ps, \"lr_sure1s.pdf\")","category":"section"},{"location":"generated/demos/07/lr-cv/#Low-rank-approximation-with-various-ranks","page":"Low-Rank Selection via Cross Validation","title":"Low-rank approximation with various ranks","text":"(U, sy, V) = svd(Y)\nnrmse_K = zeros(N)\nnrmsd_K = zeros(N)\nnrmsd = (D) -> norm(D) / norm(Y) * 100\nnrmse = (D) -> norm(D) / norm(X) * 100\nfor K in 1:N\n    Xh = U[:,1:K] * Diagonal(sy[1:K]) * V[:,1:K]'\n    nrmsd_K[K] = nrmsd(Xh - Y)\n    nrmse_K[K] = nrmse(Xh - X)\nend\nnrmsd_K = [nrmsd(0 .- Y); nrmsd_K]\nnrmse_K = [nrmse(0 .- X); nrmse_K]\nklist = 0:N;\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-cv/#Plot-normalized-root-mean-squared-error/difference-versus-rank-K","page":"Low-Rank Selection via Cross Validation","title":"Plot normalized root mean-squared error/difference versus rank K","text":"pk = plot( # legend=:outertop,\n    xaxis = (L\"K\", (1,N), [0, 2, Ktrue, N]),\n    yaxis = (\"'Error' [%]\", (0, 100), 0:20:100),\n)\nscatter!(klist, nrmse_K, color=:blue,\n    label=L\"\\mathrm{NRMSE\\ } â€– \\! \\hat{X}_K - X \\ â€–_{\\mathrm{F}} / â€–X \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n)\nscatter!(klist, nrmsd_K, color=:red, marker=:diamond,\n    label=L\"\\mathrm{NRMSD\\ } â€– \\! \\hat{X}_K - Y \\ â€–_{\\mathrm{F}} / â€–Y \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n)\n\nprompt()\n\n# savefig(pk, \"lr_sure1a.pdf\")","category":"section"},{"location":"generated/demos/07/lr-cv/#Bi-cross-validation-code","page":"Low-Rank Selection via Cross Validation","title":"Bi-cross-validation code","text":"\"\"\"\n    bcv(Y::AbstractMatrix{<:Number}, ranks=1:10)\nCompute bi-cross-validation per\nhttps://doi.org/10.1214/08-AOAS227\n\"\"\"\nfunction bcv(Y::AbstractMatrix{<:Number}, ranks=1:10, fold::Int=2)\n    M, N = size(Y)\n    any(>(min(M,N)), ranks) && throw(\"bad ranks\")\n    any(<(0), ranks) && throw(\"bad ranks\")\n    H1 = MÃ·fold # hold-out rows\n    H2 = NÃ·fold # hold-out columns\n    perm1 = randperm(M)\n    hold1 = perm1[1:H1]\n    keep1 = perm1[(H1+1):M]\n    perm2 = randperm(N)\n    hold2 = perm2[1:H2]\n    keep2 = perm2[(H2+1):N]\n    A = Y[hold1,hold2]\n    B = Y[hold1,keep2]\n    C = Y[keep1,hold2]\n    D = Y[keep1,keep2]\n    U,s,V = svd(D)\n    error = zeros(length(ranks))\n    for (i, r) in enumerate(ranks)\n        Dr_pinv = V[:,1:r] * Diagonal(pinv.(s[1:r])) * U[:,1:r]'\n        error[i] = norm(A - B * Dr_pinv * C)\n    end\n    return error / norm(A) * 100\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-cv/#Apply-BCV-to-synthetic-data","page":"Low-Rank Selection via Cross Validation","title":"Apply BCV to synthetic data","text":"In this example, (2Ã—2)-fold BCV is minimized at the correct rank of 5.\n\nfold = 2\nranks = 0:min(M,N)Ã·fold\ncv = bcv(Y, ranks, fold)\nscatter!(pk, ranks, cv, color=:green, marker=:star,\n    label=L\"\\mathrm{BCV}\",\n)\ni_bcv = argmin(cv)\nscatter!([ranks[i_bcv]], [cv[i_bcv]], color=:black, marker=:star, markersize=4,)\n\nprompt()\n\n# savefig(psk, \"lr_bcv1.pdf\")\n\nCompare with row or column hold-out CV\n\n\"\"\"\n    function lr_cross_validation_by_column(Y, fold, n_components)\n\"\"\"\nfunction lr_cross_validation_by_column(\n    X::AbstractMatrix{<:Number},\n    fold::Int,\n    n_components::AbstractVector{<:Int},\n)\n\n    n_samples = size(X, 2) # Assuming columns are samples\n    fold_size = n_samples Ã· fold\n    errors = zeros(length(n_components), fold)\n\n    for fold_idx in 1:fold\n        test_indices = ((fold_idx - 1) * fold_size + 1):min(fold_idx * fold_size, n_samples)\n        train_indices = setdiff(1:n_samples, test_indices)\n\n        X_train = X[:, train_indices]\n        X_test = X[:, test_indices]\n\n        U, _, _ = svd(X_train) # \"PCA\" of training data\n\n        for (comp_idx, n_component) in enumerate(n_components)\n            Ur = U[:,1:n_component]\n            X_test_reconstructed = Ur * (Ur' * X_test)\n            errors[comp_idx, fold_idx] = # calculate reconstruction error\n                norm(X_test - X_test_reconstructed) / norm(X_test)\n        end\n    end\n    return errors * 100\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-cv/#Apply-elementary-CV-to-same-noisy-data","page":"Low-Rank Selection via Cross Validation","title":"Apply elementary CV to same noisy data","text":"Holding out rows or columns leads to highly over-estimated ranks, as predicted in the literature.\n\nThis is the approach recommended by GPT 4.1 (circa 2025-08), presumably because holding out individual data points is prevalent in machine learning.\n\nfold = 5\nKmax = min(M,N)Ã·fold\nn_components = 0:Kmax\nerrors_by_col = lr_cross_validation_by_column(Y, fold, n_components)\nerror_means_by_col = vec(mean(errors_by_col, dims=2))\ni_col = argmin(error_means_by_col) # best based on minimum mean error\n\nerrors_by_row = lr_cross_validation_by_column(Y', fold, n_components)\nerror_means_by_row = vec(mean(errors_by_row, dims=2));\ni_row = argmin(error_means_by_row) # best based on minimum mean error\n\noptimal_k_col = n_components[i_col]\noptimal_k_row = n_components[i_row]\n\npcv = plot(\n xlims=(0,10),\n xticks=[0, 1, 5, 10],\n ylims=(0,100),\n widen = true,\n xlabel = \"rank\",\n ylabel = \"NRMSD\",\n)\nscatter!(n_components, error_means_by_col, label=\"by column\")\nscatter!(n_components, error_means_by_row, label=\"by row\", marker=:x)\nscatter!([n_components[i_col]], [error_means_by_col[i_col]],\n color=:black, marker=:circle, markersize=4, )\nscatter!([n_components[i_row]], [error_means_by_row[i_row]],\n color=:black, marker=:x, markersize=4, )","category":"section"},{"location":"generated/demos/07/lr-cv/#Reproducibility","page":"Low-Rank Selection via Cross Validation","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/02/outer/#outer","page":"Vector outer product","title":"Vector outer product","text":"This example illustrates different ways of computing vector outer products using the Julia language.\n\nThis page comes from a single Julia file: outer.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: outer.ipynb, or open it in binder here: outer.ipynb.","category":"section"},{"location":"generated/demos/02/outer/#Setup","page":"Vector outer product","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"BenchmarkTools\"\n        \"InteractiveUtils\"\n        \"LazyGrids\"\n        \"LinearAlgebra\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing BenchmarkTools: @benchmark\nusing InteractiveUtils: versioninfo\nusing LazyGrids: btime\nusing LinearAlgebra: mul!","category":"section"},{"location":"generated/demos/02/outer/#Overview-of-outer-products","page":"Vector outer product","title":"Overview of outer products","text":"The outer product between two vectors x and y is simply x * y'.\n\nThis demo explores a couple ways of coding that operation.\n\nWe write each method as a function because the most reliable way to benchmark different methods is to use functions.\n\nWe are interested in the computation time, not the time spent allocating memory, so we use mutating mul! versions of all methods.","category":"section"},{"location":"generated/demos/02/outer/#The-built-in-method:","page":"Vector outer product","title":"The built-in method:","text":"f0!(out, x, y) = mul!(out, x, y');\nnothing #hide","category":"section"},{"location":"generated/demos/02/outer/#Hand-coded-double-loop","page":"Vector outer product","title":"Hand-coded double loop","text":"function f1!(out, x, y)\n    for j in 1:length(y)\n        @simd for i in 1:length(x)\n            @inbounds out[i,j] = x[i] * conj(y[j])\n        end\n    end\n    return out\nend","category":"section"},{"location":"generated/demos/02/outer/#column-times-scalar","page":"Vector outer product","title":"column times scalar","text":"function f2!(out, x, y)\n    for j in 1:length(y)\n        @inbounds @. (@view out[:,j]) = x * conj(y[j])\n    end\n    return out\nend","category":"section"},{"location":"generated/demos/02/outer/#Using-threads-across-columns","page":"Vector outer product","title":"Using threads across columns","text":"function f3!(out, x, y)\n    Threads.@threads for j in 1:length(y)\n        @inbounds @. (@view out[:,j]) = x * conj(y[j])\n    end\n    return out\nend","category":"section"},{"location":"generated/demos/02/outer/#Data-for-timing-tests","page":"Vector outer product","title":"Data for timing tests","text":"M, N = 2^9, 2^10\nT = ComplexF32\nx = rand(T, M)\ny = rand(T, N)\nout = Matrix{T}(undef, M, N)\nout2 = Matrix{T}(undef, M, N);\nnothing #hide\n\nVerify the methods are equivalent:\n\n@assert f0!(out,x,y) â‰ˆ f1!(out2,x,y) # why is â‰ˆ needed here?!\n@assert f1!(out,x,y) == f2!(out2,x,y)\n@assert f1!(out,x,y) == f3!(out2,x,y)","category":"section"},{"location":"generated/demos/02/outer/#Benchmark-the-methods","page":"Vector outer product","title":"Benchmark the methods","text":"The results will depend on the computer used, of course.\n\nx*y'\n\nt = @benchmark f0!($out, $x, $y)\ntimeu = t -> btime(t, unit=:Î¼s)\nt0 = timeu(t);\nnothing #hide\n\ndouble loop\n\nt = @benchmark f1!($out, $x, $y)\ntimeu = t -> btime(t, unit=:Î¼s)\nt1 = timeu(t);\nnothing #hide\n\ncolumn times scalar\n\nt = @benchmark f2!($out, $x, $y)\nt2 = timeu(t);\nnothing #hide\n\nthreads\n\nt = @benchmark f3!($out, $x, $y)\nt3 = timeu(t);\nnothing #hide\n\nResult summary:\n\n[\"built-in\" t0; \"double\" t1; \"column\" t2; \"thread\" t3]","category":"section"},{"location":"generated/demos/02/outer/#Remarks","page":"Vector outer product","title":"Remarks","text":"With Julia 1.9 a 2017 iMac with 8 threads (4.2 GHz Quad-Core Intel i7), the results are\n\n\"time=235.7Î¼s mem=0 alloc=0\"\n\"time=153.9Î¼s mem=0 alloc=0\"\n\"time=153.0Î¼s mem=0 alloc=0\"\n\"time=54.1Î¼s mem=4096 alloc=43\"\n\nInterestingly, the hand coded loop is faster than the built-in mul! for x * y'.\n\nThe results in github's cloud may differ, because it uses different CPUs and typically only one thread.","category":"section"},{"location":"generated/demos/02/outer/#Reproducibility","page":"Vector outer product","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/05/double-descent/#double-descent","page":"Double Descent in LS","title":"Double Descent in LS","text":"This example illustrates the phenomenon of double descent in least squares (LS) polynomial fitting using the Julia language.\n\nInspired by the article \"Characterizations of Double Descent\" by Manuchehr Aminian in SIAM News 58(10) Dec. 2025.\n\nThis page comes from a single Julia file: double-descent.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: double-descent.ipynb, or open it in binder here: double-descent.ipynb.","category":"section"},{"location":"generated/demos/05/double-descent/#Setup","page":"Double Descent in LS","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: diag, norm, I, svdvals\nusing MIRTjim: prompt, jim\nusing Plots: default, gui, plot, plot!, scatter, scatter!, savefig\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true, linewidth=2,\n    markersize = 6, tickfontsize=12, labelfontsize = 16, legendfontsize=14)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\n\n# Simulate data\nM = 100 # number of data points\nP = 99 # highest polynomial degree\nt = range(-1, 1, M)\nfun(t) = atan(2*t) # nonlinear function\ny = fun.(t)\ntrain = 1:(MÃ·2)\ntest = (MÃ·2+1):M;\nnothing #hide","category":"section"},{"location":"generated/demos/05/double-descent/#Legendre-polynomial-basis","page":"Double Descent in LS","title":"Legendre polynomial basis","text":"Build Legendre polynomial basis using Bonnet's recursion formula:\n\n(n+1) P_n+1(x) = (2n+1) x P_n(x) - n P_n-1(x)\n\nL = ones(M, P)\nL[:,2] .= t\nfor k in 3:P\n    n = k - 2 # caution: SIAM article had an error here\n    L[:, k] = ((2n+1) * t .* L[:, k-1] - n * L[:, k-2]) / (n + 1)\nend\npl = plot(t, L[:,1:5], title=\"First 5 Legendre polynomials\", marker=:dot)\n\nCheck recursion for k=3, corresponding to n=1 in Bonnet's recursion\n\np2(x) = (1/2) * (3x^2 - 1)\n@assert p2.(t) â‰ˆ L[:,3]\n\nCheck basis function normalization (continuous vs discrete)\n\np = 0:(P-1)\nnormp = @. sqrt(2 / (2p+1)) # theoretical Lâ‚‚[-1,1] norm\nnorme = norm.(eachcol(L)) / sqrt(M/2) # empirical norm, account for dx\nplot(xlabel=\"degree\", ylabel=\"norm\")\nplot!(p, norme, marker=:dot, color=:red, label=\"empirical\")\nplot!(p, normp, marker=:dot, color=:blue, label=\"analytical\")\n\nNormalize basis functions using empirical norms\n\nL = L ./ norme' / sqrt(M/2);\nnothing #hide","category":"section"},{"location":"generated/demos/05/double-descent/#Scree-plot","page":"Double Descent in LS","title":"Scree plot","text":"Examine the singular values of Legendre basis L. Clearly L is not semiunitary, and the last ~15 values are very small. So fitting with more than ~80 components will be very unstable, even if all M samples were available.\n\nscatter(svdvals(L), xaxis = (\"k\", (0,100), 0:10:100), ylabel = L\"Ïƒ_k\")\n\nExamine orthogonality of the basis functions\n\nThe Legendre polynomials are orthogonal in Lâ‚‚-11, but the following correlation figure shows that they are not orthogonal when sampled.\n\npc = jim(p, p, L'L, \"correlation\")\n\nEvaluate OLS solutions for increasing k\n\nThe training error decreases monotonically with polynomial degree\n\nerrors = zeros(P,3)\nfor k in 1:P\n    A = L[:, 1:k]\n    xhat = A[train,:] \\ y[train]\n    residual = A*xhat - y\n    errors[k,1] = norm(residual[train])\n    errors[k,2:3] .= norm.((residual[train], residual[test]), Inf)\nend\nptrain = scatter(p, 100*errors[:,1]/norm(y[train]), title=\"NRMSE training\",\n xlabel=\"Polynomial degree\")\n\nThe test error exhibits double descent\n\nptest = plot(p, 100*errors[:,3]/norm(y[train]), title=\"NRMSE test\",\n marker=:dot,\n xlabel = \"Polynomial degree\",\n yaxis = (\"NRMSE (%)\", (0, 100), ),\n)\n\nShow fits for small, medium and large polynomial degree\n\npfit = plot(\n xaxis = (\"x\", (-1,1), -1:1),\n yaxis = (\"y\", (-1,1) .* 1.5, -1:1),\n)\nscatter!(t, y)\nfor k in (2, 10, 99)\n   A = L[:, 1:k]\n   xhat = A[train,:] \\ y[train]\n   plot!(t, A * xhat, label=\"k=$k\")\nend\npfit","category":"section"},{"location":"generated/demos/05/double-descent/#Reproducibility","page":"Double Descent in LS","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/12/round1/#rmt-round1","page":"Roundoff errors and rank","title":"Roundoff errors and rank","text":"This example examines the effects of roundoff error associated with finite-precision arithmetic on matrix rank and singular value calculations, using the Julia language. The focus is rank-1 matrices.\n\nThis page comes from a single Julia file: round1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: round1.ipynb, or open it in binder here: round1.ipynb.\n\nAdd the Julia packages that are need for this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages for this example. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: rank, svd, svdvals, Diagonal, norm\nusing MIRTjim: prompt\nusing Plots: default, gui, plot, plot!, scatter, scatter!, savefig, histogram\nusing Plots.PlotMeasures: px\nusing StatsBase: mean, var\ndefault(markerstrokecolor=:auto, label=\"\", widen=true, markersize = 6,\n tickfontsize = 12, labelfontsize = 18, legendfontsize = 18, linewidth=2,\n)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/12/round1/#Roundoff-errors-and-singular-values","page":"Roundoff errors and rank","title":"Roundoff errors and singular values","text":"Examine the singular values of a matrix that ideally should have rank=1.\n\nn = 100\nT = Float32\nu = ones(T, n) / T(sqrt(n))\nv = ones(T, n) / T(sqrt(n))\nY = 1 * u * v' # theoretically rank-1 matrix\n@assert rank(Y) == 1 # julia is aware of precision limits\nsigma = svdvals(Y)\nsigma[1], abs(sigma[1] - 1)\n\nxaxis = (L\"k\", (1,n), [1, nÃ·2, n])\np1 = scatter(sigma; xaxis, yaxis = (L\"Ïƒ_k\", (-0.02, 1.02), -1:1), color=:blue)\ngood = findall(sigma .> 0)\nxaxis = (L\"k\", (1,n), [2, nÃ·2, n])\np2 = scatter(good, log10.(sigma[good]);\n    xaxis, yaxis = (L\"\\log_{10}(Ïƒ_k)\", (-40, 2), -40:20:0), color=:red)\np12 = plot(p1, p2, layout=(2,1))\n# savefig(p12, \"round1.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/12/round1/#Roundoff-errors-and-rank","page":"Roundoff errors and rank","title":"Roundoff errors and rank","text":"For a matrix that is sufficiently large relative to the precision of the matrix elements, the threshold in the rank function can be high enough that the returned rank is 0.\n\nusing LinearAlgebra: rank, svdvals\nn = 1100 # > 1 / eps(Float16)\nT = Float16\nu = ones(T, n) / T(sqrt(n))\nv = ones(T, n) / T(sqrt(n))\nY = u * v' # theoretically rank-1 matrix\nrank(Y) # 0 !\n\nThe following plot shows the singular values and the threshold set in the rank function. There is a gap with a ratio of 10^5 between Ïƒâ‚ = 1 and Ïƒâ‚‚ so rank's threshold is unnecessarily conservative in this case.\n\nRMT suggests that the (smaller) tolerance max(MN)  Îµ  Ïƒâ‚ can suffice, and indeed in this particular example it correctly separates the nonzero signal singular value from the noise singular values.\n\nHere the matrix Y nicely fits the assumptions of RMT; there may be other situations with \"worst case\" data matrices where the conservative threshold is needed.\n\nWe have noticed that this plot looks different on a Mac and on Linux with Julia 1.9.2. Apparently some differences in the SVD libraries on different systems can affect the details of the tiny singular values.\n\ns = svdvals(Y)\ntol = minimum(size(Y)) * eps(T) * s[1] # from rank()\ntol2 = sqrt(maximum(size(Y))) * eps(T) * s[1] # from RMT\np16 = plot([1, n], [1,1] * log10(tol),\n label=\"rank threshold: tol=$(round(tol,digits=2))\",\n title = \"Rank-1 matrix with $T elements\",\n)\nplot!([1, n], [1,1] * log10(tol2),\n label=\"RMT threshold: tol=$(round(tol2,digits=2))\")\nscatter!(1:n, log10.(s); label=\"singular values\", alpha=0.8,\n xaxis = (L\"k\", (1,n), [2, nÃ·2, n]),\n yaxis = (L\"\\log_{10}(Ïƒ)\", (-45, 2), [-40:20:0; -5]),\n left_margin = 40px, bottom_margin = 20px,\n annotate = (200, -8, Sys.MACHINE, :left),\n)\n\nprompt()\n# savefig(p16, \"round1-p16-$(Sys.MACHINE).pdf\")","category":"section"},{"location":"generated/demos/12/round1/#Roundoff-error-variance","page":"Roundoff errors and rank","title":"Roundoff error variance","text":"N = 200\n\nvery high-precision reference, var=1\n\nX = (2 * rand(BigFloat, N, N) .- 1) * sqrt(3)\nfor T in (Float16, Float32, Float64)\n    local Y = T.(X) # quantize\n    local Z = T.(Y - X) # quantization error\n    @show T, mean(Z) # approximately 0\n    vr = var(Float64.(Z)) # sample variance\n    @show (vr, eps(T)^2/24) # empirical, predicted\nend","category":"section"},{"location":"generated/demos/12/round1/#Roundoff-error-plot","page":"Roundoff errors and rank","title":"Roundoff error plot","text":"T = Float32\nY = T.(X) # quantize\nZ = T.(Y - X); # quantization error\nnothing #hide\n\nExamine histogram of floating point quantization errors\n\nph = histogram(vec(Z), bins = (-40:40)/40 * eps(T))\n\nprompt()\n\nExamine floating point quantization errors\n\nx = range(BigFloat(-1), BigFloat(1), 1001) * 2\nz = T.(x) - x # quantization error\nylabel = latexstring(\"error: \\$\\\\ (q(x) - x)/Ïµ\\$\")\nscatter(x, z / eps(T), yaxis=(ylabel, (-1,1).*0.51, (-2:2)*0.25))\nplot!(x, (@. eps(T(x)) / eps(T) / 2), label=L\"Ïµ(x)/2\", color=:blue)\npq = plot!(x, x/2, xaxis=(L\"x\",), label=L\"x/2\", legend=:top, color=:red)\n\nprompt()\n\nBased on the quantization error plot above, the quantization error for a floating point number near x is bounded above by Ïµ x  2. See the Julia manual for eps. Thus if x  mathrmUnif(-aa) then Ez^2 = Eq(x) - x^2 = frac12a _-a^a q(x) - x^2 mathrmd x leq frac1a _0^a Ïµ x  2^2 mathrmd x = Ïµ^2 a^2  12","category":"section"},{"location":"generated/demos/12/round1/#Reproducibility","page":"Roundoff errors and rank","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/05/frame-cycle/#frame-cycle","page":"Wavelet frame denoising","title":"Wavelet frame denoising","text":"This example illustrates image denoising using a frame defined by a combination of an orthonormal discrete wavelet transform (ODWT) and \"cycle-spinning\" operators, using the Julia language.\n\nThis page comes from a single Julia file: frame-cycle.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: frame-cycle.ipynb, or open it in binder here: frame-cycle.ipynb.","category":"section"},{"location":"generated/demos/05/frame-cycle/#Setup","page":"Wavelet frame denoising","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"LinearMapsAA\"\n        \"MIRT\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"Statistics\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ImagePhantoms: circle, phantom\nusing InteractiveUtils: versioninfo\n# using LaTeXStrings\nusing LinearAlgebra: norm\nusing LinearMapsAA: LinearMapAA\nusing MIRT: Aodwt #, pogm_restart\nusing MIRTjim: jim, prompt\nusing Plots: default, gui, plot, savefig\nusing Random: seed!\ndefault(); default(markerstrokecolor=:auto, label = \"\", markersize=6,\n tickfontsize = 9, labelfontsize = 16, titlefontsize = 16)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/05/frame-cycle/#Generate-test-image","page":"Wavelet frame denoising","title":"Generate test image","text":"Both clean and noisy image.\n\nif !@isdefined(ydata)\n    nx, ny = 144, 128 # multiples of 2^3 for wavelet code\n    ob = circle(40f0, 6f0)\n    x = (1:nx) .- (nx-1)/2\n    y = (1:ny) .- (ny-1)/2\n    oversample = 3\n    xtrue = phantom(x, y, [ob], oversample)\n    seed!(0)\n    ydata = xtrue + 0.7f0 * randn(Float32, nx, ny)\n    nrmse = (xh) -> round(norm(xh - xtrue) / norm(xtrue) * 100, digits=1)\nend;\n\nclim = (-2, 8)\nclim = (-1, 7)\npy = jim(\n jim(xtrue; clim, title=\"True Image\"),\n jim(ydata; clim, xlabel=\"NRMSE=$(nrmse(ydata))%\", title=\"Noisy Image\"),\n size = (800, 350),\n)\n# savefig(py, \"frame-cycle-py.pdf\")\n\nOrthogonal discrete wavelet transform operator (LinearMapAO):\n\nW, scales, _ = Aodwt((nx,ny) ; T = eltype(ydata), level = 3)\nscales = Int.(scales)\nisdetail = scales .> 0\npw = jim(\n   jim(scales, \"wavelet scales\"; color=:viridis),\n   jim(real(W * xtrue) .* isdetail, \"wavelet detail coefficients\";\n       color=:cividis),\n   size = (800, 320),\n)","category":"section"},{"location":"generated/demos/05/frame-cycle/#ODTW-denoising","page":"Wavelet frame denoising","title":"ODTW denoising","text":"This simply uses soft thresholding, the proximal operator for the 1-norm.\n\nDefine proximal operator so that it shrinks only the detail coefficients:\n\nsoft = (z,c) -> sign(z) * max(abs(z) - c, 0) # soft thresholding\nreg = 0.9 # hand-tuned for small NRMSE\ng_prox = (z,c) -> soft.(z, isdetail .* (reg * c))\n\n# Apply wavelet coefficient soft thresholding\ncoef = W * ydata\nxhat1 = W' * g_prox(coef, 1)\njim(coef)\np1 = jim(xhat1; clim, xlabel=\"NRMSE=$(nrmse(xhat1))%\", title=\"ODWT denoised\")\n\nThe NRMSE is reduced substantially, but there are severe \"block\" artifacts due to the dyadic decomposition of the ODWT.","category":"section"},{"location":"generated/demos/05/frame-cycle/#Frame-approach","page":"Wavelet frame denoising","title":"Frame approach","text":"Define a frame based on combining ODWT with K circshift operations. The analysis operator is\n\nmathbfT = frac1sqrtK\nbeginbmatrix\nmathbfW mathbfP_1 \nmathbfW mathbfP_2  vdots \nmathbfW mathbfP_K\nendbmatrix\n\nwhere mathbfP_0 = mathbfI and where each mathbfP_k is a circshift operator.\n\n# Define circshift permutation map\nPforw = shifts -> (x -> circshift(x, shifts))\nPback = shifts -> (y -> circshift(y, -1 .* shifts))\nPmap = shifts -> LinearMapAA(Pforw(shifts), Pback(shifts), (nx*ny,nx*ny);\n    odim=(nx,ny), idim=(nx,ny), T=Float32, prop=(; shifts, name=\"shift\"))\n\np12 = Pmap((1,2))\n@assert p12' * (p12 * ydata) â‰ˆ ydata # check Pmap\n\n# tmp = W * p12\n# tmp * xtrue # todo fails!?\n# Top = vcat([W * Pmap((xs,ys)) for xs in shifts, ys in shifts]...)\n\n# Define Parseval tight frame analysis operator\nshifts = -3:3\nPmaps = [Pmap((xs,ys)) for xs in shifts, ys in shifts]\nK = length(Pmaps)\nTforw = x -> stack(k -> (W * (Pmaps[k] * x)) / sqrt(K), 1:K, dims=3)\nTback = y -> sum(k -> Pmaps[k]' * (W' * y[:,:,k]), 1:K) / sqrt(K)\nTop = LinearMapAA(Tforw, Tback, (nx*ny*K, nx*ny);\n    odim=(nx,ny,K), idim=(nx,ny), T=Float32, prop=(; name=\"Top\"))\n\n# Sanity check that the operator satisfies the tight frame condition:\n@assert Top' * (Top * ydata) â‰ˆ ydata","category":"section"},{"location":"generated/demos/05/frame-cycle/#Parseval-tight-frame-(PTF)-denoising","page":"Wavelet frame denoising","title":"Parseval tight frame (PTF) denoising","text":"The tight frame approach leads to lower NRMSE and reduces the block artifacts.\n\ntodo: describe cost functions and implement POGM\n\nxhat2 = Top' * g_prox(Top * ydata, 0.2) # todo: hand-tuned again\np2 = jim(xhat2; clim, xlabel=\"NRMSE=$(nrmse(xhat2))%\", title=\"PTF denoised\")\npf = jim(p1, p2; size=(800,350))\n\n# savefig(pf, \"frame-cycle-pf.pdf\")","category":"section"},{"location":"generated/demos/05/frame-cycle/#Reproducibility","page":"Wavelet frame denoising","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/05/ls-cost1/#ls-cost1","page":"LS cost functions","title":"LS cost functions","text":"This example illustrates linear least-squares (LS) cost functions and minimum-norm LS (MNLS) solutions using the Julia language.\n\nThis page comes from a single Julia file: ls-cost1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: ls-cost1.ipynb, or open it in binder here: ls-cost1.ipynb.","category":"section"},{"location":"generated/demos/05/ls-cost1/#Setup","page":"LS cost functions","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: norm, pinv\nusing MIRTjim: prompt\nusing Plots: default, contour, plot!, scatter!, savefig\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto, markersize=6, linewidth=2,\n    xlims = (-3,3), ylims = (-3,3), aspect_ratio=:equal, size=(450,400),\n    legendfontsize=12, guidefontsize=13, tickfontsize=10, labelfontsize=18)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/05/ls-cost1/#Under-determined-case","page":"LS cost functions","title":"Under-determined case","text":"A = [1 2] # 1st case: M < N\ny = [3] # obviously x=[1 1] is one possible solution (but not MNLS)\nf1(x) = norm(A*x - y)\nxh1 = A \\ y\n\nx1 = range(-1,1,101) * 3\nx2 = range(-1,1,103) * 3\nc1 = [f1([x1a, x2a]) for x1a in x1, x2a in x2];\n\nflabel(n) = L\"\\{%$bx : y_{%$n} = %$(bA)_{[%$n,:]} %$bx\\}\"\nbx = \"\\\\mathit{\\\\mathbf{x}}\"\nby = \"\\\\mathit{\\\\mathbf{y}}\"\nbA = \"\\\\mathit{\\\\mathbf{A}}\"\ncolor = :viridis\np1 = contour(x1, x2, c1', label=\"contours\"; color)\nplot!(xlabel=L\"x_1\", ylabel=L\"x_2\", legend=:bottomleft)\nscatter!([0], [0], color=:black, markershape=:square)\nplot!([0, xh1[1]], [0, xh1[2]], line=:magenta)\nplot!(x1, (y[1] .- A[1,1]*x1)/A[1,2], line=(:blue,:dash),\n    label=L\"\\{%$bx : %$by = %$bA %$bx\\}\")\nscatter!([1], [1], color=:blue, markershape=:star5, label=L\"(1,1)\")\nscatter!([xh1[1]], [xh1[2]], color=:red, markershape=:circle, label=\"MNLS\",\n    title = \"Under-determined case\")\n\nprompt()\n\n# savefig(p1, \"demo_ls_cost1a.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cost1/#Square-but-singular-case","page":"LS cost functions","title":"Square but singular case","text":"A = [1 2; 2 4] # 2nd case: M = N but singular A\ny = [3, 6] # again [1,1] is a solution (but not MNLS solution)\nf2(x) = norm(A*x - y)\nxh2 = pinv(A) * y\n\nc2 = [f2([x1a, x2a]) for x1a in x1, x2a in x2]\np2 = contour(x1, x2, c2', label=\"contours\"; color)\nplot!(xlabel=L\"x_1\", ylabel=L\"x_2\", legend=:bottomleft)\nscatter!([0], [0], color=:black, markershape=:square)\nplot!([0, xh2[1]], [0, xh2[2]], line=:magenta)\nplot!(x1, (y[1] .- A[1,1]*x1)/A[1,2], line=(:blue,:dash), label=flabel(1))\nplot!(x1, (y[2] .- A[2,1]*x1)/A[2,2], line=(:green,:dashdot), label=flabel(2))\nscatter!([1], [1], color=:blue, markershape=:star5, label=L\"(1,1)\")\nscatter!([xh2[1]], [xh2[2]], color=:red, markershape=:circle, label=\"MNLS\",\n title = \"Singular case\")\n\nprompt()\n\n# savefig(p2, \"demo_ls_cost1b.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cost1/#Square-non-singular-case","page":"LS cost functions","title":"Square non-singular case","text":"A = [1 2; 1 3] # 3rd case: M = N with non-singular A\ny = [3, 4] # now x=[1,1] is the unique solution (by design)\nf3(x) = norm(A*x - y)\nxh3 = A \\ y\n\nc3 = [f3([x1a, x2a]) for x1a in x1, x2a in x2]\np3 = contour(x1, x2, c3', label=\"contours\"; color)\nplot!(xlabel=L\"x_1\", ylabel=L\"x_2\", legend=:bottomleft)\nplot!(x1, (y[1] .- A[1,1]*x1)/A[1,2], line=(:blue,:dash), label=flabel(1))\nplot!(x1, (y[2] .- A[2,1]*x1)/A[2,2], line=(:green,:dash), label=flabel(2))\nscatter!([xh3[1]], [xh3[2]], color=:red, markershape=:circle, label=\"LLS\",\n title = \"Non-singular case\")\n\nprompt()\n\n# savefig(p3, \"demo_ls_cost1c.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cost1/#Typical-over-determined-case","page":"LS cost functions","title":"Typical over-determined case","text":"A = [1 2; 1 -1; 2 1] # 4th case: M > N with (typical) inconsistent data\ny = [3, 2, 1] # no consistent solution\nf4(x) = norm(A*x - y)\nxh4 = A \\ y\n\nc4 = [f4([x1a, x2a]) for x1a in x1, x2a in x2]\np4 = contour(x1, x2, c4', label=\"contours\"; color)\nplot!(xlabel=L\"x_1\", ylabel=L\"x_2\", legend=:bottomleft)\nplot!(x1, (y[1] .- A[1,1]*x1)/A[1,2], line=(:blue,:dash), label=flabel(1))\nplot!(x1, (y[2] .- A[2,1]*x1)/A[2,2], line=(:green,:dash), label=flabel(2))\nplot!(x1, (y[3] .- A[3,1]*x1)/A[3,2], line=(:purple,:dash), label=flabel(3))\nscatter!([xh4[1]], [xh4[2]], color=:red, markershape=:circle, label=\"LLS\",\n    title = \"Over-determined case\")\n\nprompt()\n\n# savefig(p4, \"demo_ls_cost1d.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cost1/#Reproducibility","page":"LS cost functions","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/04/svd-diff/#svd-diff","page":"SVD of finite differences","title":"SVD of finite differences","text":"This example illustrates the SVD of a first-order finite-difference matrix using the Julia language. This demo was inspired by Gilbert Strang's 2006 article.\n\nThis page comes from a single Julia file: svd-diff.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: svd-diff.ipynb, or open it in binder here: svd-diff.ipynb.","category":"section"},{"location":"generated/demos/04/svd-diff/#Setup","page":"SVD of finite differences","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: norm, I, diag, diagm, Diagonal\nusing MIRTjim: jim, prompt\nusing Plots: default, plot, scatter\ndefault(); default(label=\"\", markerstrokecolor=:auto, color=:blue, widen=:true)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/demos/04/svd-diff/#First-order-finite-difference-matrix","page":"SVD of finite differences","title":"First-order finite-difference matrix","text":"N = 16\nÎ” = diagm(0 => -ones(Int,N-1), 1 => ones(Int,N-1))[1:(N-1),:] # (N-1,N) diff\njim(Î”'; title=\"$(N-1) Ã— $N finite-difference matrix Î”\", color=:cividis)","category":"section"},{"location":"generated/demos/04/svd-diff/#Right-singular-vectors-are-cos-functions","page":"SVD of finite differences","title":"Right singular vectors are cos functions","text":"h = Ï€ / N\nv = k -> cos.(((1:N)*2 .- 1) * k * h / 2) / sqrt(N/2)\n\nplot(v(5), line=:stem, marker=:circle, title=\"5th right singular vector\",\n xaxis = (L\"i\", (1,N), 1:N), yaxis=(L\"v_5[i]\", (-0.5,0.5), (-1:1)*0.5))\n\nprompt()","category":"section"},{"location":"generated/demos/04/svd-diff/#Left-singular-vectors-are-sin-functions","page":"SVD of finite differences","title":"Left singular vectors are -sin functions","text":"u = k -> -sin.((1:(N-1)) * k * h) / sqrt(N/2) # \"derivative of cos is -sin\"\n\nplot(u(5), line=:stem, marker=:circle, title=\"5th left singular vector\",\n xaxis = (L\"i\", (1,N), 1:N-1), yaxis=(L\"u_5[i]\", (-0.5,0.5), (-1:1)*0.5))\n\nprompt()","category":"section"},{"location":"generated/demos/04/svd-diff/#Singular-values","page":"SVD of finite differences","title":"Singular values","text":"","category":"section"},{"location":"generated/demos/04/svd-diff/#(Caution:-not-in-descending-order)","page":"SVD of finite differences","title":"(Caution: not in descending order)","text":"Ïƒ = k -> 2*sin(k*h/2)\nk = 1:(N-1)\n\nscatter(k, Ïƒ.(k), title=\"$(N-1) singular values (unordered)\",\n color=:red, xaxis=(L\"k\", (1,N-1), 1:N-1), yaxis=(L\"Ïƒ_k\", (0,2), 0:2))\n\nprompt()","category":"section"},{"location":"generated/demos/04/svd-diff/#SVD-components","page":"SVD of finite differences","title":"SVD components","text":"V = hcat([v(k) for k in 1:(N-1)]...) # (N,N-1) \"V_{N-1}\" DCT\nU = hcat([u(k) for k in 1:(N-1)]...) # (N-1,N-1) DST\nÎ£ = Diagonal(Ïƒ.(1:(N-1))) # (N-1,N_1) Î£_N\n\njim(\n jim(U', \"U: Left singular vectors\"; color=:cividis),\n jim(V', \"V: Right singular vectors\"; color=:cividis),\n jim(Î£', \"Î£: Singular values\"; color=:cividis),\n)","category":"section"},{"location":"generated/demos/04/svd-diff/#Verify-correctness-of-SVD","page":"SVD of finite differences","title":"Verify correctness of SVD","text":"@assert all(>(0), diag(Î£)) # singular values are nonnegative\n@assert Î” * V â‰ˆ U * Î£ # \"derivative of cos is -sin\"\n@assert V'V â‰ˆ I(N-1) # V is semi-unitary\n@assert U'U â‰ˆ I(N-1) && U*U' â‰ˆ I(N-1) # U is unitary\n@assert Î” â‰ˆ U * Î£ * V' # SVD of Î”","category":"section"},{"location":"generated/demos/04/svd-diff/#Reproducibility","page":"SVD of finite differences","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/rank1/#rank-1","page":"Rank-1 approximation","title":"Rank-1 approximation","text":"This example illustrates rank-1 approximations using the Julia language.\n\nThis page comes from a single Julia file: rank1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: rank1.ipynb, or open it in binder here: rank1.ipynb.","category":"section"},{"location":"generated/demos/07/rank1/#Setup","page":"Rank-1 approximation","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: svd, rank\nusing MIRTjim: prompt\nusing Plots: default, plot!, scatter, scatter!, savefig\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto,\n    guidefontsize=14, legendfontsize=14, tickfontsize=12)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/07/rank1/#Generate-data","page":"Rank-1 approximation","title":"Generate data","text":"Noisy data with slope=1.  Both x and y values are noisy!\n\nseed!(0)\nx0 = 1:8 # true x locations\nx = x0 + 2*randn(size(x0)) # called \"errors in variables\"\ny = x0 + 2*randn(size(x0)); # noisy samples\nnothing #hide\n\nPlotting utility function\n\nlineplot = (p, s, c, l; w=3, t=:dash) ->\n    plot!(p, 0:10, (0:10)*s, line=(c,t), label=l, width=w)\nfunction plotdata()\n    p = scatter(x, y, label=\"data\", legend=:bottomright,\n        color=:blue, markersize=7, aspect_ratio=:equal,\n        xaxis = (L\"x\", (0, 10), 0:4:8),\n        yaxis = (L\"y\", (0, 10), 0:4:8),\n    )\n    lineplot(p, 1, :red, \"true\", t=:solid, w=2)\nend\npl = plotdata()\n\nprompt()","category":"section"},{"location":"generated/demos/07/rank1/#Rank-1-approximation","page":"Rank-1 approximation","title":"Rank-1 approximation","text":"To make a low-rank approximation, collect data into a matrix\n\nA = [x'; y']\n\nExamine singular values\n\nU, s, V = svd(A)\ns # 2nd singular value is much smaller than 1st\n\nConstruct rank-1 approximation\n\nB = U[:,1] * s[1] * V[:,1]' # rank-1 approximation\nrank(B)\n\nB","category":"section"},{"location":"generated/demos/07/rank1/#Plot-rank-1-approximation","page":"Rank-1 approximation","title":"Plot rank-1 approximation","text":"xb = B[1,:]\nyb = B[2,:]\n\nlineplot(pl, (xb\\yb)[1], :black, \"\")\nscatter!(pl, xb, yb, color=:black, markersize=5, marker=:square, label=\"rank1\")\n\nprompt()","category":"section"},{"location":"generated/demos/07/rank1/#Use-least-squares-estimation-to-estimate-slope:","page":"Rank-1 approximation","title":"Use least-squares estimation to estimate slope:","text":"slope = y'*x / (x'*x) # cf inv(A'A) * A'b\nslope = (x \\ y)[1] # cf A \\ b","category":"section"},{"location":"generated/demos/07/rank1/#Plot-the-LS-fit-and-the-low-rank-approximation-on-same-graph","page":"Rank-1 approximation","title":"Plot the LS fit and the low-rank approximation on same graph","text":"pa = lineplot(pl, slope, :green, \"LS\")\n\nprompt()\n\n# savefig(pa, \"06_low_rank1_all.pdf\")","category":"section"},{"location":"generated/demos/07/rank1/#Illustrate-the-Frobenius-norm-approximation-error-graphically","page":"Rank-1 approximation","title":"Illustrate the Frobenius norm approximation error graphically","text":"pf = plotdata()\nfor i in 1:length(xb)\n    plot!(pf, [x[i], xb[i]], [y[i], yb[i]], color=:black, width=2)\nend\nlineplot(pf, (xb\\yb)[1], :black, \"\")\nscatter!(pf, xb, yb, color=:black, markersize=5, marker=:square, label=\"rank1\")\n\nprompt()\n\n# savefig(pf, \"06_low_rank1_r1.pdf\")","category":"section"},{"location":"generated/demos/07/rank1/#Illustrate-the-LS-residual-graphically","page":"Rank-1 approximation","title":"Illustrate the LS residual graphically","text":"xl = x; yl = slope*xl # LS points\nps = plotdata()\nfor i in 1:length(x)\n    plot!(ps, [x[i], xl[i]], [y[i], yl[i]], color=:green, width=2)\nend\nlineplot(ps, slope, :green, \"\")\nscatter!(ps, xl, yl, color=:green, markersize=5, marker=:square, label=\"LS\")\n\nprompt()\n\n# savefig(ps, \"06_low_rank1_ls.pdf\")","category":"section"},{"location":"generated/demos/07/rank1/#Reproducibility","page":"Rank-1 approximation","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/source-local/#source-local","page":"Source localization","title":"Source localization","text":"This example illustrates source localization via multi-dimensional scaling using the Julia language.\n\nThis page comes from a single Julia file: source-local.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: source-local.ipynb, or open it in binder here: source-local.ipynb.","category":"section"},{"location":"generated/demos/07/source-local/#Setup","page":"Source localization","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Optim\"\n        \"Plots\"\n        \"Random\"\n        \"Statistics\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: svd, norm, Diagonal\nusing MIRTjim: jim, prompt\nusing Optim: optimize\nusing Plots: default, scatter, savefig\nusing Random: seed!\nusing Statistics: mean\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true,\n    guidefontsize=14, tickfontsize=12, legendfontsize=14)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nisinteractive() && jim(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/07/source-local/#Generate-data","page":"Source localization","title":"Generate data","text":"Coordinates - can you identify the pattern?  probably not...\n\nC0 = [\n3 2.5 2 2 2 2 2 1 0 0 1 1 1 1 0 0 1 2 2.5 3  4 4.5 5 6 7 7 6 6 6 6 7 7 6 5 5 5 5 5 4.5 4;\n2 3.0 4 3 2 1 0 0 0 1 1 2 3 4 4 5 5 5 4.0 3  3 4.0 5 5 5 4 4 3 2 1 1 0 0 0 1 2 3 4 3.0 2\n];\nnothing #hide\n\nInterpolate the locations to make the final set more familiar\n\nC2 = (C0[:,2:end] + C0[:,1:end-1])/2\nCe = (C0[:,1] + C0[:,end])/2\nC3 = [C2 Ce]\nC4 = [C0; C3]\nC = reshape(C4, 2, :);\nnothing #hide","category":"section"},{"location":"generated/demos/07/source-local/#Compute-J-J-distance-array-and-display-it","page":"Source localization","title":"Compute J Ã— J distance array and display it","text":"J = size(C,2) # number of points\nD = [norm(C[:,j] - C[:,i]) for i in 1:J, j in 1:J] # \"comprehension\" in julia!\npd = jim(D, L\"D\", color=:cividis, xlabel=L\"j\", ylabel=L\"i\")\n\n# savefig(pd, \"06_source_local1_d.pdf\")","category":"section"},{"location":"generated/demos/07/source-local/#MDS-algorithm","page":"Source localization","title":"MDS algorithm","text":"Compute Gram matrix by de-meaning squared distance matrix\n\nS = D.^2 # squared distances\nG = S .- mean(S, dims=1) # trick: use \"broadcasting\" feature of julia\nG = G .- mean(G, dims=2) # now we have de-meaned the columns and rows of S\nG = -1/2 * G;\nnothing #hide\n\nWe still cannot determine visually the point locations:\n\npg = jim(G, L\"G\", color=:cividis, xlabel=L\"j\", ylabel=L\"i\")\n\n# savefig(pg, \"06_source_local1_g.pdf\")\n\nExamine singular values\n\n(_, Ïƒ, V) = svd(G) # svd returns singular values in descending order\nps = scatter(Ïƒ, label=\"singular values (noiseless case)\",\n    xlabel=L\"k\", ylabel=L\"Ïƒ_k\") # two nonzero (d=2)\n\nprompt()\n\n# savefig(ps, \"06_source_local1_eig.pdf\")","category":"section"},{"location":"generated/demos/07/source-local/#Estimate-the-source-locations-using-rank2","page":"Source localization","title":"Estimate the source locations using rank=2","text":"Ch = Diagonal(sqrt.(Ïƒ[1:2])) * V[:,1:2]' # here is the key step","category":"section"},{"location":"generated/demos/07/source-local/#Plot-estimated-source-locations","page":"Source localization","title":"Plot estimated source locations","text":"pc = scatter(Ch[1,:], -Ch[2,:], xtick=-4:4, ytick=-3:3, aspect_ratio=1,\n title=\"Location estimates (noiseless case)\")\n\nprompt()","category":"section"},{"location":"generated/demos/07/source-local/#Noisy-case","page":"Source localization","title":"Noisy case","text":"seed!(0)\nDn = D + 0.3 * randn(size(D))\nSn = Dn.^2\nGn = Sn .- mean(Sn, dims=1)\nGn = Gn .- mean(Gn, dims=2) # de-meaned\nGn = -1/2 * Gn\npgn = jim(Gn, \"G noisy\", color=:cividis)\n\nSingular values\n\n(_, sn, Vn) = svd(Gn)\npsn = scatter(sn, label=\"singular values (noisy case)\") # Ïƒâ‚‚ â‰« Ïƒâ‚ƒ\n\nprompt()","category":"section"},{"location":"generated/demos/07/source-local/#Plot-estimated-source-locations-from-noisy-distance-measurements","page":"Source localization","title":"Plot estimated source locations from noisy distance measurements","text":"Cn = Diagonal(sqrt.(sn[1:2])) * Vn[:,1:2]'\npcn = scatter(Cn[1,:], -Cn[2,:], xtick=-4:4, ytick=-3:3, aspect_ratio=1,\n title=\"Location estimates (noisy case)\")\n\nprompt()\n\nNonlinear LS fitting / optimization approach to noisy case. This cost function was considered in de Leeuw, 1988.\n\nDfun(C) = [norm(C[:,j] - C[:,i]) for i in 1:J, j in 1:J]\ncost(C) = norm(Dfun(C) - Dn)^2\noutp = optimize(cost, Cn)\nCf = outp.minimizer\npcf = scatter(Cf[1,:], -Cf[2,:], xtick=-4:4, ytick=-3:3, aspect_ratio=1,\n title=\"Location estimates (noisy case - fitted)\")\n\nprompt()\n\nCompare fitting approach and SVD-based approach to the noiseless distance matrix. The fitted approach is closer to the noiseless D.\n\nDfits = norm(D - Dfun(C)), norm(D - Dfun(Ch)), norm(D - Dfun(Cn)), norm(D - Dfun(Cf))\nround.(Dfits, digits=2)\n\nCompare fitting approach and SVD-based approach to the noiseless coordinates. Probably there should be a permutation here? In principle there should be a Procrustes rotation here to make the comparison more meaningful. The fitted approach is closer to the noiseless coordinates.\n\n(Un,_,Vn) = svd(Ch * Cn')\n(Uf,_,Vf) = svd(Ch * Cf')\nround.( [\n norm(Cn - Ch),\n norm(Ch - Un*Vn'*Cn),\n norm(Ch - Cf),\n norm(Ch - Uf*Vf'*Cf) ],\n digits = 3,\n)","category":"section"},{"location":"generated/demos/07/source-local/#Constant-bias-case","page":"Source localization","title":"Constant bias case","text":"seed!(0)\nDb = D .+ 0.3 * maximum(D) # fairly large bias\nSb = Db.^2\nGb = Sb .- mean(Sb, dims=1)\nGb = Gb .- mean(Gb, dims=2) # de-meaned\nGb = -1/2 * Gb\npgb = jim(Gb, \"G biased\", color=:cividis)\n\nSingular values\n\n(_, sb, Vb) = svd(Gb)\npsb = scatter(sb, label=\"singular values (biased case)\") # Ïƒâ‚‚ â‰« Ïƒâ‚ƒ\n\nprompt()","category":"section"},{"location":"generated/demos/07/source-local/#Plot-estimated-source-locations-from-biased-distance-measurements","page":"Source localization","title":"Plot estimated source locations from biased distance measurements","text":"Cb = Diagonal(sqrt.(sb[1:2])) * Vb[:,1:2]'\npcb = scatter(Cb[1,:], -Cb[2,:], xtick=-4:4, ytick=-3:3, aspect_ratio=1,\n title=\"Location estimates (biased case)\")\n\nprompt()","category":"section"},{"location":"generated/demos/07/source-local/#Equilateral-triangle-example","page":"Source localization","title":"Equilateral triangle example","text":"G = [-2 1 1; 1 -2 1; 1 1 -2] / (-6.)\n(~, Ïƒ, V) = svd(G)\nCh = Diagonal(sqrt.(Ïƒ[1:2])) * V[:,1:2]'\nscatter(Ch[1,:], Ch[2,:], aspect_ratio=1, title=\"Location estimates\")\n\nprompt()","category":"section"},{"location":"generated/demos/07/source-local/#Reproducibility","page":"Source localization","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/01/2-vector/#tutor-2-vector","page":"Tutorial: Vectors in Julia","title":"Tutorial: Vectors in Julia","text":"Vectors in Julia differ a bit from Matlab. In Matlab, everything is an array, including vectors (and even scalars). In Julia, there are distinct data types for scalars, vectors, rowvectors, and 1D arrays. This tutorial illustrates the differences.\n\nJeff Fessler, University of Michigan\n2017-07-24, original\n2020-08-05, Julia 1.5.0\n2021-08-23, Julia 1.6.2\n2023-08-03, Julia 1.9.2, Literate\n\nThis page comes from a single Julia file: 2-vector.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 2-vector.ipynb, or open it in binder here: 2-vector.ipynb.","category":"section"},{"location":"generated/demos/01/2-vector/#Scalars,-Vectors,-Arrays","page":"Tutorial: Vectors in Julia","title":"Scalars, Vectors, Arrays","text":"a = 4 # this is a scalar\n\ntypeof(a)\n\nb1 = [4] # this is a Vector with one element\n\nb2 = reshape([4], 1, 1) # here is a 1Ã—1 Array\n\nb3 = reshape([4], 1, 1, 1) # here is a 1Ã—1Ã—1 Array\n\nIn Julia the following all differ! (In Matlab they are the same.)\n\na==b1, b1==b2, a==b2, b2==b3","category":"section"},{"location":"generated/demos/01/2-vector/#Vectors-and-Transpose","page":"Tutorial: Vectors in Julia","title":"Vectors and Transpose","text":"This construction (with just spaces) makes a 1Ã—3 Matrix:\n\nc = [4 5 6]\n\nThis construction (using commas) makes a 1D Vector:\n\nd = [4, 5, 6]\n\nSo does this construction, whereas in Matlab the \",\" and \";\" work differently:\n\ne = [4; 5; 6]\n\nThe transpose of a Vector is slightly different than a 1Ã—N array! This is a subtle point!\n\nd'\n\nNevertheless, the values are the same:\n\nd' == c\n\nTransposing back gives a vector again (not a NÃ—1 array):\n\n(d')'\n\nThese are all true, as expected, despite the adjoint type:\n\nd==e, d'==c, (c')'==d'\n\nThese are all false:\n\nc==d,  c==e,  c'==d,  (d')'==c'\n\nAn \"inner product\" of a 1Ã—3 Matrix with a 3Ã—1 Matrix returns a 1Ã—1 Matrix, not a scalar:\n\nc * c'\n\nThis inner product of an adjoint Matrix with a Vector returns a scalar:\n\nd' * d\n\nHow to make a vector from an array:\n\nvec(c)\n\nHere is another way (but it uses more memory than vec):\n\nc[:]","category":"section"},{"location":"generated/demos/01/2-vector/#Call-by-reference","page":"Tutorial: Vectors in Julia","title":"Call by reference","text":"Julia uses call-by-reference (not value), like C/C++, unlike Matlab!\n\nHere B is the same \"pointer\" so this changes A:\n\nA = zeros(2); B = A; B[1] = 7\nA\n\nHere B is different, so this does not change A:\n\nA = zeros(2); B = A .+ 2; B[1] = 7\nA\n\nThis changes A because B and A point to same data:\n\nA = B = zeros(2); B[1] = 7\nA\n\nThis changes B for the same reason:\n\nA = B = zeros(2); A[1] = 7\nB\n\nTo avoid this issue, one can use copy;\n\nA = zeros(2); B = copy(A); B[1] = 7; # B here uses different memory than A\nA # here it is unchanged\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/12/gauss1/#rmt-gauss1","page":"Random matrix theory and rank-1 signal + noise","title":"Random matrix theory and rank-1 signal + noise","text":"This example compares results from random matrix theory with empirical results for rank-1 matrices with additive white Gaussian noise using the Julia language. This demo illustrates the phase transition that occurs when the singular value is sufficiently large relative to the matrix aspect ratio c = MN.\n\nThis page comes from a single Julia file: gauss1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: gauss1.ipynb, or open it in binder here: gauss1.ipynb.\n\nAdd the Julia packages that are need for this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages for this example. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: dot, rank, svd, svdvals\nusing MIRTjim: prompt, jim\nusing Plots: default, gui, plot, plot!, scatter!, savefig, histogram\nusing Plots.PlotMeasures: px\nusing Random: seed!\nusing StatsBase: mean, var\ndefault(markerstrokecolor=:auto, label=\"\", widen=true, markersize = 6,\n labelfontsize = 24, legendfontsize = 18, tickfontsize = 14, linewidth = 3,\n)\nseed!(0)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/12/gauss1/#Helper-functions","page":"Random matrix theory and rank-1 signal + noise","title":"Helper functions","text":"Generate random data for one trial:\n\nfunction gen1(\n    Î¸::Real = 3,\n    M::Int = 100,\n    N::Int = 2M,\n    T::Type{<:Real} = Float32,\n)\n    u = randn(T, M) / T(sqrt(M))\n    v = randn(T, N) / T(sqrt(N))\n    X = Î¸ * u * v' # theoretically rank-1 matrix\n    Z = randn(T, M, N) / T(sqrt(N)) # gaussian noise\n    Y = X + Z\n    return Y, u, v, Î¸\nend;\nnothing #hide\n\nSVD results for 1 trial:\n\nfunction trial1(args...)\n    Y, u, v, Î¸ = gen1(args...)\n    fac = svd(Y)\n    Ïƒ1 = fac.S[1]\n    u1 = fac.U[:,1]\n    v1 = fac.Vt[1,:]\n    return [Ïƒ1, abs2(dot(u1, u)), abs2(dot(v1, v))]\nend;\nnothing #hide\n\nAverage nrep trials:\n\ntrial2(nrep::Int, args...) = mean((_) -> trial1(args...), 1:nrep);\nnothing #hide\n\nSVD for each of multiple trials, for different SNRs and matrix sizes:\n\nif !@isdefined(vgrid)\n\n    # Simulation parameters\n    T = Float32\n    Mlist = [30, 300]\n    Î¸max = 4\n    nÎ¸ = Î¸max * 4 + 1\n    nrep = 100\n    Î¸list = T.(range(0, Î¸max, nÎ¸));\n    labels = map(n -> latexstring(\"\\$M = $n\\$\"), Mlist)\n\n    c = 1 # square matrices for simplicity\n    c4 = c^0.25\n    tmp = ((Î¸, M) -> trial2(nrep, Î¸, M, ceil(Int, M/c) #= N =#)).(Î¸list, Mlist')\n    Ïƒgrid = map(x -> x[1], tmp)\n    ugrid = map(x -> x[2], tmp)\n    vgrid = map(x -> x[3], tmp)\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/12/gauss1/#Results","page":"Random matrix theory and rank-1 signal + noise","title":"Results","text":"Compare theory predictions and empirical results. There is again notable agreement between theory and empirical results here.\n\nÏƒ1 plot\n\ncolors = [:orange, :red]\nÎ¸fine = range(0, Î¸max, 40Î¸max + 1)\nsbg(Î¸) = Î¸ > c4 ? sqrt((1 + Î¸^2) * (c + Î¸^2)) / Î¸ : 1 + âˆš(c)\nstheory = sbg.(Î¸fine)\nbm = s -> \"\\\\mathbf{\\\\mathit{$s}}\"\nylabel = latexstring(\"\\$Ïƒ_1($(bm(:Y)))\\$ (Avg)\")\nps = plot(Î¸fine, Î¸fine, color=:black,\n    aspect_ratio = 1, linewidth = 2,\n    xaxis = (L\"Î¸\", (0,Î¸max), 0:Î¸max),\n    yaxis = (ylabel, (1,Î¸max), 1:Î¸max),\n    annotate = (2.1, 3.6, latexstring(\"c = $c\"), :left),\n)\nplot!(Î¸fine, stheory, color=:blue, label=\"theory\")\nscatter!(Î¸list, Ïƒgrid[:,1], marker=:square, color=colors[1], label = labels[1])\nscatter!(Î¸list, Ïƒgrid[:,2], marker=:circle, color=colors[2], label = labels[2])\n\nprompt()\n\nu1 plot\n\nubg(Î¸) = (Î¸ > c4) ? 1 - c * (1 + Î¸^2) / (Î¸^2 * (Î¸^2 + c)) : 0\nutheory = ubg.(Î¸fine)\nylabel = latexstring(\"\\$|âŸ¨\\\\hat{$(bm(:u))}, $(bm(:u))âŸ©|^2\\$ (Avg)\")\npu = plot(Î¸fine, utheory, color=:blue, label=\"theory\",\n    left_margin = 10px, legend = :bottomright,\n    xaxis = (L\"Î¸\", (0,Î¸max), 0:Î¸max),\n    yaxis = (ylabel, (0,1), 0:0.5:1),\n)\nscatter!(Î¸list, ugrid[:,1], marker=:square, color=colors[1], label = labels[1])\nscatter!(Î¸list, ugrid[:,2], marker=:circle, color=colors[2], label = labels[2])\n\nprompt()\n\nv1 plot\n\nvbg(Î¸) = (Î¸ > c^0.25) ? 1 - (c + Î¸^2) / (Î¸^2 * (Î¸^2 + 1)) : 0\nvtheory = vbg.(Î¸fine)\nylabel = latexstring(\"\\$|âŸ¨\\\\hat{$(bm(:v))}, $(bm(:v))âŸ©|^2\\$ (Avg)\")\npv = plot(Î¸fine, vtheory, color=:blue, label=\"theory\",\n    left_margin = 10px, legend = :bottomright,\n    xaxis = (L\"Î¸\", (0,Î¸max), 0:Î¸max),\n    yaxis = (ylabel, (0,1), 0:0.5:1),\n)\nscatter!(Î¸list, vgrid[:,1], marker=:square, color=colors[1], label = labels[1])\nscatter!(Î¸list, vgrid[:,2], marker=:circle, color=colors[2], label = labels[2])\n\nprompt()\n\n\nif false\n    savefig(ps, \"gauss1-s.pdf\")\n    savefig(pu, \"gauss1-u.pdf\")\n    savefig(pv, \"gauss1-v.pdf\")\nend","category":"section"},{"location":"generated/demos/12/gauss1/#MarÄenkoâ€“Pastur-distribution","page":"Random matrix theory and rank-1 signal + noise","title":"MarÄenkoâ€“Pastur distribution","text":"Examine the singular values of the noise-only matrix Z. having elements z_ij  N(0 1N). and compare to the asymptotic prediction by the MarÄenkoâ€“Pastur distribution. The agreement is remarkably good, even for a modest matrix size of 100 Ã— 100.\n\nMarÄenkoâ€“Pastur pdf\n\nfunction mp_predict(x::Real, c::Real)\n    Ïƒm = 1 - sqrt(c)\n    Ïƒp = 1 + sqrt(c)\n    return (Ïƒm < x < Ïƒp) ?\n        sqrt(4c - (x^2 - 1 - c)^2) / (Ï€ * c * x) : 0.\nend;\n\n\nfunction mp_plot(M::Int, N::Int, rando::Function, name::String;\n    ntrial = 150,\n    bins = range(0, 2, 101),\n)\n    c = M//N\n    pred = mp_predict.(bins, c)\n    pmax = ceil(maximum(pred), digits=1)\n    data = [svdvals(rando()) for _ in 1:ntrial]\n    data = reduce(hcat, data)\n    Ïƒm = 1 - sqrt(c)\n    Ïƒp = 1 + sqrt(c)\n    xticks = (c == 1) ? (0:2) : round.([0, Ïƒm, 1, Ïƒp, 2]; digits=2)\n    cstr = c == 1 ? L\"c = 1\" : latexstring(\"c = $(c.num)/$(c.den) $name\")\n    histogram(vec(data); bins, linewidth=0,\n     xaxis = (L\"Ïƒ\", (0, 2), xticks),\n     yaxis = (\"\", (0, 2.0), [-1, 0, pmax]),\n     label = \"Empirical\", normalize = :pdf,\n     left_margin = 10px,\n     annotate = (0.1, 1.5, cstr, :left),\n    )\n    return plot!(bins, pred, label=\"Predicted\")\nend;\n\nM = 100\nNlist = [1, 4, 9] * M\nfun1 = N -> mp_plot(M, N, () -> randn(M, N) / sqrt(N), \"\")\npp = fun1.(Nlist)\np3 = plot(pp...; layout=(3,1), size=(600,800))\n\n# savefig(p3, \"gauss-mp.pdf\")\n# savefig(pp[2], \"gauss-mp-c4.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/12/gauss1/#Universality","page":"Random matrix theory and rank-1 signal + noise","title":"Universality","text":"Repeat the previous experiment with a (zero-mean) Bernoulli distribution.\n\nM = 100\nN = 4 * M\nrandb = () -> rand((-1,1), M, N) / sqrt(N) # Bernoulli, variance 1/N\nif false\n    tmp = randb()\n    @show mean(tmp) # check mean is 0\n    @show mean(abs2, tmp), 1/N # check variance is 1/N (exact!)\nend\npb = mp_plot(M, N, randb, \", \\\\mathrm{Bernoulli}\")\n\nprompt()\n\nShow a typical Bernoulli matrix realization\n\npb0 = jim(randb()', \"'Bernoulli' matrix\"; clim = (-1,1) .* 0.05,\n size=(600,200), right_margin = 30px, cticks=(-1:1)*0.05)","category":"section"},{"location":"generated/demos/12/gauss1/#Sparsity","page":"Random matrix theory and rank-1 signal + noise","title":"Sparsity","text":"Universality can break down if the data is too sparse. Here we modify Bernoulli to be a categorical distribution with values (-a 0 a) and probabilities ((1-p)2 p (1-p)2), with a set so that the variance is 1N.\n\nHere we set p so that most of the random matrix elements are zero. In this extremely sparse case, the MarÄenkoâ€“Pastur distribution no longer applies.\n\nM = 100\nN = 4 * M\np = (1 - 8/N) # just a few non-zero per row\n\nrands = () -> rand((-1,1), M, N) / sqrt(N * (1-p)) .* (rand(M,N) .> p)\nif false\n    tmp = rands()\n    @show count(==(0), tmp) / (M*N), p\n    @show mean(tmp) # check mean is 0\n    @show mean(abs2, tmp), 1/N # check variance is 1/N (exact!)\nend\n\nShow a typical matrix realization to illustrate the sparsity\n\npb1 = jim(rands()', \"Very sparse 'Bernoulli' matrix\";\n size=(600,200), right_margin = 20px)\n\nNow make the plot\n\npss = mp_plot(M, N, rands, \", \\\\mathrm{Sparse},  p = $p\")\n\nprompt()","category":"section"},{"location":"generated/demos/12/gauss1/#Reproducibility","page":"Random matrix theory and rank-1 signal + noise","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/align1/#align1","page":"Image alignment by rank-1 method","title":"Image alignment by rank-1 method","text":"This example illustrates 2D and 3D image alignment (estimating a simple translation between an image pair) using a rank-1 approximation to the normalized cross power spectrum, following the method of Hoge, IEEE T-MI, 2003, using the Julia language.\n\nThis page comes from a single Julia file: align1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: align1.ipynb, or open it in binder here: align1.ipynb.","category":"section"},{"location":"generated/demos/07/align1/#Setup","page":"Image alignment by rank-1 method","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"ImagePhantoms\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"Statistics\"\n        \"Unitful\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing ImageGeoms: ImageGeom, axesf\nusing ImagePhantoms: SheppLoganEmis, spectrum, phantom\nusing ImagePhantoms: ellipse, ellipse_parameters\nusing ImagePhantoms: ellipsoid, ellipsoid_parameters\nusing LaTeXStrings\nusing LinearAlgebra: norm, svd\nusing MIRTjim: jim, prompt\nusing Plots: default, gui, plot, plot!, scatter, scatter!, savefig\nusing Random: seed!\nusing Unitful: cm, @u_str # use of physical units (cm here)\ndefault(); default(label=\"\", markerstrokecolor=:auto,\n    guidefontsize=14, legendfontsize=14, tickfontsize=12)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/07/align1/#Generate-data","page":"Image alignment by rank-1 method","title":"Generate data","text":"FOV = 256cm # physical units\nNx, Ny = 128, 126\nÎ”x = FOV / Nx # pixel size\nÎ”y = FOV / Ny\nx = ((-NxÃ·2):(NxÃ·2-1)) * Î”x\ny = ((-NyÃ·2):(NyÃ·2-1)) * Î”y\nÎ½x = ((-NxÃ·2):(NxÃ·2-1)) / Nx / Î”x\nÎ½y = ((-NyÃ·2):(NyÃ·2-1)) / Ny / Î”y;\nnothing #hide\n\nEllipse parameters for 1st image:\n\nparam1 = ellipse_parameters(SheppLoganEmis(), fovs=(FOV,FOV))\nshift2 = (1.7, 9.2) .* oneunit(Î”x) # true non-integer shift\n\nEllipse parameters for 2nd image:\n\nparam2 = [((p[1:2] .+ shift2)..., p[3:end]...) for p in param1];\nnothing #hide\n\nPhantom images:\n\nobj1 = ellipse(param1)\nobj2 = ellipse(param2)\nimage1 = phantom(x, y, obj1)\nimage2 = phantom(x, y, obj2)\npim = jim(\n jim(x, y, image1, \"image1\"),\n jim(x, y, image2, \"image2\"),\n jim(x, y, image2-image1, \"image2-image1\")\n)\n\nprompt()\n\nAnalytical spectra of these images (cf MRI)\n\nspec1 = spectrum(Î½x, Î½y, obj1)\nspec2 = spectrum(Î½x, Î½y, obj2);\nnothing #hide\n\nAdd noise\n\nseed!(0)\nsnr2sigma(db, y) = 10^(-db/20) * norm(y) / sqrt(length(y))\nÏƒnoise = snr2sigma(40, spec1)\naddnoise(y, Ïƒ) = y + Ïƒ * randn(ComplexF32, size(y))\ndata1 = addnoise(spec1, Ïƒnoise)\ndata2 = addnoise(spec2, Ïƒnoise);\nnothing #hide\n\nNormalized cross power spectrum\n\nncps = @. data1 * conj(data2) / (abs(data1 * data2));\nnothing #hide\n\nShow spectra and noisy phase difference\n\nfun = data -> log10.(abs.(data) / maximum(abs, data1))\npsp = jim(\n jim(Î½x, Î½y, fun(data1), \"|spectrum1|\"),\n jim(Î½x, Î½y, fun(data2), \"|spectrum2|\"),\n jim(Î½x, Î½y, angle.(ncps); color = :hsv, title=\"phase difference\")\n)\n\nprompt()","category":"section"},{"location":"generated/demos/07/align1/#SVD","page":"Image alignment by rank-1 method","title":"SVD","text":"By the shift property of the 2D Fourier transform, the normalized cross power spectrum (NCPS) is\n\ne^Ä± 2Ï€ (Î½_x d_x + Î½_y d_y)\n=\ne^Ä± 2Ï€ Î½_x d_x\ne^Ä± 2Ï€ Î½_y d_y\n\nwhere (d_x d_y) is the 2D translation.\n\nSo the 2D NCPS (in the absence of noise) is an outer product of 1D vectors, each of which has the phase associated with the translation in one direction.\n\nU, s, V = svd(ncps)\npsig = scatter(s, xlabel=L\"k\", ylabel=L\"Ïƒ_k\", title=\"Scree plot\")\n\nprompt()\n\nPhase of principal components\n\nu = U[:,1]\nv = conj(V[:,1]) # need conjugate here because of Ïƒ u v'\npuv = plot(\n plot(Î½x, angle.(u), xlabel=L\"Î½_x\"),\n plot(Î½y, angle.(v), xlabel=L\"Î½_y\"),\n)\n\nWe could unwrap the phase and then fit a line as suggested in the original paper.\n\nInstead, we use a phase slope estimate described in Feiweier 2013 US Patent 8497681B2 that avoids any need for phase unwrapping.\n\nfunction phase_slope(x::AbstractVector, Î”Î½::Number; weights = 1)\n   tmp = x[2:end] .* conj(x[1:end-1])\n   return angle(sum(weights .* tmp)) / (2Ï€ * Î”Î½)\nend;\n\nmyshift2 = phase_slope.((u,v), 1/FOV)\n_round(x) = round(u\"cm\", x; digits=4)\n_round.(myshift2)\n\nError: the estimated shift is remarkably close to the true shift.\n\nerror2 = myshift2 .- shift2\n_round.(error2)","category":"section"},{"location":"generated/demos/07/align1/#3D-case","page":"Image alignment by rank-1 method","title":"3D case","text":"Here is an illustration of an extension of the method to 3D image registration.\n\nEllipsoid parameters for 1st image volume:\n\nfovs = (24cm, 24cm, 20cm)\nparam1 = ellipsoid_parameters( ; fovs)\nob1 = ellipsoid(param1); # Vector of Ellipsoid objects\nnothing #hide\n\nEllipsoid parameters for 2nd image volume:\n\nshift3 = (1.1, 2.2, 3.3) .* oneunit.(fovs)\nparam2 = [((p[1:3] .+ shift3)..., p[4:end]...) for p in param1];\nob2 = ellipsoid(param2);\nnothing #hide\n\nVisualize\n\ndims = (128, 130, 30)\nig = ImageGeom( ; dims, deltas = fovs ./ dims )\noversample = 3\nimage1 = phantom(axes(ig)..., ob1, oversample)\nimage2 = phantom(axes(ig)..., ob2, oversample)\nclim = (0.95, 1.05)\nplot(\n jim(axes(ig)[1:2]..., image2; title = \"3D Shepp-Logan phantom slices\", clim),\n jim(axes(ig)[1:2]..., image2-image1; title = \"Difference\", clim),\n)\n\nprompt()\n\nSpectra\n\nspectrum1 = spectrum(axesf(ig)..., ob1)\nspectrum2 = spectrum(axesf(ig)..., ob2);\n\nÏƒnoise = snr2sigma(40, spectrum1)\ndata1 = addnoise(spectrum1, Ïƒnoise)\ndata2 = addnoise(spectrum2, Ïƒnoise);\nnothing #hide\n\nNormalized cross power spectrum\n\nncps = @. data1 * conj(data2) / (abs(data1 * data2));\nnothing #hide\n\nShow spectra and noisy phase difference\n\nfun = data -> log10.(abs.(data / maximum(abs, data1)))\niz = dims[3]Ã·2 .+ (0:1)\npsp = jim(\n jim(axesf(ig)[1:2]..., fun(data1)[:,:,iz], \"|spectrum1|\"),\n jim(axesf(ig)[1:2]..., angle.(ncps[:,:,iz]), color = :hsv, title=\"phase difference\"),\n)\n\nprompt()","category":"section"},{"location":"generated/demos/07/align1/#SVD-for-3-different-foldings-of-the-3D-NCPS","page":"Image alignment by rank-1 method","title":"SVD for 3 different foldings of the 3D NCPS","text":"This follows the folded NCPS method of Hoge et al., ICIP 2003, described therein in terms of the dominant singular vectors of a high-order SVD of the tensor data.\n\nfold1 = reshape(ncps, dims[1], :)\nU, s, V = svd(fold1)\nu1 = U[:,1]\npsig1 = scatter(s, xlabel=L\"k\", ylabel=L\"Ïƒ_k\", title=\"Scree plot\")\npa1 = plot(angle.(u1));\n\nfold2 = reshape(permutedims(ncps, [2 1 3]), dims[2], :)\nU, s, V = svd(fold2)\nu2 = U[:,1]\npsig2 = scatter(s, xlabel=L\"k\", ylabel=L\"Ïƒ_k\", title=\"Scree plot\")\npa2 = plot(angle.(u2));\n\nfold3 = reshape(permutedims(ncps, [3 1 2]), dims[3], :)\nU, s, V = svd(fold3)\nu3 = U[:,1]\npsig3 = scatter(s, xlabel=L\"k\", ylabel=L\"Ïƒ_k\", title=\"Scree plot\")\npa3 = plot(angle.(u3));\n\np3 = plot(\n psig1, psig2, psig3,\n pa1, pa2, pa3,\n layout = (2, 3),\n)\n\nprompt()\n\nEstimate translation\n\nÎ”Î½ = map(i -> diff(axesf(ig)[i])[1], 1:3)\nmyshift3 = phase_slope.((u1,u2,u3), Î”Î½)\n_round.(myshift3)\n\nError is small:\n\nerror3 = myshift3 .- shift3\n_round.(error3)","category":"section"},{"location":"generated/demos/07/align1/#Reproducibility","page":"Image alignment by rank-1 method","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/01/1-intro/#tutor-1-intro","page":"Tutorial: Julia Overview","title":"Tutorial: Julia Overview","text":"Julia overview.\n\n2018-08-11 Julia 0.7.0 Jeff Fessler (based on 2017 version by David Hong)\n2019-01-20 Julia 1.0.3 and add note about line breaks\n2020-08-05 Julia 1.5.0\n2021-08-23 Julia 1.6.2\n2023-09-03 Julia 1.9.2, Literate\n\nThis page comes from a single Julia file: 1-intro.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 1-intro.ipynb, or open it in binder here: 1-intro.ipynb.","category":"section"},{"location":"generated/demos/01/1-intro/#Setup","page":"Tutorial: Julia Overview","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LinearAlgebra\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\n# using InteractiveUtils: versioninfo\nusing LinearAlgebra: Diagonal, det, dot, tr","category":"section"},{"location":"generated/demos/01/1-intro/#Numbers,-arithmetic,-types","page":"Tutorial: Julia Overview","title":"Numbers, arithmetic, types","text":"Define a real number:\n\nr = 3.0\n\nVariables in Julia have a type:\n\ntypeof(r)\n\ni = 3\n\ntypeof(i)\n\nc = 3. + 2im\n\ntypeof(c)\n\nWe can add, subtract, multiply and divide like usual:\n\n4. + 5\n\n4. - 5\n\n4. * 3\n\n2. / 3\n\nDividing Int values with / produces a Float:\n\n2/3\n\n4/2\n\nThis is different from Python 2, but similar to Python 3.\n\nTo divide integers with rounding, use Ã· instead. Type \\div then hit tab:\n\n5 Ã· 2\n\nMore info about numbers here:\n\nnumbers\noperations\ncomplex","category":"section"},{"location":"generated/demos/01/1-intro/#Vectors-and-matrices-(i.e.,-arrays)","page":"Tutorial: Julia Overview","title":"Vectors and matrices (i.e., arrays)","text":"Make a vector of real numbers:\n\nx = beginbmatrix 10  35  2 endbmatrix\n\nx = [1, 3.5, 2]\n\nNote the type: Vector{Float64}.\n\nHaving just one real number in the array sufficed for the array have all Float64 elements.\n\nThis is a true one-dimensional array of Float64 values.\n\n(Matlab does not have 1D arrays; it fakes it using 2D arrays of size N Ã— 1.)\n\nsize(x) # returns a tuple\n\nlength(x)\n\nx_ints = [1,3,2]\n\nThis is a one-dimensional array of Int64 values. We use these less often.\n\nsize(x_ints)\n\nlength(x_ints)","category":"section"},{"location":"generated/demos/01/1-intro/#Make-a-matrix-using-a-semicolon-to-separate-rows:","page":"Tutorial: Julia Overview","title":"Make a matrix using a semicolon to separate rows:","text":"A = beginbmatrix\n11  12  13 \n21  22  23\nendbmatrix\n\nA = [1.1 1.2 1.3; 2.1 2.2 2.3]\n\nThis is a two-dimensional array (aka a matrix) of Float64 values.\n\nsize(A)\n\nlength(A)\n\nDifferent from Matlab, length always returns the total number of elements.\n\nMake vectors and matrices of all zeros.\n\nzeros(3)\n\nDifferent from Matlab! Do not write zeros(3,1) because Julia has proper 1D arrays. zeros(3,1) and zeros(3) are different!\n\nzeros(2,3)\n\nAnd ones:\n\nones(3)\n\nones(2,3)\n\nThe \"identity matrix\" I in Julia's LinearAlgebra package is sophisticated.\n\nLook at the following examples:\n\nusing LinearAlgebra: I\nones(3,3) - I\n\nones(2,2) * I\n\nI(3)\n\nIf that I seems too fancy, then you could make your own eye function akin to Matlab as follows (but it should not be needed and it uses unnecessary memory):\n\neye = n -> Matrix(1.0*I(n))\neye(2)\n\nMake diagonal matrices using the Diagonal function in LinearAlgebra:\n\nDiagonal(3:6)\n\nThis is far more memory efficient than Matlab's diag command or Julia's LinearAlgebra.diagm method.  Avoid using those!\n\nMake random vectors and matrices.\n\nx = beginbmatrix\n mathcalN(01)  mathcalN(01)  mathcalN(01)\n endbmatrix\nqquad textie\nquad x_i oversettextiidsim mathcalN(01)\n\nA = beginbmatrix\nmathcalN(01)  mathcalN(01)  mathcalN(01) \nmathcalN(01)  mathcalN(01)  mathcalN(01)\nendbmatrix\nqquad textie \nquad A_ij oversettextiidsim mathcalN(01)\n\nx = randn(3)\n\nA = randn(2,3)","category":"section"},{"location":"generated/demos/01/1-intro/#Matrix-operations","page":"Tutorial: Julia Overview","title":"Matrix operations","text":"Indexing is done with square brackets (like in C and Python, unlike Matlab).\n\nIndex and begins at 1 (like in Matlab and counting) not 0 (like in C or Python).\n\nA = [1.1 1.2 1.3; 2.1 2.2 2.3]\n\nA[1,1]\n\nA[1,2:3]\n\nThis row-slice is a one-dimensional slice (!) not a 1Ã—2 matrix:\n\nA[1:2,1]\n\nA[2,:]\n\nVector dot product:\n\nx = randn(3)\nxdx = x'x\n\nxdx = dot(x,x)\n\nxdx = x'*x\n\nDifferent from Matlab! The output is a scalar, not a 1Ã—1 \"matrix:\"\n\ntypeof(xdx)\n\nMatrix times vector:\n\nA = randn(2,3)\nx = randn(3)\nA*x\n\nMatrix times matrix:\n\nA = randn(2,3)\nB = randn(3,4)\nA*B\n\nMatrix transpose (conjugate and non-conjugate):\n\nA = 10*reshape(1:6, 2, 3) + im * reshape(1:6, 2, 3)\n\nconjugate transpose, could also use adjoint(A):\n\nA'\n\nFor complex arrays, rarely do we need a non-conjugate transpose. Usually we need A' instead.  But if we do:\n\ntranspose(A) # essentially sets a flag about transpose without reordering data\n\nMatrix determinant:\n\nA = Diagonal(2:4)\ndet(A)\n\nB = randn(3,3)\n[det(A*B) det(A)*det(B)]\n\nMatrix trace:\n\nA = ones(3,3)\ntr(A) # in Matlab would be \"trace(A)\"\n\nMore info in Julia manual","category":"section"},{"location":"generated/demos/01/1-intro/#Getting-help","page":"Tutorial: Julia Overview","title":"Getting help","text":"Julia analogue of Matlab's help is ?.\n\nType ?pwd in the REPL to get help on the pwd function.\n\nIt does not work in this online documentation so we use @doc instead:\n\n@doc pwd\n\nFull documentation\nSearching Julia's Github repo can sometimes also uncover similar issues.\nLots of neat talks on their YouTube channel\nHere is an interesting one about vector transpose","category":"section"},{"location":"generated/demos/01/1-intro/#Ranges","page":"Tutorial: Julia Overview","title":"Ranges","text":"Ranges are different from (and much more efficient than) Matlab!\n\nmyrange = -2:3\n\ntypeof(myrange)\n\nNot an Array! But it can be indexed:\n\nmyrange[1]\n\nUsed often in for loops:\n\nfor a in myrange\n    println(a)\nend\n\nForm an array by using collect if needed (use rarely):\n\ncollect(myrange)\n\nOther ways to make ranges:\n\nsrange = 1:-1:-5\n\ntypeof(srange)\n\nlrange = range(0, 2, 10)\n\ntypeof(lrange)\n\nYet another option that looks the most like linspace:\n\nLinRange(0,10,6)","category":"section"},{"location":"generated/demos/01/1-intro/#Comprehensions","page":"Tutorial: Julia Overview","title":"Comprehensions","text":"A convenient way to create arrays!\n\ncomp = [i+0.1 for i in 1:5]\n\ncomp = [10i + j for i in 1:5, j in 1:4]","category":"section"},{"location":"generated/demos/01/1-intro/#Defining-functions","page":"Tutorial: Julia Overview","title":"Defining functions","text":"Way 1:\n\nfunction f1(x,y)\n    z = x+y\n    return z\nend\n\nWay 2:\n\nf2(x,y) = x+y\n\nWay 3: Anonymous function:\n\nf3 = (x,y) -> x+y\n\nFunctions can return multiple outputs:\n\nfunction f_mult(x, y)\n    add = x + y\n    sub = x - y\n    return add, sub\nend;\n\nf_mult(2,3)\n\nThe output is a Tuple of the returned values:\n\nout_tuple = f_mult(2,3)\n\ntypeof(out_tuple)\n\nConvenient way to split out the outputs:\n\nout1, out2 = f_mult(2,3)\n\nout1\n\nout2","category":"section"},{"location":"generated/demos/01/1-intro/#Broadcast","page":"Tutorial: Julia Overview","title":"Broadcast","text":"Any Julia function can be \"vectorized\" using \"broadcast\"\n\nmyquad = x -> (x+1)^2\n\nmyquad(1)\n\ntry\n    myquad([1,2,3]) # this does not work!\ncatch\n    \"failed, as expected\"\nend\n\nThis particular function was not designed to be applied to vector input arguments! But it can be used with vectors (or arrays) by adding a . to tell Julia to apply it element-wise. This is called broadcasting.\n\nmyquad.([1,2,3])","category":"section"},{"location":"generated/demos/01/1-intro/#Conditionals","page":"Tutorial: Julia Overview","title":"Conditionals","text":"if else end for\n\nGenerally similar to Matlab. Optional use of in instead of = in the for loop.\n\nfor j in 1:3\n    if j == 2\n        println(\"$j is a two! ^^\")\n    else\n        println(\"$j is not a two. :(\")\n    end\nend\n\nJulia has the convenient ternary operator:\n\nmystring = 2 > 3 ? \"2 is greater than 3\" : \"2 is not greater than 3\"","category":"section"},{"location":"generated/demos/01/1-intro/#Plotting","page":"Tutorial: Julia Overview","title":"Plotting","text":"Suggested package: Plots.jl with its default gr backend.\n\nNote: Usually slower the first time you plot due to precompiling. You must add the \"Plots\" package first. In a regular Julia REPL you do this by using the ] key to enter the package manager REPL, and then type add Plots then wait.\n\nIn a Jupyter notebook, type using Pkg then add Plots and wait.\n\nusing Plots\nbackend()\n\nPlot values from a vector.  (The labels are optional arguments.)\n\nx = range(-5,5,101)\ny = x.^2\nplot(x, y, xlabel=\"x\", ylabel=\"y\", label=\"parabola\")","category":"section"},{"location":"generated/demos/01/1-intro/#heatmap","page":"Tutorial: Julia Overview","title":"heatmap","text":"x = range(-2, 2, 101)\ny = range(-1.1, 1.1, 103)\nA = x.^2 .+ 30 * (y.^2)'\nF = exp.(-A)\np1 = heatmap(x, y, F', # for F(x,y)\n    color=:grays, aspect_ratio=:equal, xlabel=\"x\", ylabel=\"y\", title=\"bump\")\n\nUsing the jim function the MIRTjim.jl package simplifies the display of 2D images, among other features. See its examples.","category":"section"},{"location":"generated/demos/01/1-intro/#Plotting-functions","page":"Tutorial: Julia Overview","title":"Plotting functions","text":"Plots.jl allows you to pass in the domain and a function. It does the rest. :) This is one many examples of how Julia exploits \"multiple dispatch.\"\n\nplot(range(0,1,100), abs2, label=\"x^2\")\n\nheatmap(range(-2,2,102), range(-1.1,1.1,100),\n    (x,y) -> exp(-x^2-30*y^2), aspect_ratio=1)\n\nMore info about plotting at https://juliaplots.github.io","category":"section"},{"location":"generated/demos/01/1-intro/#Caution:-line-breaks-(newlines)","page":"Tutorial: Julia Overview","title":"Caution: line breaks (newlines)","text":"If you want an expression to span multiple lines, then be sure to enclose it in parentheses.\n\nCompare the following 3 (actually 4) expressions:\n\nx = 9\n    - 7\n\ny = 9 -\n    7\n\nz = (9\n    - 7)\n\n(x,y,z)","category":"section"},{"location":"generated/demos/01/1-intro/#Submitting-homework","page":"Tutorial: Julia Overview","title":"Submitting homework","text":"This part is just for EECS 551 students at UM.\n\nA quick example to try submitting problems.\n\nTask: Implement a function that takes two inputs and outputs them in reverse order.\n\nfunction template1(x, y)\n    return (y, x)\nend;\n\ntemplate1(2, 3)\n\nCopy the above function code into a file named template1.jl and email to eecs551@autograder.eecs.umich.edu.\n\nMake sure that:\n\nAll reasonable input types can be handled. Internally trying to convert a Float64 to an Int64 can produce InexactError\nFile extension is .jl. Watch out for hidden extensions!\nFile has just the Julia function.\n(Your HW solutions can also contain using statements.)\n\nAn undocumented function is bad programming practice. Julia supports docstrings for comments like this:\n\n\"\"\"\n    template2(x,y)\nThis function reverses the order of the two input arguments.\n\"\"\"\nfunction template2(x,y)\n    return (y,x)\nend\n\nYou can see the docstring by using the ? key or @doc:\n\n@doc template2\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/06/procrustes/#procrustes","page":"Procrustes method","title":"Procrustes method","text":"This example illustrates the orthogonal Procrustes method using the Julia language.\n\nThis page comes from a single Julia file: procrustes.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: procrustes.ipynb, or open it in binder here: procrustes.ipynb.","category":"section"},{"location":"generated/demos/06/procrustes/#Setup","page":"Procrustes method","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LinearAlgebra: svd, norm, Diagonal\nusing MIRTjim: prompt\nusing Random: seed!\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/06/procrustes/#Coordinate-data","page":"Procrustes method","title":"Coordinate data","text":"Coordinates from rotated image example in Ch. 5 (n-05-norm/fig/)\n\nA = [-59 -25 49;\n    6 -33 20]\nB = [-54.1 -5.15 32.44;\n    -24.3 -41.08 41.82]","category":"section"},{"location":"generated/demos/06/procrustes/#Procrustes-solution-steps","page":"Procrustes method","title":"Procrustes solution steps","text":"C = B * A'\n\n(U,s,V) = svd(C)\ns\n\nQ = U * V'","category":"section"},{"location":"generated/demos/06/procrustes/#Fitting-residual","page":"Procrustes method","title":"Fitting residual","text":"residual = B - Q * A # small!\n\nRotation angle in degrees:\n\nacos(Q[1]) * (180/Ï€) # very close to 30Â° as expected","category":"section"},{"location":"generated/demos/06/procrustes/#Fitting-function","page":"Procrustes method","title":"Fitting function","text":"function procrustes(A, B)\n    C = B * A'\n    (U,s,V) = svd(C)\n    Q = U*V'\n    scale = sum(s) / norm(A,2)^2\n    return Q, scale\nend","category":"section"},{"location":"generated/demos/06/procrustes/#Explore-additional-special-cases","page":"Procrustes method","title":"Explore additional special cases","text":"","category":"section"},{"location":"generated/demos/06/procrustes/#Three-points-along-a-line,-symmetrical:","page":"Procrustes method","title":"Three points along a line, symmetrical:","text":"A = [-1 0 1; 0 0 0]\nB = [0 0 0; -2 0 2]\nQ, scale = procrustes(A, B)\n\nCheck:\n\n@assert B â‰ˆ scale * Q * A","category":"section"},{"location":"generated/demos/06/procrustes/#Three-points-along-a-line,-not-symmetrical:","page":"Procrustes method","title":"Three points along a line, not symmetrical:","text":"A = [-1 0 2; 0 0 0]\nB = [0 0 0; -2 0 4]\nQ, scale = procrustes(A, B)\n\nCheck:\n\n@assert B â‰ˆ scale * Q * A","category":"section"},{"location":"generated/demos/06/procrustes/#A-single-point-works-fine!","page":"Procrustes method","title":"A single point - works fine!","text":"A = [1; 0]\nB = [2; 2] # different length!\nQ, scale = procrustes(A, B)\n\nCheck:\n\n@assert B â‰ˆ scale * Q * A\n\nAngle:\n\nrad2deg(acos(Q[1]))\n\nExamine some other options for Q\n\n(U,s,V) = svd(B*A')\nQ1 = U*V'\n@assert B â‰ˆ scale * Q1 * A # same as above\n\nQ2 = U * Diagonal([1, 0]) * V' # (not unitary)\n\n@assert B â‰ˆ scale * Q2 * A # also works for this case!\n\nQ3 = U * Diagonal([1, -1]) * V' # is unitary\n\n@assert B â‰ˆ scale * Q3 * A # also works for this case!","category":"section"},{"location":"generated/demos/06/procrustes/#Examine-effect-of-noise","page":"Procrustes method","title":"Examine effect of noise","text":"seed!(0)\nÏƒ = 0.1\nAn = A + Ïƒ * randn(size(A))\nBn = B + Ïƒ * randn(size(B))\nQ_n, scale_n = procrustes(An, Bn)\n\nAngle:\n\nrad2deg(acos(Q_n[1]))","category":"section"},{"location":"generated/demos/06/procrustes/#Reproducibility","page":"Procrustes method","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#Demos-for-Book","page":"Home","title":"Demos for Book","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"https://github.com/JeffFessler/book-la-demo\n\nDemos in the Julia language, compiled using Literate and Documenter to accompany the 2024 book Linear Algebra for Data Science, Machine Learning, and Signal Processing by Jeff Fessler and Raj Nadakuditi at the University of Michigan.","category":"section"},{"location":"#Getting-started-with-Julia","page":"Home","title":"Getting started with Julia","text":"Install Julia from https://julialang.org\nLaunch the Julia app; it should open a Julia REPL.\nTo develop code, select an editor, preferably with Julia integration, such as VSCode or vim perhaps with tmux. Appropriate editor plug-ins are needed to use LaTeX-like tab-completion of unicode characters like Ã· âŠ— âŠ• âˆ˜ Ã— and Î± Î² Î³.\nPeruse the demos listed in the menu here. If your browser window is wide enough, you should see a menu to the left. If your window is narrow, you should see a â˜° hamburger menu button that will toggle open the demos menu sidebar.\nCheck out some Julia tutorials, especially the one titled \"Just the Julia you need to get started in Data Science and ML\" by Raj Rao.","category":"section"},{"location":"#More-resources","page":"Home","title":"More resources","text":"Install the web browser shortcut for fast access to the online Julia manual.\nUse the package AbbreviatedStackTraces.jl to get more interpretable error messages.\nUse the package Infiltrator.jl for convenient code debugging.\nFor image processing, view the excellent documentation at JuliaImages\nFor a machine learning introduction, see the Julia programming for Machine Learning course material.","category":"section"},{"location":"#Getting-started-with-Julia-for-matrix-methods","page":"Home","title":"Getting started with Julia for matrix methods","text":"These examples show you Julia code and the corresponding output in an HTML format suitable for viewing in a web browser without installing any software.\n\nYou could cut and paste portions of that Julia code into the Julia REPL, but that becomes tedious. Instead, click on the \"Edit on GitHub\" link (in the upper right, with github icon), where you can then download the entire Julia code file that generated any of these examples.\n\nFor example, the code for the SVD demo is at this url. After downloading such a file such as svd-demo.jl, you can run it by typing include(\"svd-demo.jl\") at the Julia REPL.","category":"section"},{"location":"generated/demos/05/ls-cv/#ls-cv","page":"LS fitting with cross validation","title":"LS fitting with cross validation","text":"This example illustrates least squares (LS) polynomial fitting, with cross validation for selecting the polynomial degree, using the Julia language.\n\nThis page comes from a single Julia file: ls-cv.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: ls-cv.ipynb, or open it in binder here: ls-cv.ipynb.","category":"section"},{"location":"generated/demos/05/ls-cv/#Setup","page":"LS fitting with cross validation","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Polynomials\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: norm\nusing MIRTjim: prompt\nusing Plots: default, plot, plot!, scatter, scatter!, savefig\nusing Polynomials: fit\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true, linewidth=2,\n    markersize = 6, tickfontsize=12, labelfontsize = 16, legendfontsize=14)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/05/ls-cv/#Simulated-data-from-latent-nonlinear-function","page":"LS fitting with cross validation","title":"Simulated data from latent nonlinear function","text":"f(x) = 0.5 * exp(1.8 * x) # nonlinear function\n\nseed!(0) # seed rng\nM = 12 # how many data points\nxm = sort(2*rand(M)) # M random sample locations\nÏƒ = 0.5 # noise standard deviation\nz = Ïƒ * randn(M) # noise\ny = f.(xm) + z # noisy samples\n\nx0 = range(0, 2, 501) # fine sampling for showing curve\nxaxis = (L\"x\", (0,2), 0:2)\nyaxis = (L\"y\", (-2, 21), 0:4:20)\np0 = scatter(xm, y, color=:black, label=\"y (noisy data), M = $M\"; xaxis, yaxis)\nplot!(x0, f.(x0), color=:blue, label=\"f(x) : latent function\", legend=:topleft)\n\nprompt()\n# savefig(p0, \"ls-cv-data.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cv/#Polynomial-fitting","page":"LS fitting with cross validation","title":"Polynomial fitting","text":"Illustrate polynomial fits with degrees that are too low, just right, and too high.\n\np1 = deepcopy(p0)\ndegs = [1, 3, 9]\nfor deg in degs\n    pol = fit(xm, y, deg)\n    plot!(p1, x0, pol.(x0), label = \"degree $deg\")\nend\np1\n\nprompt()\n# savefig(p1, \"ls-cv-fits.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cv/#Over-fitting-to-noisy-data","page":"LS fitting with cross validation","title":"Over-fitting to noisy data","text":"As the polynomial degree increases, the fit to the noisy data improves. In contrast, the error w.r.t. the latent function f initially decreases, but then increases as the model over-fits to the noise.\n\ndegs = 0:(M-1)\nfits = zeros(length(degs))\naccs = zeros(length(degs))\nfor (id, deg) in enumerate(degs)\n    pol = fit(xm, y, deg)\n    fits[id] = norm(pol.(xm) - y) # fit to noisy data\n    accs[id] = norm(pol.(xm) - f.(xm)) # \"accuracy\" w.r.t. true function\nend\npf = scatter(degs, fits; color=:red,\n xaxis = (\"degree\", extrema(degs), [0,3,11]),\n yaxis = (\"fits\", (0,8), ),\n label = (L\"â€– A_d \\hat{x}_d - y â€–_2\"),\n)\nscatter!(degs, accs;\n label = L\"â€– A_d \\hat{x}_d - f â€–_2\", marker=:uptri, color=:blue)\nplot!([extrema(degs)...], ones(2)*norm(f.(xm) - y),\n label = L\"â€– y - f â€–_2\", color=:green,)\n\nprompt()\n# savefig(pf, \"ls-cv-over.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cv/#Illustrate-uncertainty","page":"LS fitting with cross validation","title":"Illustrate uncertainty","text":"Leave out one point at a time, fit the remaining M-1 points with a degree 8 polynomial, and predict the held-out point.\n\ncolors = [:red, :orange, :yellow, :green, :cyan, :blue, :grey, :black]\npols = Vector{Any}(undef, M)\ndeg1 = 8\np2 = plot(; xaxis, yaxis, title = \"degree = $deg1\", legend=:top)\nscatter!(p2, [-9], [-9]; marker=:square, color=:gray, label=\"prediction\")\nscatter!(p2, [-9], [-9]; marker=:circle, color=:black, label=\"data\")\nfor m in 1:M\n    mm = (1:M)[[1:(m-1); (m+1):M]] # omit mth point\n    pols[m] = fit(xm[mm], y[mm], deg1)\n    color = colors[mod1(m, length(colors))]\n    plot!(p2, x0, pols[m].(x0); xaxis, yaxis, title = \"degree = $deg1\",\n     color,)\n    pred = pols[m](xm[m])\n    scatter!([xm[m]], [pred]; color, marker=:square)\n    scatter!([xm[m]], [y[m]]; color=:black)\n    plot!([1, 1]*xm[m], [y[m], pred]; color, line=:dash)\nend\np2\n\nprompt()\n# savefig(p2, \"ls-cv-uq.pdf\")","category":"section"},{"location":"generated/demos/05/ls-cv/#Cross-validation-(leave-one-out)","page":"LS fitting with cross validation","title":"Cross validation (leave-one-out)","text":"degs = 1:8\nerrs = zeros(length(degs), M)\nfor (id, deg) in enumerate(degs)\n    for m in 1:M\n        mm = (1:M)[[1:(m-1); (m+1):M]]\n        pol = fit(xm[mm], y[mm], deg)\n        errs[id, m] = pol(xm[m]) - y[m]\n    end\nend\ncv_loss = sqrt.(sum(abs2, errs, dims=2))\n\np3 = scatter(degs, cv_loss; legend = :top,\n xlabel = \"degree\",\n ylabel = \"error\",\n label = \"Cross-validation loss\",\n)\nscatter!(p3, degs, accs;\n label = L\"â€– A_d \\hat{x}_d - f â€–_2\", marker=:uptri, color=:blue)\n\nprompt()\n# savefig(p3, \"ls-cv-scat.pdf\")\n\nEstimate best polynomial degree using the cross validation loss.\n\nIn this case the estimate is degree=4, which happens to match the best degree in terms of 2-norm fit to the latent function f.\n\ncv_degree = degs[argmin(cv_loss)]\n\noracle_degree = degs[argmin(accs)]\n\nDiscrepancy principle\n\nAn alternative to cross validation to see the hyper-parameter (polynomial degree in this case) that makes\n\n A_d hatx_d - y _2  Ïƒ sqrtM\n\nThis is called the Discrepancy principle (DP) and its rationale is the fact that\n\nmathbbE  y - f _2^2 \n= mathbbE  Îµ _2^2 \n= Ïƒ^2 M\n\nwhen y = f + Îµ  mathbbR^M.\n\nThe DP approach requires that the user know the standard deviation Ïƒ of the elements of the noise vector Îµ, whereas cross-validation does not require that knowledge.\n\nIn this particular demo, the DP approach happens to pick the best degree=4, but in general DP is known to over-regularize.\n\ndp_degree = argmin(abs.(fits .- Ïƒ * sqrt(M)))","category":"section"},{"location":"generated/demos/05/ls-cv/#Reproducibility","page":"LS fitting with cross validation","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/10/nmf/#nmf1","page":"Non-negative matrix factorization","title":"Non-negative matrix factorization","text":"Non-negative matrix factorization of hand-written digit images in Julia.\n\nThis page comes from a single Julia file: nmf.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: nmf.ipynb, or open it in binder here: nmf.ipynb.","category":"section"},{"location":"generated/demos/10/nmf/#Setup","page":"Non-negative matrix factorization","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"MLDatasets\"\n        \"NMF\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LinearAlgebra: svd\nusing MIRTjim: jim, prompt\nusing MLDatasets: MNIST\nusing NMF: nnmf\nusing Plots: default, gui, savefig\ndefault(); default(markersize=5, markerstrokecolor=:auto, label=\"\",\n tickfontsize=14, labelfontsize=18, legendfontsize=18, titlefontsize=18)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/demos/10/nmf/#Load-data","page":"Non-negative matrix factorization","title":"Load data","text":"Read the MNIST data for some handwritten digits. This code will automatically download the data from web if needed and put it in a folder like: ~/.julia/datadeps/MNIST/.\n\nif !@isdefined(data)\n    digitn = 0:9 # which digits to use\n    isinteractive() || (ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true) # avoid prompt\n    dataset = MNIST(Float32, :train)\n    nrep = 100 # how many of each digit\n    # function to extract the 1st `nrep` examples of digit n:\n    data = n -> dataset.features[:,:,findall(==(n), dataset.targets)[1:nrep]]\n    data = cat(dims=4, data.(digitn)...)\n    labels = vcat([fill(d, nrep) for d in digitn]...) # to check later\n    nx, ny, nrep, ndigit = size(data)\n    data = data[:,2:ny,:,:] # make images non-square to force debug\n    ny = size(data,2)\n    size(data) # (nx, ny, nrep, ndigit)\nend\n\nLook at some of the image data\n\npd = jim(data[:,:,1:50,:], \"Data, M=$(nx*ny), N=$(nrep*ndigit)\";\n    colorbar=nothing, size=(600,400), tickfontsize=6, ncol=25)\n\n# savefig(pd, \"nmf-data.pdf\")","category":"section"},{"location":"generated/demos/10/nmf/#Run-NMF","page":"Non-negative matrix factorization","title":"Run NMF","text":"Y = reshape(data, nx*ny, :) # unfold\nK = 20\nout = nnmf(Y, K)","category":"section"},{"location":"generated/demos/10/nmf/#Results","page":"Non-negative matrix factorization","title":"Results","text":"Examine the left factor vectors as images.\n\nW = reshape(out.W, nx, ny, K)\n# H = out.H\npw = jim(W/maximum(W); title=\"Left NMF factor images, K=$K\", color=:cividis)\n\n# savefig(pw, \"nmf-w.pdf\")","category":"section"},{"location":"generated/demos/10/nmf/#SVD-basis","page":"Non-negative matrix factorization","title":"SVD basis","text":"Examine the left singular vectors as images.\n\nU = reshape(svd(Y).U[:,1:K], nx, ny, K)\npu = jim(U/maximum(U); title=\"Left singular vectors\", color=:cividis)\n\n# savefig(pu, \"nmf-u.pdf\")","category":"section"},{"location":"generated/demos/10/nmf/#Reproducibility","page":"Non-negative matrix factorization","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/08/eigmap/#eigmap","page":"Laplacian eigenmaps","title":"Laplacian eigenmaps","text":"This example illustrates Laplacian eigenmaps using the Julia language. These eigenmaps provide nonlinear dimensionality reduction for data lying near manifolds (rather than near subspaces).\n\nSee Belkin & Niyogi, 2003.\n\nThis page comes from a single Julia file: eigmap.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: eigmap.ipynb, or open it in binder here: eigmap.ipynb.","category":"section"},{"location":"generated/demos/08/eigmap/#Setup","page":"Laplacian eigenmaps","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"ImagePhantoms\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ImagePhantoms: rect, phantom\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: norm, Diagonal, eigen, svd, svdvals\nusing MIRTjim: jim, prompt\nusing Plots: default, gui, plot, savefig, scatter\nusing Plots.PlotMeasures: px\nusing Random: seed!\ndefault(); default(label = \"\", markerstrokecolor = :auto,\n tickfontsize = 12, labelfontsize = 16,\n)\nseed!(0)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/08/eigmap/#Generate-data","page":"Laplacian eigenmaps","title":"Generate data","text":"The example considered here uses synthetic data consisting of images of rectangles of various widths and rotations.\n\nThe only latent parameters here are one of the rectangle widths and the rotation angle. So all of the images lie in a manifold of dimension 2 in a 16^2 = 256 dimensional ambient space.\n\nfunction rect_phantom( ;\n    T = Float32,\n    nx = 40,\n    ny = 40,\n    sigma1::Real = 0f0,\n    sigma2::Real = 0f0,\n    level1::Function = () -> 0 + sigma1 * randn(T),\n    level2::Function = () -> 1 + sigma2 * randn(T),\n    angle::Function = () -> rand(T) * Ï€/4f0,\n    width_min::Real = min(nx,ny) / 4f0 + 3,\n    width_max::Real = min(nx,ny) / 1.6f0, # âˆš2\n    width::Function = () -> [width_min/2,\n        width_min + rand(T) * (width_max - width_min)],\n    center::Function = () -> 0 * (rand(T, 2) .- 1//2),\n    oversample::Int = 6,\n)\n    ob = rect(Tuple(center()), Tuple(width()), angle())\n    l1 = level1()\n    l2 = level2()\n    x = (1:nx) .- (nx-1)/2\n    y = (1:ny) .- (ny-1)/2\n    return l1 .+ (l2 - l1) * phantom(x, y, [ob], oversample),\n        ob.angle[1], ob.width[2]\nend\n\nfunction make_phantoms(nx, ny, nrep)\n    data = zeros(Float32, nx, ny, nrep)\n    angles = zeros(nrep)\n    widths = zeros(nrep)\n    for iz in 1:nrep\n        data[:,:,iz], angles[iz], widths[iz] = rect_phantom(; nx, ny)\n    end\n    return data, angles, widths\nend\n\n\nif !@isdefined(data)\n    nx,ny = 40,40\n    nrep = 500\n    @time data, angles, widths = make_phantoms(nx, ny, nrep)\n    pj = jim(data[:,:,1:72]; title = \"72 of $nrep images\",\n     xaxis = false, yaxis = false, colorbar = :none, # book\n     size = (400, 480), nrow = 9,\n    )\nend\n# savefig(pj, \"eigmap-data.pdf\")\n\nThese data do not lie in a 2-dimensional subspace. To see that, we plot the first 40 singular values. Even in the absence of noise here, there are many singular values that are are far from zero.\n\ntmp = svdvals(reshape(data, nx*ny, nrep))\nps = scatter(tmp; title = \"Data singular values\", widen=true,\n xaxis = (L\"k\", (1, 40), [1, 40, nx*ny]),\n yaxis = (L\"Ïƒ_k\", (0, 216), [0, 32, 48, 72, 215]),\n)\n# savefig(ps, \"eigmap-svd.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/08/eigmap/#Eigenmaps","page":"Laplacian eigenmaps","title":"Eigenmaps","text":"Apply one of the Laplacian eigenmap methods. First compute the distances between all pairs of data points (images).\n\nThere is little if any humanly visible structure in the distance map.\n\ndistance = [norm(d1 - d2) for\n    d1 in eachslice(data; dims=3),\n    d2 in eachslice(data; dims=3)\n]\npd = jim(distance; title = L\"â€– X_j - X_i \\; â€–\", xlabel = L\"i\", ylabel = L\"j\")\n# savefig(pd, \"eigmap-dis.pdf\")\n\nCompute the weight matrix that describes an affinity between data points i and j. There are many ways to do this; here we follow the approach given in Sanders et al. 2016.\n\nÎ± = Float64(sum(abs2, distance) / nrep^2) # per Sanders eqn. (4)\nW = @. exp(-distance^2 / Î±)\npw = jim(W; title = L\"W_{ij}\", xlabel = L\"i\", ylabel = L\"j\")\n# savefig(pw, \"eigmap-w.pdf\")","category":"section"},{"location":"generated/demos/08/eigmap/#Graph-Laplacian","page":"Laplacian eigenmaps","title":"Graph Laplacian","text":"Compute the graph Laplacian from the weight matrix.\n\nd = vec(sum(W; dims=2))\nD = Diagonal(d)\nL = D - W # Laplacian\npl = jim(L; title = L\"L_{ij}\", xlabel = L\"i\", ylabel = L\"j\", color=:cividis)\n# savefig(pl, \"eigmap-l.pdf\")\n\nCompute the generalized eigendecomposition to find solutions of L v = Î» D v.\n\nBecause the vector mathbf1 is in the null space of L, the first (smallest) generalized eigenvalue is 0.\n\nF = eigen(L, Matrix(D))\n\npe = scatter(F.values;\n title = \"Generalized eigenvalues\",\n xlabel=L\"k\", ylabel=L\"Î»_k\",\n xticks = [1,nrep],\n yticks = [0, 0.7, 0.9, 1],\n)\n# savefig(pe, \"eigmap-eigval.pdf\")\n\nprompt()\n\nHere we are interested in the first couple of generalized eigenvectors, excluding the one corresponding to Î»=0. Those vectors serve as features for the reduced dimension.\n\nfeatures = 1000 * F.vectors[:,2:3]\npf = scatter(features[:,1], features[:,2];\n  title = \"Eigenmap features\",\n  xlabel = \"feature 1\", ylabel = \"feature 2\")\n# savefig(pf, \"eigmap-f.pdf\")\n\nprompt()\n\nTo examine whether those features encode the relevant manifold here, we plot the features with color-coding corresponding to the rectangle width and angle.\n\nThe first feature is correlated with rectangle angle. The second feature is correlated with rectangle width.\n\npc = plot(\n scatter(features[:,1], features[:,2], marker_z = widths, color=:cividis,\n  xaxis = (\"feature 1\", (-7,8), -6:6:6),\n  yaxis = (\"feature 2\", (-6,5), -4:2:4),\n# colorbar_title = \"width\", colorbar_fontsize = 15,\n  annotate = (7, 4, \"width\"),\n ),\n scatter(features[:,1], features[:,2], marker_z = angles, color=:cividis,\n  xaxis = (\"feature 1\", (-7,8), -6:6:6),\n  yaxis = (\"feature 2\", (-6,5), -4:2:4),\n  xlabel=\"feature 1\", ylabel=\"feature 2\",\n# colorbar_title = \"angle\",\n  annotate = (7, 4, \"angle\"),\n );\n layout = (2,1), size = (600, 400), right_margin = 10px,\n)\n# savefig(pc, \"eigmap-c.pdf\")\n\nprompt()\n\nAs another way to illustrate this correlation, suppose we examine all images where feature 1 is in some interval like (-15-1). These images are all of rectangles of similar orientation, but various widths.\n\ntmp = data[:,:, -1.5 .< features[:,1] .< -1]\npf1 = jim(tmp; title = \"Feature 1 set\")\n# savefig(pf1, \"eigmap-pf1.pdf\")\n\nConversely, the images where feature 2 is in some interval like (0205) are all rectangles of similar widths, but various rotations.\n\ntmp = data[:,:, 0.2 .< features[:,2] .< 0.5]\npf2 = jim(tmp; nrow=2, title = \"Feature 2 set\", size=(600,300))\n# savefig(pf2, \"eigmap-pf2.pdf\")\n\nExamine PCA for comparison\n\ntmp = reshape(data, nx*ny, nrep)\nU2 = svd(tmp).U[:,1:2]\npup = jim(reshape(U2, nx, ny, 2); title=\"First 2 PCA components\")\n# savefig(pup, \"eigmap-pca-u.pdf\")\n\nProjection onto a 2-dimensional subspace works poorly:\n\nlr2 = reshape(U2 * (U2' * tmp), nx, ny, :)\nplr = jim(lr2[:,:,1:88], \"Projection onto 2-dimensional subspace\")\n# savefig(plr, \"eigmap-pca-lr2.pdf\")\n\nNevertheless, the corresponding features are reasonably correlated with width and angle.\n\nfeat_pca = (U2' * tmp)' # leading coefficients\n\npp = plot(\n scatter(feat_pca[:,1], feat_pca[:,2], marker_z = widths, color=:cividis,\n  xlabel=\"feature 1\", ylabel=\"feature 2\",\n  colorbar_title=\"width\",\n ),\n scatter(feat_pca[:,1], feat_pca[:,2], marker_z = angles, color=:cividis,\n  xlabel=\"feature 1\", ylabel=\"feature 2\",\n  colorbar_title=\"angle\",\n ),\n plot_title = \"First two PCA coefficients\",\n)\n# savefig(pp, \"eigmap-pca-f.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/08/eigmap/#Reproducibility","page":"Laplacian eigenmaps","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/03/eig-locus/#eig-locus","page":"Eigenvalue locus","title":"Eigenvalue locus","text":"This example illustrates how the eigenvalues of a 2  2 symmetric matrix evolve with increasing off-diagonal components.\n\nThis page comes from a single Julia file: eig-locus.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: eig-locus.ipynb, or open it in binder here: eig-locus.ipynb.","category":"section"},{"location":"generated/demos/03/eig-locus/#Setup","page":"Eigenvalue locus","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, det, eigvals\nusing MIRTjim: prompt\nusing Plots: default, plot, plot!, scatter!\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=:true)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt)","category":"section"},{"location":"generated/demos/03/eig-locus/#Eigenvalues","page":"Eigenvalue locus","title":"Eigenvalues","text":"of (1 - c) A + c B for c  01\n\nA = Diagonal([8, 2])\nB = [0 1; 1 0] # has eigenvalues Â±1\nN = 21\nclist = range(0, 1, N)\nd = det(A)\nc0 = (d - sqrt(d)) / (d - 1) # c for which the convex combo becomes singular\nclist = sort([clist; c0])\nmats = [(1-c) * A + c * B for c in clist] # convex combinations\neigs = eigvals.(mats);\n\ne1 = map(first, eigs)\ne2 = map(last, eigs)\nplot(xaxis = (\"c\", (0,1), 0:0.2:1), yaxis = (\"Î»\"),)\nplot!(clist, e2, label = \"Î»â‚‚((1-c) A + c B)\", color=:blue, width=2)\nplot!(clist, e1, label = \"Î»â‚((1-c) A + c B)\", color=:red, width=2)\nscatter!([0,0], eigvals(A), label=\"A eigs\", color=:black)\nscatter!([1,1], eigvals(B), label=\"B eigs\", color=:black, marker=:square)\nscatter!([c0], [0], color=:black, marker=:star)\n\nAdd Gershgorin disk (intervals)\n\ndisk1u = [X[1] + X[3] for X in mats]\ndisk1d = [X[1] - X[3] for X in mats]\ndisk2u = [X[4] + X[2] for X in mats]\ndisk2d = [X[4] - X[2] for X in mats]\nplot!(clist, disk1u, line=:dash, color=:blue)\nplot!(clist, disk1d, line=:dash, color=:blue)\nplot!(clist, disk2u, line=:dash, color=:red)\nplot!(clist, disk2d, line=:dash, color=:red)\n\nprompt()","category":"section"},{"location":"generated/demos/03/eig-locus/#Reproducibility","page":"Eigenvalue locus","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/05/ls-lift/#ls-lift","page":"LS lifting","title":"LS lifting","text":"This example illustrates \"lifting\" in linear regression using the Julia language.\n\nThis page comes from a single Julia file: ls-lift.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: ls-lift.ipynb, or open it in binder here: ls-lift.ipynb.","category":"section"},{"location":"generated/demos/05/ls-lift/#Setup","page":"LS lifting","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, svd\nusing MIRTjim: prompt\nusing Plots: default, gr, plotly, plot!, scatter, surface!, savefig\nusing Plots.PlotMeasures: px\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true, linewidth=2,\n markersize = 6, tickfontsize=14, labelfontsize = 18, legendfontsize=16)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/05/ls-lift/#Noisy-data","page":"LS lifting","title":"Noisy data","text":"Samples of a nonlinear function.\n\nseed!(1) # seed rng\nsfun = (t) -> 1 - cos(Ï€/2*t)\nM = 25\ntm = sort(rand(M)) # M random sample locations\nÏƒ = 0.02\ny = sfun.(tm) + Ïƒ * randn(M); # noisy samples\n\nt0 = range(0, 1, 101) # fine sampling for showing curve\np1 = scatter(tm, y, color=:blue, label=L\"\\mathrm{data\\ } y\",\n\txaxis = (L\"t\", (0, 1), 0:0.5:1),\n\tyaxis = (L\"y\", (-0.1, 1.1), 0:0.5:1),\n)\nplot!(t0, sfun.(t0), color=:black, label=L\"s(t)\", legend=:topleft)\n\nprompt()","category":"section"},{"location":"generated/demos/05/ls-lift/#Polynomial-fits","page":"LS lifting","title":"Polynomial fits","text":"Afun = (tt, deg) -> [t.^i for t in tt, i in 1:deg] # matrix of monomials\n\nA1 = Afun(tm, 1) # M Ã— 1 matrix\nA2 = Afun(tm, 2) # M Ã— 2 matrix\n\nx1 = A1 \\ y # LS solution for degree=1\nplot!(p1, t0, Afun(t0,1)*x1, color=:red, label=\"linear model fit\")\n\nprompt()\n\nx2 = A2 \\ y # quadratic fit\nplot!(p1, t0, Afun(t0,2)*x2, color=:orange, label=\"quadratic model fit\")\n\nprompt()\n\n# savefig(p1, \"04-ls-lift-1.pdf\")","category":"section"},{"location":"generated/demos/05/ls-lift/#Lifting","page":"LS lifting","title":"Lifting","text":"We can view quadratic polynomial fitting as nonlinear \"lifting\" from a 1D function of t to a 2D function of (t t^2). After such lifting, regression with a linear model fits much better, as seen because the data points nearly lie on the 2D plane.\n\nUse plotly() backend here to view surface interactively.\n\n# plotly()\np2 = scatter(A2[:,1], A2[:,2], y, color=:blue, right_margin = 15px,\n    cticks = [0,1],\n    xaxis = (L\"t\", (0,1), -1:1),\n    yaxis = (L\"t^2\", (0,1), -1:1),\n    zaxis = (L\"y\", (0,1), -1:1),\n)\nt1 = range(0, 1, 101)\nt2 = range(0, 1, 102)\nsurface!(t1, t2, (t1,t2) -> x2[1]*t1 + x2[2]*t2, alpha=0.3)\n\nprompt()\n\n# gr(); # restore\n# savefig(p2, \"04-ls-lift-2.pdf\") # with gr()","category":"section"},{"location":"generated/demos/05/ls-lift/#Reproducibility","page":"LS lifting","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#mul-mat-vec","page":"Matrix-vector product","title":"Matrix-vector product","text":"This example illustrates different ways of computing matrix-vector products using the Julia language.\n\nThis page comes from a single Julia file: mul-mat-vec.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: mul-mat-vec.ipynb, or open it in binder here: mul-mat-vec.ipynb.","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Setup","page":"Matrix-vector product","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"BenchmarkTools\"\n        \"InteractiveUtils\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing BenchmarkTools: @benchmark\nusing InteractiveUtils: versioninfo","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Overview-of-matrix-vector-multiplication","page":"Matrix-vector product","title":"Overview of matrix-vector multiplication","text":"The product between a matrix and a compatible vector is such a basic method in linear algebra that, of course, Julia has a function * built-in for it.\n\nIn practice one should simply call that method via A * x or possibly *(A, x).\n\nThis demo explores other ways of coding the product, to explore techniques for writing efficient code.\n\nWe write each method as a function because the most reliable way to benchmark different methods is to use functions.","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Built-in-*","page":"Matrix-vector product","title":"Built-in *","text":"Conventional high-level matrix-vector multiply function:\n\nfunction mul0(A::Matrix, x::Vector)\n    @boundscheck size(A,2) == length(x) || error(\"DimensionMismatch(A,x)\")\n    return A * x\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Double-loop-over-m,n","page":"Matrix-vector product","title":"Double loop over m,n","text":"This is the textbook version.\n\nfunction mul_mn(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = similar(x, M)\n    for m in 1:M\n        inprod = zero(eltype(x)) # accumulator\n        for n in 1:N\n            inprod += A[m,n] * x[n]\n        end\n        y[m] = inprod\n    end\n    return y\nend;\nnothing #hide\n\nUsing @inbounds\n\nfunction mul_mn_inbounds(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    @assert N == length(x)\n    y = similar(x, M)\n    for m in 1:M\n        inprod = zero(x[1]) # accumulator\n        for n in 1:N\n            @inbounds inprod += A[m,n] * x[n]\n        end\n        @inbounds y[m] = inprod\n    end\n    return y\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Double-loop-over-n,m","page":"Matrix-vector product","title":"Double loop over n,m","text":"We expect this way to be faster because of cache access over m.\n\nfunction mul_nm(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        for m in 1:M\n            y[m] += A[m,n] * x[n]\n        end\n    end\n    return y\nend;\nnothing #hide\n\nWith @inbounds\n\nfunction mul_nm_inbounds(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    @assert N == length(x)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        for m in 1:M\n            @inbounds y[m] += A[m,n] * x[n]\n        end\n    end\n    return y\nend;\nnothing #hide\n\nWith @inbounds and @simd\n\nfunction mul_nm_inbounds_simd(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    @assert N == length(x)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        @simd for m in 1:M\n            @inbounds y[m] += A[m,n] * x[n]\n        end\n    end\n    return y\nend;\nnothing #hide\n\nAnd with @views\n\nfunction mul_nm_inbounds_simd_views(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    @assert N == length(x)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        @simd for m in 1:M\n            @inbounds @views y[m] += A[m,n] * x[n]\n        end\n    end\n    return y\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Row-versions","page":"Matrix-vector product","title":"Row versions","text":"Loop over m.\n\nfunction mul_row(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = similar(x, M)\n    for m in 1:M\n        y[m] = transpose(A[m,:]) * x\n    end\n    return y\nend;\nnothing #hide\n\nwith @inbounds\n\nfunction mul_row_inbounds(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    @assert N == length(x)\n    y = similar(x, M)\n    for m in 1:M\n        @inbounds y[m] = transpose(A[m,:]) * x\n    end\n    return y\nend;\nnothing #hide\n\nwith @views\n\nfunction mul_row_views(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = similar(x, M)\n    for m in 1:M\n        @views y[m] = transpose(A[m,:]) * x\n    end\n    return y\nend;\nnothing #hide\n\nwith both\n\nfunction mul_row_inbounds_views(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    @assert N == length(x)\n    y = similar(x, M)\n    for m in 1:M\n        @inbounds @views y[m] = transpose(A[m,:]) * x\n    end\n    return y\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Col-versions","page":"Matrix-vector product","title":"Col versions","text":"Loop over n:\n\nfunction mul_col(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        y += A[:,n] * x[n]\n    end\n    return y\nend;\nnothing #hide\n\nwith broadcast via @. to coalesce operations:\n\nfunction mul_col_dot(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        @. y += A[:,n] * x[n]\n    end\n    return y\nend;\nnothing #hide\n\nand with @views\n\nfunction mul_col_dot_views(A::Matrix, x::Vector)\n    (M,N) = size(A)\n    y = zeros(eltype(x), M)\n    for n in 1:N\n        @views @. y += A[:,n] * x[n]\n    end\n    return y\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Test-and-time-each-version","page":"Matrix-vector product","title":"Test and time each version","text":"The results will depend on the computer used, of course.\n\nM = 2^11\nN = M - 4 # non-square to stress test\nA = randn(Float32, M, N)\nx = randn(Float32, N);\n\nflist = (mul0,\n    mul_mn, mul_mn_inbounds,\n    mul_nm, mul_nm_inbounds, mul_nm_inbounds_simd, mul_nm_inbounds_simd_views,\n    mul_row, mul_row_inbounds, mul_row_views, mul_row_inbounds_views,\n    mul_col, mul_col_dot, mul_col_dot_views,\n);\n\nfor f in flist # warm-up and test each version\n    @assert A * x â‰ˆ f(A, x)\nend;\n\nout = Vector{String}(undef, length(flist))\nfor (i, f) in enumerate(flist) # benchmark timing for each\n    b = @benchmark $f($A,$x)\n    tim = round(minimum(b.times)/10^6, digits=1) # in ms\n    tim = lpad(tim, 4)\n    name = rpad(f, 27)\n\talloc = lpad(b.allocs, 5)\n\tmem = round(b.memory/2^10, digits=1)\n    tmp = \"$name : $tim ms $alloc alloc $mem KiB\"\n    out[i] = tmp\n    isinteractive() && println(tmp)\nend\nout\n\nThe following results were for a 2017 iMac with 4.2GHz quad-core Intel Core i7 with macOS Mojave 10.14.6 and Julia 1.6.2.\n\nAs expected, simple A*x is the fastest, but one can come quite close to that using proper double loop order with @inbounds or using \"dots\" and @views to coalesce. Without @views the vector versions have huge memory overhead!\n\n[\n\"mul0                       :  0.9 ms     1 alloc    16.1 KiB\"\n\"mul_mn                     : 22.5 ms     1 alloc    16.1 KiB\"\n\"mul_mn_inbounds            : 22.0 ms     1 alloc    16.1 KiB\"\n\"mul_nm                     :  3.1 ms     1 alloc    16.1 KiB\"\n\"mul_nm_inbounds            :  1.5 ms     1 alloc    16.1 KiB\"\n\"mul_nm_inbounds_simd       :  1.5 ms     1 alloc    16.1 KiB\"\n\"mul_nm_inbounds_simd_views :  1.5 ms     1 alloc    16.1 KiB\"\n\"mul_row                    : 32.8 ms  2049 alloc 33040.1 KiB\"\n\"mul_row_inbounds           : 32.7 ms  2049 alloc 33040.1 KiB\"\n\"mul_row_views              : 22.4 ms     1 alloc    16.1 KiB\"\n\"mul_row_inbounds_views     : 22.4 ms     1 alloc    16.1 KiB\"\n\"mul_col                    : 16.0 ms  6133 alloc 98894.6 KiB\"\n\"mul_col_dot                :  7.0 ms  2045 alloc 32975.6 KiB\"\n\"mul_col_dot_views          :  1.5 ms     1 alloc    16.1 KiB\"\n];\nnothing #hide","category":"section"},{"location":"generated/demos/02/mul-mat-vec/#Reproducibility","page":"Matrix-vector product","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/05/ls-fit1/#ls-fit1","page":"LS fitting","title":"LS fitting","text":"This example illustrates least squares (LS) polynomial fitting using the Julia language.\n\nThis page comes from a single Julia file: ls-fit1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: ls-fit1.ipynb, or open it in binder here: ls-fit1.ipynb.","category":"section"},{"location":"generated/demos/05/ls-fit1/#Setup","page":"LS fitting","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, svd\nusing MIRTjim: prompt\nusing Plots: default, plot, plot!, scatter, scatter!, savefig\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true, linewidth=2,\n    markersize = 6, tickfontsize=12, labelfontsize = 16, legendfontsize=14)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/05/ls-fit1/#Simulated-data-from-latent-nonlinear-function","page":"LS fitting","title":"Simulated data from latent nonlinear function","text":"s = (t) -> atan(4*(t-0.5)) # nonlinear function\n\nseed!(0) # seed rng\nM = 15 # how many data points\ntm = sort(rand(M)) # M random sample locations\ny = s.(tm) + 0.1 * randn(M) # noisy samples\n\nt0 = range(0, 1, 101) # fine sampling for showing curve\nxaxis=(L\"t\", (0,1), 0:0.5:1)\nyaxis=(L\"y\", (-1.3, 1.3), -1:1)\np0 = scatter(tm, y, color=:black, label=\"y (noisy data)\"; xaxis, yaxis)\nplot!(t0, s.(t0), color=:blue, label=\"s(t) : latent signal\", legend=:topleft)\n\nprompt()","category":"section"},{"location":"generated/demos/05/ls-fit1/#Polynomial-fitting","page":"LS fitting","title":"Polynomial fitting","text":"deg = 3 # polynomial degree\nAfun = (tt) -> [t.^i for t in tt, i in 0:deg] # matrix of monomials\nA = Afun(tm) # M Ã— 4 matrix\np1 = plot(title=\"Columns of matrix A\", xlabel=L\"t\", legend=:left)\nfor i in 0:deg\n    plot!(p1, tm, A[:,i+1], marker=:circle, label = \"A[:,$(i+1)]\")\nend\np1\n\nprompt()","category":"section"},{"location":"generated/demos/05/ls-fit1/#Fit-4-unknowns-with-4-equations","page":"LS fitting","title":"Fit 4 unknowns with 4 equations","text":"m4 = Int64.(round.(range(1, M-1, 4))) # pick 4 points well separated\nA4 = A[m4,:] # 4 Ã— 4 matrix\nx4 = inv(A4) * y[m4] # inverse of 4Ã—4 matrix to solve \"y = A x\"\n\np1 = scatter(tm[m4], y[m4], marker=:square, color=:red)\nscatter!(tm, y, color=:black, label=\"y (noisy data)\"; xaxis, yaxis)\nplot!(t0, s.(t0), color=:blue, label=\"s(t) : latent signal\", legend=:topleft)\nplot!(t0, Afun(t0)*x4, color=:red, label=\"Fit 4 of $M points\")\n\nprompt()","category":"section"},{"location":"generated/demos/05/ls-fit1/#Fit-4-unknowns-using-all-M15-equations","page":"LS fitting","title":"Fit 4 unknowns using all M=15 equations","text":"xh = A \\ y # backslash for LS solution using all M samples\n\nplot!(p1, t0, Afun(t0)*xh, color=:orange, label=\"Fit cubic to M=$(M) points\")\n\nprompt()\n\n# savefig(p1, \"ls-fit-4-15.pdf\")","category":"section"},{"location":"generated/demos/05/ls-fit1/#SVD-solution","page":"LS fitting","title":"SVD solution","text":"U, s, V = svd(A)\ns","category":"section"},{"location":"generated/demos/05/ls-fit1/#Verify-equivalence-of-SVD-and-backslash-solutions-to-LS-problem","page":"LS fitting","title":"Verify equivalence of SVD and backslash solutions to LS problem","text":"xh2 = V * Diagonal(1 ./ s) * (U' * y) # SVD-based solution\nxh3 = V * ( (1 ./ s) .* (U' * y) ) # mathematically equivalent alternate expression\n\n@assert xh â‰ˆ xh2\n@assert xh â‰ˆ xh3","category":"section"},{"location":"generated/demos/05/ls-fit1/#Reproducibility","page":"LS fitting","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/lr-sure/#lr-sure","page":"Low-Rank SURE","title":"Low-Rank SURE","text":"This example illustrates Stein's unbiased risk estimation (SURE) for parameter selection in low-rank matrix approximation, using the Julia language.\n\nThis page comes from a single Julia file: lr-sure.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: lr-sure.ipynb, or open it in binder here: lr-sure.ipynb.","category":"section"},{"location":"generated/demos/07/lr-sure/#Setup","page":"Low-Rank SURE","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: svd, svdvals, Diagonal, norm\nusing MIRTjim: prompt\nusing Plots: default, gui, plot, plot!, scatter!, savefig\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto, markersize=7,\n    labelfontsize=20, tickfontsize=16, legendfontsize=17, widen=true)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-sure/#Generate-data","page":"Low-Rank SURE","title":"Generate data","text":"Noiseless low-rank matrix and noisy data matrix\n\nM, N = 100, 50 # problem size\nseed!(0)\nKtrue = 5 # true rank (planted model)\nX = svd(randn(M,Ktrue)).U * Diagonal(1:Ktrue) * svd(randn(Ktrue,N)).Vt\nsig0 = 0.03 # noise standard deviation\nY = X + sig0 * randn(size(X)) # noisy\nsy = svdvals(Y)\nsx = svdvals(X)\nsx[1:Ktrue]\n\nsy[1:Ktrue]","category":"section"},{"location":"generated/demos/07/lr-sure/#Plot-singular-values","page":"Low-Rank SURE","title":"Plot singular values","text":"ps = plot(xaxis = (L\"k\", (1,N), [1, Ktrue, N]), yaxis = (L\"Ïƒ\", (0,5.5), 0:5))\nscatter!(1:N, sy, color=:red, marker=:hexagon,\n label=L\"\\sigma_k(Y) \\ \\mathrm{noisy}\")\nscatter!(1:N, sx, color=:blue, label=L\"\\sigma_k(X) \\ \\mathrm{noiseless}\")\n\nprompt()\n\n# savefig(ps, \"lr_sure1s.pdf\")","category":"section"},{"location":"generated/demos/07/lr-sure/#Low-rank-approximation-with-various-ranks","page":"Low-Rank SURE","title":"Low-rank approximation with various ranks","text":"(U, sy, V) = svd(Y)\nnrmse_K = zeros(N)\nnrmsd_K = zeros(N)\nnrmsd = (D) -> norm(D) / norm(Y) * 100\nnrmse = (D) -> norm(D) / norm(X) * 100\nfor K in 1:N\n    Xh = U[:,1:K] * Diagonal(sy[1:K]) * V[:,1:K]'\n    nrmsd_K[K] = nrmsd(Xh - Y)\n    nrmse_K[K] = nrmse(Xh - X)\nend\nnrmsd_K = [nrmsd(0 .- Y); nrmsd_K]\nnrmse_K = [nrmse(0 .- X); nrmse_K]\nklist = 0:N;\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-sure/#Plot-normalized-root-mean-squared-error/difference-versus-rank-K","page":"Low-Rank SURE","title":"Plot normalized root mean-squared error/difference versus rank K","text":"pk = plot( # legend=:outertop,\n    xaxis = (L\"K\", (1,N), [0, 2, Ktrue, N]),\n    yaxis = (\"'Error' [%]\", (0, 100), 0:20:100),\n)\nscatter!(klist, nrmse_K, color=:blue,\n    label=L\"\\mathrm{NRMSE\\ } â€– \\! \\hat{X}_K - X \\ â€–_{\\mathrm{F}} / â€–X \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n)\nscatter!(klist, nrmsd_K, color=:red, marker=:diamond,\n    label=L\"\\mathrm{NRMSD\\ } â€– \\! \\hat{X}_K - Y \\ â€–_{\\mathrm{F}} / â€–Y \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n)\n\nprompt()\n\n# savefig(pk, \"lr_sure1a.pdf\")","category":"section"},{"location":"generated/demos/07/lr-sure/#Explore-(nuclear-norm)-regularized-version","page":"Low-Rank SURE","title":"Explore (nuclear norm) regularized version","text":"soft = (s,Î²) -> max.(s-Î²,0) # soft threshold function\ndsoft = (s,Î²) -> Float32.(s .> Î²) # \"derivative\" thereof\nreglist = [range(0, 0.5, 11); 0.75:0.25:6]\nNr = length(reglist)\nnrmse_reg = zeros(Nr)\nnrmsd_reg = zeros(Nr)\nfor ir in 1:Nr\n    reg = reglist[ir]\n    Xh = U * Diagonal(soft.(sy,reg)) * V'\n    nrmsd_reg[ir] = nrmsd(Xh - Y)\n    nrmse_reg[ir] = nrmse(Xh - X)\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/07/lr-sure/#Plot-NRMSE-and-NRMSD-versus-regularization-parameter","page":"Low-Rank SURE","title":"Plot NRMSE and NRMSD versus regularization parameter","text":"pb = plot(legend=:topleft, xaxis = (L\"Î²\", (0,6), 0:6),\n    yaxis = (\"'Error' [%]\", (0, 100), 0:20:100))\nscatter!(reglist, nrmse_reg, color=:blue,\n    label=L\"\\mathrm{NRMSE\\ } â€– \\! \\hat{X}_{\\beta} - X \\ â€–_{\\mathrm{F}} / â€–X \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n#  label=L\"\\mathrm{NRMSE}\", # book\n)\nscatter!(reglist, nrmsd_reg, color=:red, marker=:diamond,\n    label=L\"\\mathrm{NRMSD\\ } â€– \\! \\hat{X}_{\\beta} - Y \\ â€–_{\\mathrm{F}} / â€–Y \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n#  label=L\"\\mathrm{NRMSD}\", # book\n)\n\nprompt()\n\n# savefig(pb, \"lr_sure1b.pdf\")","category":"section"},{"location":"generated/demos/07/lr-sure/#Explore-SURE-for-selecting-Î²","page":"Low-Rank SURE","title":"Explore SURE for selecting Î²","text":"mathrmSURE(Î²) =  hatX - Y ^2 - MN sigma_0^2\n + 2 Ïƒ_0^2 left( M - N sum_i=1^min(MN) frach(Ïƒ_iÏƒ)Ïƒ_i\n + sum_i=1^min(MN) doth_i(Ïƒ_iÎ²)\n + 2 sum_i neq j^min(MN) fracÏƒ_i h_i(Ïƒ_iÎ²)Ïƒ_i^2 - Ïƒ_j^2 right)\n\nsy singular values of Y\nreg regularization parameter\nv0 = sigma_0^2 noise variance\n\nfunction sure(sy, reg, v0, M, N)\n    sh = soft.(sy, reg) # estimated singular values\n    big = sy.^2 .- (sy.^2)'\n    big[big .== 0] .= Inf # trick to avoid divide by 0\n    big = (sy .* sh) ./ big # [sy[i] * sh[i] / big[i,j] for i in 1:N, j in 1:N]\n    big = sum(big)\n    norm(sh - sy)^2 - M*N*v0 + 2*v0*(abs(M-N)*sum(sh ./ sy) + sum(dsoft.(sy,reg)) + 2*big)\nend","category":"section"},{"location":"generated/demos/07/lr-sure/#Evaluate-SURE-for-each-candidate-regularization-parameter","page":"Low-Rank SURE","title":"Evaluate SURE for each candidate regularization parameter","text":"sure_reg = [sure(sy, reglist[ir], sig0^2, M, N) for ir in 1:Nr]\nreg_best = reglist[argmin(sure_reg)] # SURE pick for Î²","category":"section"},{"location":"generated/demos/07/lr-sure/#Plot-NRMSE-and-NRMSD-versus-regularization-parameter-2","page":"Low-Rank SURE","title":"Plot NRMSE and NRMSD versus regularization parameter","text":"psb = plot(legend=:bottomright, widen=true,\n    xaxis = (L\"Î²\", (0,6), [reg_best, 5, 6]),\n    yaxis = (\"'Error' [%]\", (0,100), 0:20:100),\n)\nscatter!(reglist, nrmse_reg, color=:blue,\n    label=L\"\\mathrm{NRMSE\\ } â€– \\! \\hat{X}_\\beta - X \\ â€–_{\\mathrm{F}} / â€–X \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n#  label=L\"\\mathrm{NRMSE}\", # book\n)\nscatter!(reglist, nrmsd_reg, color=:red, marker=:diamond,\n    label=L\"\\mathrm{NRMSD\\ } â€– \\! \\hat{X}_\\beta - Y \\ â€–_{\\mathrm{F}} / â€–Y \\ â€–_{\\mathrm{F}} \\cdot 100\\%\",\n#  label=L\"\\mathrm{NRMSD}\", # book\n)\nscatter!(reglist, sqrt.(sure_reg)/norm(Y)*100, color=:green, marker=:star,\n    label=L\"(\\mathrm{SURE}(\\beta))^{1/2} / â€–Y \\ â€–_{\\mathrm{F}} \\cdot 100\\%\")\n\nprompt()\n\n# savefig(psb, \"lr_sure1c.pdf\")","category":"section"},{"location":"generated/demos/07/lr-sure/#Examine-shrunk-singular-values-for-best-regularization-parameter","page":"Low-Rank SURE","title":"Examine shrunk singular values for best regularization parameter","text":"sh = soft.(sy,reg_best)\npsk = plot(\n    xaxis = (L\"k\", (1, N), [1, Ktrue, sum(sh .!= 0), N]),\n    yaxis = (L\"Ïƒ\", (0, 5.5), 0:6),\n    legendfontsize = 20,\n)\nscatter!(1:N, sy, color=:red, marker=:hexagon, label=L\"\\sigma(Y) \\ \\mathrm{noisy}\")\nscatter!(1:N, sx, color=:blue, label=L\"\\sigma(X) \\ \\mathrm{noiseless}\")\nscatter!(1:N, sh, color=:green, marker=:star, label=L\"\\hat{\\sigma} \\ \\ \\mathrm{SURE} \\ \\hat{\\beta}\")\n\nprompt()\n\n# savefig(psk, \"lr_sure1t.pdf\")","category":"section"},{"location":"generated/demos/07/lr-sure/#Reproducibility","page":"Low-Rank SURE","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/02/conv-mat/#conv-mat","page":"Convolution matrix","title":"Convolution matrix","text":"This example illustrates 1D signal convolution represented as matrix operations (for various boundary conditions) using the Julia language.\n\nThis page comes from a single Julia file: conv-mat.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: conv-mat.ipynb, or open it in binder here: conv-mat.ipynb.","category":"section"},{"location":"generated/demos/02/conv-mat/#Setup","page":"Convolution matrix","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"ColorSchemes\"\n        \"DSP\"\n        \"FFTW\"\n        \"FFTViews\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearMapsAA\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ColorSchemes\nusing DSP: conv\nusing FFTW: fft, ifft\nusing FFTViews: FFTView\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearMapsAA\nusing MIRTjim: jim\nusing Plots: RGB, default, gui, heatmap, plot, plot!, savefig, scatter!, @layout\nusing Plots.PlotMeasures: px\ndefault(markerstrokecolor=:auto, markersize=11, linewidth=5, label=\"\",\n tickfontsize = 11, labelfontsize = 15, titlefontsize=18)","category":"section"},{"location":"generated/demos/02/conv-mat/#Filter","page":"Convolution matrix","title":"Filter","text":"Define a 5-tap finite impulse response (FIR) filter hk with support -2:2 and display it.\n\npsf = [1, 3, 5, 4, 2] # filter impulse response\nK = length(psf)\nhp = Int((K-1)/2) # 2 (half width)\n\ncols = [ColorSchemes.viridis[x] for x in range(0,1,K)]\n\npp = plot(widen=true, tickfontsize = 14, labelfontsize = 20,\n    xaxis = (L\"k\", (-1,1) .* 4, -4:4),\n    yaxis = (L\"h[k]\", (-0.2, 5.2)),\n    size = (600, 250), left_margin = 15px, bottom_margin = 18px,\n)\nplot!([-5, 5], [0, 0], color=:black, linewidth=2)\nfor k in 1:K\n    c = psf[k]\n    c = cols[c]\n    plot!([k-hp-1], [psf[k]], line=:stem, marker=:circle, color=c)\nend\nfor k in (hp+1):4\n    scatter!([k], [0], color=:grey)\n    scatter!([-k], [0], color=:grey)\nend\ngui()\n# savefig(pp, \"psf.pdf\")","category":"section"},{"location":"generated/demos/02/conv-mat/#Convolution-matrices","page":"Convolution matrix","title":"Convolution matrices","text":"Define convolution matrices for various end conditions. The names of the conditions below match those of Matlab's conv function. Submit a github issue to request elaboration on these cases.\n\nN = 10 # input signal length\nkernel = FFTView(zeros(Int, N))\nkernel[-hp:hp] = psf;\nnothing #hide","category":"section"},{"location":"generated/demos/02/conv-mat/#'full'","page":"Convolution matrix","title":"'full'","text":"for zero end conditions where the output signal length is M = N + K - 1.\n\nAz = LinearMapAA(x -> conv(x, psf), (N+K-1, N))\nAz = Matrix(Az)\nAz = round.(Int, Az) # because `conv` apparently uses `fft`\nsize(Az)","category":"section"},{"location":"generated/demos/02/conv-mat/#'circ'","page":"Convolution matrix","title":"'circ'","text":"for periodic end conditions for which M = N.\n\nAc = LinearMapAA(x -> ifft(fft(x) .* fft(kernel)), (N,N) ; T = ComplexF64)\nAc = Matrix(Ac)\nAc = round.(Int, real(Ac))\nsize(Ac)","category":"section"},{"location":"generated/demos/02/conv-mat/#'same'","page":"Convolution matrix","title":"'same'","text":"for matching input and output signal lengths, so M = N.\n\nAs = Az[hp .+ (1:N),:]\nsize(As)","category":"section"},{"location":"generated/demos/02/conv-mat/#'valid'","page":"Convolution matrix","title":"'valid'","text":"where output is defined only for samples where the shifted impulse overlaps the input signal, for which M = N-K+1.\n\nAv = Az[(K-1) .+ (1:(N-(K-1))),:]\nsize(Av)","category":"section"},{"location":"generated/demos/02/conv-mat/#Display-convolution-matrices","page":"Convolution matrix","title":"Display convolution matrices","text":"using colors that match the 1D impulse response plot.\n\ncmap = [RGB{Float64}(1.,1.,1.) .* 0.8; cols];\n\njy = (x,t,y) -> jim(x'; color=cmap, ylims=(0.5,14.5), ylabel=y, labelfontsize=12);\n\npz = jy(Az, \"'full'\", L\"M=N+K-1\")\npv = jy(Av, \"'valid'\", L\"M=N-K+1\")\nps = jy(As, \"'same'\", L\"M=N\")\npc = jy(Ac, \"circulant\", L\"M=N\")\n# plot(pz, pv, ps, pc; size=(400,400))\np4 = plot(pz, pv, ps, pc; size=(1000,200), layout=(1,4), left_margin = 20px)\n\n# savefig(p4, \"match4.pdf\")\n# savefig(pz, \"a-full.pdf\")\n# savefig(pv, \"a-valid.pdf\")\n# savefig(ps, \"a-same.pdf\")\n# savefig(pc, \"a-circ.pdf\")\n\nl = @layout [\n a{0.3w} b{0.3w}\n c{0.3w} d{0.3w}\n]\np22 = plot(pz, pv, ps, pc) #, layout = l, size = (500,500))\n# savefig(p22, \"all4.pdf\")","category":"section"},{"location":"generated/demos/02/conv-mat/#Reproducibility","page":"Convolution matrix","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/10/lrmc3/#lrmc3","page":"Low-rank matrix completion: ADMM","title":"Low-rank matrix completion: ADMM","text":"This example illustrates low-rank matrix completion using the Julia language.\n\nHistory:\n\n2017-11-07 Greg Ongie, University of Michigan, original version\n2017-11-12 Jeff Fessler, minor modifications\n2021-08-23 Julia 1.6.2\n2023-04-11 Literate version for Julia 1.8\n\nThis page comes from a single Julia file: lrmc3.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: lrmc3.ipynb, or open it in binder here: lrmc3.ipynb.","category":"section"},{"location":"generated/demos/10/lrmc3/#Setup","page":"Low-rank matrix completion: ADMM","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"DelimitedFiles\"\n        \"Downloads\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRT\"\n        \"MIRTjim\"\n        \"Plots\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing DelimitedFiles: readdlm\nusing Downloads: download\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: svd, svdvals, Diagonal, norm\nusing MIRT: pogm_restart\nusing MIRTjim: jim, prompt\nusing Plots: plot, scatter, scatter!, savefig, default\ndefault(); default(markerstrokecolor=:auto, label = \"\")\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/10/lrmc3/#TOP-SECRET","page":"Low-rank matrix completion: ADMM","title":"TOP SECRET","text":"On 2017-11-06 10:34:12 GMT, Agent 556 obtained a photo of the illegal arms dealer, code name Professor X-Ray. However, Agent 556 spilled soup on their lapel-camera, shorting out several CCD sensors. The image matrix has several missing entries; we were only able to recover 25% of data!\n\nAgent 551: Your mission, should you choose to accept it, is to recover the missing entries and uncover the true identity of Professor X-Ray.\n\nRead in data with missing pixels set to zero\n\nif !@isdefined(Y)\n    tmp = homedir() * \"/web/course/551/julia/demo/professorxray.txt\" # jf\n    if !isfile(tmp)\n        url = \"https://github.com/JeffFessler/book-la-demo/raw/refs/heads/main/data/professorxray.txt\"\n        tmp = download(url)\n    end\n    Y = collect(readdlm(tmp)')\n    py = jim(Y, \"Y: Corrupted image matrix of Professor X-Ray\\n (missing pixels set to 0)\")\nend\n\nCreate binary mask Î© (true=observed, false=unobserved)\n\nÎ© = Y .!= 0\npercent_nonzero = sum(Î©) / length(Î©) # count proportion of missing entries\n\nShow mask\n\npm = jim(Î©, \"Î©: Locations of observed entries\")","category":"section"},{"location":"generated/demos/10/lrmc3/#Low-rank-approximation","page":"Low-rank matrix completion: ADMM","title":"Low-rank approximation","text":"A simple low-rank approximation works poorly for this much missing data\n\nr = 20\nU,s,V = svd(Y)\nXr = U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]'\npr = jim(Xr, \"Low-rank approximation for r=$r\")","category":"section"},{"location":"generated/demos/10/lrmc3/#Low-rank-matrix-completion","page":"Low-rank matrix completion: ADMM","title":"Low-rank matrix completion","text":"Instead, we will try to uncover the identity of Professor X-Ray using low-rank matrix completion.\n\nThe optimization problem we will solve is:\n\nmin_mathbf X\nfrac12  P_Î©(mathbf X) - P_Î©(mathbf Y) _2^2\n+ Î²  mathbf X _*\nquadquadtext(NN-min)\n\nwhere mathbf Y is the zero-filled input data matrix, and P_Î© is the operator that extracts a vector of entries belonging to the index set Î©.\n\nDefine cost function for optimization problem:\n\nnucnorm = (X) -> sum(svdvals(X))\ncostfun = (X, beta) -> 0.5 * norm(X[Î©] - Y[Î©])^2 + beta * nucnorm(X);\nnothing #hide\n\nDefine singular value soft thresholding (SVST) function\n\nfunction SVST(X, beta)\n    U,s,V = svd(X)\n    sthresh = @. max(s - beta, 0)\n    return U * Diagonal(sthresh) * V'\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/10/lrmc3/#Iterative-Soft-Thresholding-Algorithm-(ISTA)","page":"Low-rank matrix completion: ADMM","title":"Iterative Soft-Thresholding Algorithm (ISTA)","text":"ISTA is an extension of gradient descent to convex cost functions that look like min_x f(x) + g(x) where f(x) is smooth and g(x) is non-smooth. Also known as a proximal gradient method.\n\nISTA algorithm for solving (NN-min):\n\ninitialize mathbf X_0 = mathbf Y (zero-fill missing entries)\nfor k=012\nhatmathbf X_k_ij = begincasesmathbf X_k_ij  (ij)  Î©  mathbf Y_ij  (ij)  Î© endcases (Put back in known entries)\nmathbf X_k+1 = textSVST(hatmathbf X_kÎ²) (Singular value soft-thresholding)\nend\n\nApply ISTA:\n\nniter = 400\nbeta = 0.01 # chosen by trial-and-error here\nfunction lrmc_ista(Y)\n    X = copy(Y)\n    Xold = copy(X)\n    cost_ista = zeros(niter+1)\n    cost_ista[1] = costfun(X,beta)\n    for k in 1:niter\n        X[Î©] = Y[Î©]\n        X = SVST(X,beta)\n        cost_ista[k+1] = costfun(X,beta)\n    end\n    return X, cost_ista\nend;\n\nif !@isdefined(Xista)\n    Xista, cost_ista = lrmc_ista(Y)\n    pj_ista = jim(Xista, \"ISTA result at $niter iterations\")\nend\n\nWhat went wrong? Let's investigate. First, let's see if the above solution is actually low-rank.\n\ns_ista = svdvals(Xista)\ns0 = svdvals(Y)\nplot(title = \"singular values\",\n    xtick = [1, sum(s .> 20*eps()), minimum(size(Y))])\nscatter!(s0, color=:black, label=\"Y (initialization)\")\nscatter!(s_ista, color=:red, label=\"X (ISTA)\")\n\nprompt()\n\nNow let's check the cost function descent:\n\nscatter(cost_ista, color=:red,\n    title = \"cost vs. iteration\",\n    xlabel = \"iteration\",\n    ylabel = \"cost function value\",\n    label = \"ISTA\",\n)\n\nprompt()","category":"section"},{"location":"generated/demos/10/lrmc3/#Fast-Iterative-Soft-Thresholding-Algorithm-(FISTA)","page":"Low-rank matrix completion: ADMM","title":"Fast Iterative Soft-Thresholding Algorithm (FISTA)","text":"Modification of ISTA that includes Nesterov acceleration for faster convergence.\n\nReference:\n\nBeck, A. and Teboulle, M., 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. on Imaging Sciences, 2(1), pp.183-202.\n\nFISTA algorithm for solving (NN-min)\n\ninitialize matrices mathbf Z_0 = mathbf X_0 = mathbf Y\nfor k=012\nhatmathbf Z_k_ij = begincasesmathbf Z_k_ij  (ij)  Î©  mathbf Y_ij  (ij)  Î© endcases (Put back in known entries)\nmathbf X_k+1 = textSVST(hatmathbf Z_kbeta)\nt_k+1 = frac1 + sqrt1+4t_k^22 (Nesterov step-size)\nmathbf Z_k+1 = mathbf X_k+1 + fract_k-1t_k+1(mathbf X_k+1-mathbf X_k) (Momentum update)\nend\n\nRun FISTA:\n\nniter = 200\nfunction lrmc_fista(Y)\n    X = copy(Y)\n    Z = copy(X)\n    Xold = copy(X)\n    told = 1\n    cost_fista = zeros(niter+1)\n    cost_fista[1] = costfun(X,beta)\n    for k in 1:niter\n        Z[Î©] = Y[Î©]\n        X = SVST(Z,beta)\n        t = (1 + sqrt(1+4*told^2))/2\n        Z = X + ((told-1)/t)*(X-Xold)\n        Xold = X\n        told = t\n        cost_fista[k+1] = costfun(X,beta) # comment out to speed-up\n    end\n    return X, cost_fista\nend;\n\nif !@isdefined(Xfista)\n    Xfista, cost_fista = lrmc_fista(Y)\n    pj_fista = jim(Xfista, \"FISTA result at $niter iterations\")\nend\n\nplot(title = \"cost vs. iteration\",\n    xlabel=\"iteration\", ylabel = \"cost function value\")\nscatter!(cost_ista, label=\"ISTA\", color=:red)\nscatter!(cost_fista, label=\"FISTA\", color=:blue)\n\nprompt()\n\nSee if the FISTA result is \"low rank\"\n\ns_fista = svdvals(Xfista)\neffective_rank = count(>(0.01*s_fista[1]), s_fista)\n\nps = plot(title=\"singular values\",\n    xtick = [1, effective_rank, count(>(20*eps()), s_fista), minimum(size(Y))])\nscatter!(s0, label=\"Y (initial)\", color=:black)\nscatter!(s_fista, label=\"X (FISTA)\", color=:blue)\n\nprompt()\n\nExercise: think about why Ïƒ_1(X)  Ïƒ_1(Y) !","category":"section"},{"location":"generated/demos/10/lrmc3/#Alternating-directions-method-of-multipliers-(ADMM)","page":"Low-rank matrix completion: ADMM","title":"Alternating directions method of multipliers (ADMM)","text":"ADMM is another approach that uses SVST as a sub-routine, closely related to proximal gradient descent.\n\nIt is faster than FISTA, but the algorithm requires a tuning parameter Î¼. (Here we use Î¼ = Î²).\n\nReferences:\n\nCai, J.F., CandÃ¨s, E.J. and Shen, Z., 2010. A singular value thresholding algorithm for matrix completion. SIAM J. Optimization, 20(4), pp. 1956-1982.\nBoyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J., 2011. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1), pp. 1-122.\n\nRun alternating directions method of multipliers (ADMM) algorithm:\n\nniter = 50\n\nChoice of parameter Î¼ can greatly affect convergence rate\n\nfunction lrmc_admm(Y; mu::Real = beta)\n    X = copy(Y)\n    Z = zeros(size(X))\n    L = zeros(size(X))\n    cost_admm = zeros(niter+1)\n    cost_admm[1] = costfun(X,beta)\n    for k in 1:niter\n        Z = SVST(X + L, beta / mu)\n        X = (Y + mu * (Z - L)) ./ (mu .+ Î©)\n        L = L + X - Z\n        cost_admm[k+1] = costfun(X,beta) # comment out to speed-up\n    end\n    return X, cost_admm\nend;\n\nif !@isdefined(Xadmm)\n    Xadmm, cost_admm = lrmc_admm(Y)\n    pj_admm = jim(Xadmm, \"ADMM result at $niter iterations\")\nend\n\npc = plot(title = \"cost vs. iteration\",\n    xtick = [0, 50, 200, 400],\n    xlabel = \"iteration\", ylabel = \"cost function value\")\nscatter!(0:400, cost_ista, label=\"ISTA\", color=:red)\nscatter!(0:200, cost_fista, label=\"FISTA\", color=:blue)\nscatter!(0:niter, cost_admm, label=\"ADMM\", color=:magenta)\n\nprompt()\n\nAll singular values\n\ns_admm = svdvals(Xadmm)\nscatter!(ps, s_admm, label=\"X (ADMM)\", color=:magenta, marker=:square)\n\nprompt()\n\nFor a suitable choice of Î¼, ADMM converges faster than FISTA.","category":"section"},{"location":"generated/demos/10/lrmc3/#Proximal-optimized-gradient-method-(POGM)","page":"Low-rank matrix completion: ADMM","title":"Proximal optimized gradient method (POGM)","text":"The proximal optimized gradient method (POGM) with adaptive restart is faster than FISTA with very similar computation per iteration. Unlike ADMM, POGM does not require any algorithm tuning parameter Î¼, making it easier to use in many practical composite optimization problems.\n\nif !@isdefined(Xpogm)\n    Fcost = X -> costfun(X, beta)\n    f_grad = X -> Î© .* (X - Y) # gradient of smooth term\n    f_L = 1 # Lipschitz constant of f_grad\n    g_prox = (X, c) -> SVST(X, c * beta)\n    fun = (iter, xk, yk, is_restart) -> (xk, Fcost(xk), is_restart)\n    niter = 150\n    Xpogm, out = pogm_restart(Y, Fcost, f_grad, f_L; g_prox, fun, niter)\n    cost_pogm = [o[2] for o in out]\n    pj_pogm = jim(Xpogm, \"POGM result at $niter iterations\")\nend\n\nscatter!(pc, 0:niter, cost_pogm, label=\"POGM\", color=:green)\n\nprompt()","category":"section"},{"location":"generated/demos/10/lrmc3/#Reproducibility","page":"Low-rank matrix completion: ADMM","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/08/kron-sum-inv/#kron-sum-inv","page":"Kronecker sum of circulant","title":"Kronecker sum of circulant","text":"This example illustrates efficient computation of the inverse of a Kronecker sum of circulant matrices using the Julia language.\n\nRelated to Problem 8.7 in the textbook.\n\nThis page comes from a single Julia file: kron-sum-inv.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: kron-sum-inv.ipynb, or open it in binder here: kron-sum-inv.ipynb.","category":"section"},{"location":"generated/demos/08/kron-sum-inv/#Setup","page":"Kronecker sum of circulant","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"FFTW\"\n        \"InteractiveUtils\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing FFTW: fft, ifft\nusing InteractiveUtils: versioninfo\nusing LinearAlgebra: I\nusing MIRTjim: jim, prompt\nusing Plots: savefig\nusing Random: seed!\nseed!(0)\n\nThe following line helps when running this jl-file as a script; this way it prompts user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide\n\nCirculant matrix with given first column\n\nfunction circm(x)\n    N = length(x)\n    return hcat([circshift(x, k-1) for k in 1:N]...)\nend","category":"section"},{"location":"generated/demos/08/kron-sum-inv/#Numerical-test","page":"Kronecker sum of circulant","title":"Numerical test","text":"First perform a numerical test to verify the formulas.\n\nTest data\n\nM, N = 64, 32\nb = randn(N)\nc = randn(M)\nX = randn(M, N);\nnothing #hide\n\nMatrix-inverse solution\n\nB = circm(b) # N\nC = circm(c) # M\nA = kron(B, I(M)) + kron(I(N), C)\ny = inv(A) * vec(X);\nnothing #hide\n\nDenominator\n\nP = fft(c) .+ transpose(fft(b));\nnothing #hide\n\nDFT solution\n\nFn = fft(I(N), 1)\nFm = fft(I(M), 1)\nY1 = (Fm' * ((Fm * X * transpose(Fn)) ./ P) * conj(Fn)) / (M * N)\n@assert Y1 â‰ˆ real(Y1) # should be real-valued\nY1 = real(Y1); # so discard the imaginary part\nnothing #hide\n\nFFT solution\n\nY2 = ifft(fft(X) ./ P)\n@assert Y2 â‰ˆ real(Y2)\nY2 = real(Y2);\nnothing #hide\n\nVerify\n\n@assert y â‰ˆ vec(Y1)\n@assert y â‰ˆ vec(Y2)","category":"section"},{"location":"generated/demos/08/kron-sum-inv/#Image-data","page":"Kronecker sum of circulant","title":"Image data","text":"Now illustrate visually using additively separable blur with individually invertible blur kernels.\n\nX = ones(60, 64); X[20:50,10:50] .= 2\nM, N = size(X)\nb = zeros(N); b[1 .+ mod.(-3:3,N)] = (4 .- abs.(-3:3)); b[1] +=  1\nb /= sum(b)\nc = zeros(M); c[1 .+ mod.(-4:4,M)] = (5 .- abs.(-4:4)); c[1] +=  1\nc /= sum(c)\nB = circm(b)\nC = circm(c)\npb = jim(B, \"circulant B\", size=(300,300))\npc = jim(C, \"circulant C\", size=(300,300))\np1 = jim(pb, pc; size=(600,300))\n\nprompt()\n\n\np2 = jim(X; title=\"X original\", size=(300,300))\nY = C * X + X * transpose(B)\np3 = jim(Y; title=\"Y blurred\", size=(300,300))\np23 = jim(p2, p3; size=(600,300))\n\nprompt()\n\nFFT solution\n\nP = fft(c) .+ transpose(fft(b)) # denominator\nXhat = ifft(fft(Y) ./ P)\n@assert Xhat â‰ˆ real(Xhat)\nXhat = real(Xhat)\n@assert Xhat â‰ˆ X","category":"section"},{"location":"generated/demos/08/kron-sum-inv/#Reproducibility","page":"Kronecker sum of circulant","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/06/robust-regress/#robust-regress","page":"Robust regression","title":"Robust regression","text":"This example illustrates robust polynomial fitting with â„“â‚š norm cost functions using the Julia language.\n\nThis page comes from a single Julia file: robust-regress.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: robust-regress.ipynb, or open it in binder here: robust-regress.ipynb.","category":"section"},{"location":"generated/demos/06/robust-regress/#Setup","page":"Robust regression","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Optim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: norm\nusing MIRTjim: prompt\nusing Optim: optimize\nusing Plots: default, plot, plot!, scatter, scatter!, savefig\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto, widen=true, linewidth=2,\n    markersize = 6, tickfontsize=12, labelfontsize = 16, legendfontsize=14)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/06/robust-regress/#Simulated-data-from-latent-nonlinear-function","page":"Robust regression","title":"Simulated data from latent nonlinear function","text":"s = (t) -> atan(4*(t-0.5)) # nonlinear function\n\nseed!(0) # seed rng\nM = 12 # how many data points\ntm = sort(rand(M)) # M random sample locations\ny = s.(tm) + 0.1 * randn(M) # noisy samples\ny[2] = 0.3 # simulate an outlier\ny[M-2] = -0.3 # another outlier\n\nt0 = range(0, 1, 101) # fine sampling for showing curve\nxaxis = (L\"t\", (0,1), 0:0.5:1)\nyaxis = (L\"y\", (-1.2, 1.7), -1:1)\np0 = scatter(tm, y; color=:black, label=\"y (data with outliers)\",\n xaxis, yaxis)\nplot!(t0, s.(t0), color=:blue, label=\"s(t) : latent signal\", legend=:topleft)\n\nprompt()","category":"section"},{"location":"generated/demos/06/robust-regress/#Polynomial-model","page":"Robust regression","title":"Polynomial model","text":"deg = 3 # polynomial degree\nAfun = (tt) -> [t.^i for t in tt, i in 0:deg] # matrix of monomials\nA = Afun(tm) # M Ã— 4 matrix\np1 = plot(title=\"Columns of matrix A\", xlabel=L\"t\", legend=:left)\nfor i in 0:deg\n    plot!(p1, tm, A[:,i+1], marker=:circle, label = \"A[:,$(i+1)]\")\nend\np1\n\nprompt()","category":"section"},{"location":"generated/demos/06/robust-regress/#LS-estimation","page":"Robust regression","title":"LS estimation","text":"This is not robust to the outliers.\n\nxls = A \\ y # backslash for LS solution using all M samples\n\np2 = deepcopy(p0)\nplot!(p2, t0, Afun(t0)*xls, color=:magenta, label=\"LS fit\")\n\nprompt()","category":"section"},{"location":"generated/demos/06/robust-regress/#Robust-regression","page":"Robust regression","title":"Robust regression","text":"Using (differentiable) p-norm with 1  p  2 avoids over-fitting the outlier data points.\n\np = 1.1 # close to â„“â‚\ncost = x -> norm(A * x - y, p)\nx0 = xls # initial guess\noutp = optimize(cost, x0)\nxlp = outp.minimizer\n\nplot!(p2, t0, Afun(t0)*xlp, color=:green, line=:dash,\n label=\"Robust fit p=$p\")\n\nUsing 1-norm produces nearly the same results as using the p=1.1 norm.\n\ncost1 = x -> norm(A * x - y, 1) # â„“â‚\nout1 = optimize(cost1, x0)\nxl1 = out1.minimizer\n\nplot!(p2, t0, Afun(t0)*xl1, color=:orange, line=:dashdot,\n label=\"Robust fit p=1\")\n\n# savefig(p2, \"robust-regress.pdf\")","category":"section"},{"location":"generated/demos/06/robust-regress/#Reproducibility","page":"Robust regression","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/08/markov-chain/#markovchain","page":"Markov chain","title":"Markov chain","text":"This example illustrates a Markov chain using the Julia language.\n\nIn this demo, the elements of the transition matrix P are p_ij = p(X_k+1 = i  X_k = j).\n\nThis page comes from a single Julia file: markov-chain.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: markov-chain.ipynb, or open it in binder here: markov-chain.ipynb.","category":"section"},{"location":"generated/demos/08/markov-chain/#Setup","page":"Markov chain","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing LinearAlgebra: eigen\nusing MIRTjim: prompt\nusing Plots: annotate!, default, plot, plot!, scatter, scatter!\nusing Plots: gif, @animate, Plots\nusing Random: seed!\nusing StatsBase: wsample\n\ndefault(\n markerstrokecolor = :auto, label=\"\",\n labelfontsize=8, legendfontsize=8, size = (1,1) .* 600,\n)\nseed!(0);\nnothing #hide\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide\n\nTransition matrix\n\np = 0.4\nP = [\n 0 0.0 1 0;\n 1 0.0 0 0;\n 0 1-p 0 1;\n 0  p  0 0;\n];\nnothing #hide\n\nDefine plot helpers\n\ncolor = [:red, :green, :blue, :purple];\nxc = [-1, 1, -1, 1] * 20 # node centers\nyc = [1, 1, -1, -1] * 20\n\nfunction plot_circle!(x, y, ic=0; r=10)\n    t = range(0, 2Ï€, 101)\n    plot!(x .+ r * cos.(t), y .+ r * sin.(t), color=color[ic], width=2)\n    annotate!(x, y+0.7r, (\"$ic\", 14, color[ic]))\nend;\nnothing #hide\n\nfunction for plotting the Markov chain diagram\n\nfunction plot_chain(steps::Int)\n\n    plot(xtick=nothing, ytick=nothing, axis=:off, aspect_ratio=1,)\n    plot_circle!.(xc, yc, 1:4)\n\n    for ii in 1:4\n        if P[ii,ii] != 0\n            @show \"Bug\"\n            #add_loop!()\n        end\n        for jj in 1:4\n            ((ii == jj) || P[ii,jj] == 0) && continue\n            xi = xc[ii]\n            xj = xc[jj]\n            yi = yc[ii]\n            yj = yc[jj]\n            xd = xi - xj\n            yd = yi - yj\n            phi = atan(yd, xd) + Ï€/2\n            rd = sqrt(xd^2 + yd^2)\n            frac = (rd - 1*10) / rd\n            xs = frac * xj + (1-frac) * xi\n            xe = frac * xi + (1-frac) * xj\n            ys = frac * yj + (1-frac) * yi\n            ye = frac * yi + (1-frac) * yj\n            xm = (xi + xj) / 2\n            ym = (yi + yj) / 2\n            xo = 5 * cos(phi)\n            yo = 5 * sin(phi)\n            plot!([xs, xe], [ys, ye], arrow=:arrow, color=:black, width=2)\n            annotate!(xm+xo, ym+yo, (\"$(P[ii,jj])\", 13))\n        end\n    end\n    title = steps > 0 ? \"$steps steps\" : \"Initial state\"\n    plot!(; title)\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/08/markov-chain/#Initial-conditions","page":"Markov chain","title":"Initial conditions","text":"This block starts the simulation with an initial grid of 100 particles\n\nxg = repeat(-4.5:4.5, 1, 10)\nyg = xg'\nxg = xg[:]\nyg = yg[:]\nnode = fill(2, length(xg)) # all particles start in node (state) X_0 = 2\nplot_chain(0)\nscatter!(xc[node] + xg[:], yc[node] + yg[:])\n\nUse wsample (weighted sampling) for transitions\n\nfunction node_update!(node)\n    for kk in 1:length(node)\n        node[kk] = wsample(1:size(P,1), P[:,node[kk]]) # random process\n    end\nend;\n\nfunction run_and_plot(iter::Int)\n    node_update!(node)\n    plot_chain(iter)\n    scatter!(xc[node] + xg[:], yc[node] + yg[:])\n    for ii in 1:4\n        tmp = sum(node .== ii) / length(node)\n        annotate!(xc[ii]+10, yc[ii]-10, (\"$tmp\", 7, color[ii]))\n    end\n    plot!()\nend;\nnothing #hide\n\nSimulate\n\nanim1 = @animate for iter in [1:20; 30:10:100]\n    run_and_plot(iter)\nend\ngif(anim1; fps = 4)","category":"section"},{"location":"generated/demos/08/markov-chain/#Clicker-question-1","page":"Markov chain","title":"Clicker question 1","text":"The transition matrix P in this example is (choose most specific correct answer):\n\nA. Square\nB. Nonnegative\nC. Irreducible\nD. Primitive\nE. Positive\"","category":"section"},{"location":"generated/demos/08/markov-chain/#Clicker-question-2-(later)","page":"Markov chain","title":"Clicker question 2 (later)","text":"Which state has the lowest probability in equilibrium?\n\nA 1\nB 2\nC 3\nD 4\nE None: they are all equally likely\"\n\nEigenvectors:\n\n(d, V) = eigen(P)\nround.(V; digits=3)\n\nEigenvalues:\n\n[d abs.(d)] # exactly one Î»=1 and only one |Î»| = 1\n\nPlot eigenvalues in complex plane\n\nscatter(real(d), imag(d), color=:blue,\n xaxis = (\"Re(Î»)\", -1:1),\n yaxis = (\"Im(Î»)\", -1:1),\n framestyle = :origin,\n size = (400,400),\n)\ntmp = range(0, 2Ï€, 301)\nplot!(cos.(tmp), sin.(tmp), color=:black)\n\nprompt()\n\nSteady-state distribution\n\nv = real(V[:,4])\nÏ€ss = v / sum(v) # normalize\n[Ï€ss; \"check:\"; 1 / (p + 4 - 1); p / (p + 4 - 1)]\n\nFor insight: 4^2 - 2  4 + 2 = N^2 - 2N + 2 in Ch8\n\nP^10\n\nApproximate limiting distribution\n\nP^200","category":"section"},{"location":"generated/demos/08/markov-chain/#Reproducibility","page":"Markov chain","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/pca/#pca","page":"PCA","title":"PCA","text":"","category":"section"},{"location":"generated/demos/07/pca/#Principal-component-analysis-(PCA)-illustration","page":"PCA","title":"Principal component analysis (PCA) illustration","text":"This example illustrates PCA of hand-written digit data.\n\nThis page comes from a single Julia file: pca.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: pca.ipynb, or open it in binder here: pca.ipynb.","category":"section"},{"location":"generated/demos/07/pca/#Setup","page":"PCA","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"MLDatasets\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings # nice plot labels\nusing LinearAlgebra: svd\nusing MIRTjim: jim, prompt\nusing MLDatasets: MNIST\nusing Plots: default, gui, plot, savefig, scatter, scatter!\nusing Plots.PlotMeasures: px\nusing Random: seed!, randperm\nusing StatsBase: mean\ndefault(); default(markersize=5, markerstrokecolor=:auto, label=\"\",\n tickfontsize=14, labelfontsize=18, legendfontsize=18, titlefontsize=18)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/demos/07/pca/#Load-data","page":"PCA","title":"Load data","text":"Read the MNIST data for some handwritten digits. This code will automatically download the data from web if needed and put it in a folder like: ~/.julia/datadeps/MNIST/.\n\nif !@isdefined(data)\n    digitn = (0, 1, 4) # which digits to use\n    isinteractive() || (ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true) # avoid prompt\n    dataset = MNIST(Float32, :train)\n    nrep = 60 # how many of each digit\n    # function to extract the 1st `nrep` examples of digit n:\n    data = n -> dataset.features[:,:,findall(==(n), dataset.targets)[1:nrep]]\n    data = cat(dims=4, data.(digitn)...)\n    labels = vcat([fill(d, nrep) for d in digitn]...) # to check later\n    nx, ny, nrep, ndigit = size(data)\n    data = data[:,2:ny,:,:] # make images non-square to force debug\n    ny = size(data,2)\n    data = reshape(data, nx, ny, :)\n    seed!(0)\n    tmp = randperm(nrep * ndigit)\n    data = data[:,:,tmp]\n    labels = labels[tmp]\n    size(data) # (nx, ny, nrep*ndigit)\nend\n\nLook at \"unlabeled\" image data prior to unsupervised dimensionality reduction\n\npd = jim(data, \"Data\"; size=(600,300), cticks=0:1,\n# xticks = false, yticks = false, tickfontsize=12, right_margin=-5px, # book\n)\n# savefig(pd, \"pca-data.pdf\")\n\nCompute sample average of data\n\nÎ¼ = mean(data, dims=3)\npm = jim(Î¼, \"Mean\")\n# savefig(pm, \"pca-mean.pdf\")","category":"section"},{"location":"generated/demos/07/pca/#Scree-plot","page":"PCA","title":"Scree plot","text":"Show singular values.\n\ndata2 = reshape(data .- Î¼, :, nrep*ndigit) # (nx*ny, nrep*ndigit)\nf = svd(data2)\nps = scatter(f.S; title=\"Scree plot\", widen=true,\n xaxis = (L\"k\", (1,ndigit*nrep), [1, 6, ndigit*nrep]),\n yaxis = (L\"Ïƒ_k\", (0,48), [0, 0, 47]),\n)\n# savefig(ps, \"pca-scree.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/07/pca/#Principal-components","page":"PCA","title":"Principal components","text":"The first 6 or so singular values are notably larger than the rest, but for simplicity of visualization here we just use the first two components.\n\nK = 2\nQ = f.U[:,1:K]\npq = jim(reshape(Q, nx,ny,:), \"First $K singular components\"; size=(600,300))\n# savefig(pq, \"pca-q.pdf\")\n\nNow use the learned subspace basis Q to perform dimensionality reduction. The resulting coefficients are called \"factors\" in factor analysis and \"scores\" in PCA.\n\nz = Q' * data2 # (K, nrep*ndigit)","category":"section"},{"location":"generated/demos/07/pca/#PCA-scores","page":"PCA","title":"PCA scores","text":"The three digits are remarkably well separated even in just two dimensions.\n\npz = plot(title = \"Score plot for $ndigit digits\",\n xaxis=(\"Score 1\", (-5,8), -3:3:6),\n yaxis=(\"Score 2\", (-6,4), -4:4:4),\n)\nmarkers = (:circle, :diamond, :square)\nfor (i,d) in enumerate(digitn)\n    scatter!(z[1,labels .== d], z[2,labels .== d], label=\"Digit $d\", marker=markers[i])\nend\npz\n# savefig(pz, \"pca-score.pdf\")\n\nprompt()","category":"section"},{"location":"generated/demos/07/pca/#Reproducibility","page":"PCA","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/09/logistic1/#logistic1","page":"Logistic regression","title":"Logistic regression","text":"Binary classification via logistic regression in Julia.\n\nThis page comes from a single Julia file: logistic1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: logistic1.ipynb, or open it in binder here: logistic1.ipynb.","category":"section"},{"location":"generated/demos/09/logistic1/#Setup","page":"Logistic regression","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: dot, eigvals\nusing MIRTjim: prompt\nusing Plots: default, gui, savefig\nusing Plots: plot, plot!, scatter, scatter!\nusing Random: seed!\nusing StatsBase: mean\ndefault(); default(markersize=6, linewidth=2, markerstrokecolor=:auto, label=\"\",\n tickfontsize=12, labelfontsize=18, legendfontsize=18, titlefontsize=18)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? prompt(:prompt) : prompt(:draw)","category":"section"},{"location":"generated/demos/09/logistic1/#Data","page":"Logistic regression","title":"Data","text":"Generate synthetic data from two classes\n\nif !@isdefined(yy)\n    seed!(0)\n    n0 = 60\n    n1 = 50\n    mu0 = [-1, 1]\n    mu1 = [1, -1]\n    v0 = mu0 .+ randn(2,n0) # class -1\n    v1 = mu1 .+ randn(2,n1) # class 1\n    nex = 0\n    if true # 2017-01-18\n        nex = 4 # extra dim (beyond the 2 shown) to make \"larger scale\"\n        v0 = [v0; rand(nex,n0)] # (2+nex, n0)\n        v1 = [v1; rand(nex,n1)] # (2+nex, n1)\n    end\n    M = n0 + n1 # how many samples\n    yy = [-ones(n0); ones(n1)] # (M) labels\n    vv = [[v0 v1]; ones(1,n0+n1)] # (npar, M) training data\n    npar = 3 + nex # unknown parameters\nend;\nnothing #hide\n\nscatter plot and initial decision boundary\n\nif !@isdefined(ps)\n    x0 = [-1; 3; rand(nex); 5]\n    v1p = range(-1,1,101) * 4\n    v2p_fun = x -> @. (-x[end] - x[1] * v1p) / x[2]\n\n    ps = plot(aspect_ratio = 1, size = (550, 500), legend=:topright,\n     xaxis = (L\"v_1\", (-4, 4), [-4 -1 0 1 4]),\n     yaxis = (L\"v_2\", (-4, 4), [-4 -1 0 1 4]),\n    )\n    plot!(v1p, v2p_fun(x0), color=:red, label=\"initial\")\n    plot!(v1p, v1p, color=:yellow, label=\"ideal\")\n    scatter!(v0[1,:], v0[2,:], color=:green, alpha=0.7)\n    scatter!(v1[1,:], v1[2,:], color=:blue, marker=:square, alpha=0.7)\n    # savefig(ps, \"demo_fgm1_ogm1_s0.pdf\")\nend\nplot(ps)\n\nprompt()","category":"section"},{"location":"generated/demos/09/logistic1/#Cost-function","page":"Logistic regression","title":"Cost function","text":"Logistic regression with Tikhonov regularization:\n\nf(x) = 1_M h(A x) + Î²2  x _2^2\n\nwhere h(z) = log(1 + e^-z) is the logistic loss function.\n\nIts gradient is  f(x) = A doth(A x) + Î² x, and its Lipschitz constant is A_2^2  4 + Î².\n\nif !@isdefined(kost)\n    pot = (t) -> log(1 + exp(-t)) # logistic\n    dpot = (t) -> -1 / (exp(t) + 1)\n    tmp = vv * vv' # (npar, npar) covariance\n    tmp = eigvals(tmp)\n    @show maximum(tmp) / minimum(tmp)\n    pLip = maximum(tmp) / 4 # 1/4 comes from logistic curvature\n\n    reg = 2^0\n    Lip = pLip + reg # Lipschitz constant\n\n    A = yy .* vv'\n    gfun = x -> A' * dpot.(A * x) + reg * x # gradient\n    if false\n        tmp = gfun(x0)\n        @show size(tmp)\n    end\n\n    kost = x -> sum(pot, A * x, dims=1) .+ reg/2 * sum(abs2, x, dims=1)\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/09/logistic1/#GD","page":"Logistic regression","title":"GD","text":"Iterate GD\n\ntol = 1e-6\ntol_gd = tol\ntol_n1 = tol\ntol_o1 = tol\n\nfunction gd(x0, gfun::Function, niter::Int)\n    xg = copy(x0)\n    xgs = copy(xg)\n    for n in 1:niter\n        xold = xg\n        xg -= 1/Lip * gfun(xg)\n        if false && (norm(xg - xold, Inf) / norm(x0, Inf) < tol_gd)\n            @show n\n            break\n        end\n        any(isnan, xg) && throw(\"nan\")\n        # projected GD ??\n        # xg = xg / norm(xg) # decision boundary unaffected by norm!\n        xgs = [xgs xg] # archive\n    end\n    return xgs\nend\n\nif !@isdefined(xgs)\n    niter_gd = 300\n    xgs = gd(x0, gfun, niter_gd)\n    pgs = plot(xgs', xlabel=\"Iteration\", title = \"GD\")\nend\nplot(pgs)\n\nprompt()","category":"section"},{"location":"generated/demos/09/logistic1/#Nesterov-FGM","page":"Logistic regression","title":"Nesterov FGM","text":"do_restart = true;\n\nfunction fgm(x0, grad, Lip::Real, niter::Int)\n    re_nest = Int[]\n    told = 1\n    x = copy(x0)\n    zold = copy(x0)\n    xns = copy(x)\n    for n in 1:niter\n        xold = copy(x)\n        grad = gfun(x)\n        znew = x - 1/Lip * grad\n        zdiff = znew - zold # dk - for restart\n\n        tnew = 1/2 * (1 + sqrt(1 + 4 * told^2))\n\n        x = znew + (told - 1) / tnew * (znew - zold)\n        zold = copy(znew)\n        told = tnew\n        if false && (norm(x - xold, Inf) / norm(x0, Inf) < tol_n1)\n            @show n\n            break\n        end\n        any(isnan, x) && throw(\"nan\")\n\n        if do_restart && (dot(grad, zdiff) > 0) # dk new version\n            told = 1\n            x = copy(znew) # check\n            zold = copy(x)\n            @show \"nest. restart\", n\n            push!(re_nest, n)\n        end\n        xns = [xns x] # archive\n    end\n    return xns, re_nest\nend\n\nif !@isdefined(xns)\n    niter_n1 = 300\n    niter_n1 = 500 # nex\n    xns, re_nest = fgm(x0, gfun, Lip, niter_n1)\n    pns = plot(xns', xlabel = \"Iteration\", title = \"FGM\")\nend\nplot(pns)\n\nprompt()","category":"section"},{"location":"generated/demos/09/logistic1/#OGM","page":"Logistic regression","title":"OGM","text":"Optimized gradient method\n\nfunction ogm(\n    x0,\n    gfun,\n    Lip::Real,\n    niter::Int,\n)\n    re_ogm1 = Int[]\n    told = 1\n    x = copy(x0)\n    zold = copy(x0)\n    x1s = copy(x)\n    for n in 1:niter\n        xold = x\n        grad = gfun(x)\n        znew = x - 1/Lip * grad\n\n        tnew = 1/2 * (1 + sqrt(1 + 4 * told^2))\n\n        x = znew + (told - 1) / tnew * (znew - zold) + told/tnew * (znew - xold)\n        zdiff = znew - zold # dk - for restart\n        zold = znew\n        told = tnew\n        if false && (norm(x - xold, Inf) / norm(x0, Inf) < tol_n1)\n            @show n\n            break\n        end\n        any(isnan, x) && throw(\"nan\")\n\n        if do_restart && (dot(grad, zdiff) > 0) # dk new version\n            push!(re_ogm1, n)\n            told = 1\n            x = znew # check\n            zold = x # dk fixed from x0\n            @info \"ogm1 restart $n\"\n        end\n        x1s = [x1s x] # archive\n    end\n    return x1s, re_ogm1\nend\n\n\nif !@isdefined(x1s)\n    niter_o1 = 300\n    niter_o1 = 500 # nex\n    x1s, re_ogm1 = ogm(x0, gfun, Lip, niter_o1)\n    po1 = plot(x1s', xlabel = \"Iteration\", title = \"OGM\")\nend\nplot(po1)\n\nprompt()\n\nimpartial version of x Íš\n\nxh_tmp = [xgs[:,end] xns[:,end] x1s[:,end]]\nxh = vec(mean(xh_tmp[:,2:3], dims=2)); # GD too slow to include\nnothing #hide\n\nplot cost\n\nextra = do_restart ? \" (restart)\" : \"\"\npc = plot(xaxis=(\"iteration\", (0,10)), yaxis=(\"Cost function\",))\nplot!(0:niter_gd, vec(kost(xgs)) .- kost(xh), label = \"GD\" * extra)\nplot!(0:niter_n1, vec(kost(xns)) .- kost(xh), label = \"FGM\" * extra)\nplot!(0:niter_o1, vec(kost(x1s)) .- kost(xh), label = \"OGM1\" * extra)\n\nprompt()\n\nPlot decision boundaries\n\nif true\n    psh = deepcopy(ps)\n    v2p = @. (-xh[end] - xh[1] * v1p) / xh[2]\n    plot!(psh, v1p, v2p, color = :magenta, label=\"final\")\n# savefig(psh, \"demo-fgm1-fgm1a.pdf\")\nend\nplot(psh)\n\nprompt()","category":"section"},{"location":"generated/demos/09/logistic1/#Plot-iterate-convergence","page":"Logistic regression","title":"Plot iterate convergence","text":"efun1 = (x) -> vec(sqrt.(sum(abs2, x .- xh, dims=1)))\nefun = (x) -> do_restart ? log10.(efun1(x)) : efun1(x)\nifun = (x) -> 0:(size(x,2)-1);\n\npic = plot(\n xaxis = (\"Iteration\", (0, 40+10*nex), 0:20:80),\n yaxis = do_restart ?\n  (L\"\\log_{10}(â€– \\mathbf{x}_k - \\mathbf{x}_* â€–)\", (-3, 1), -3:1) :\n  (L\"â€– \\mathbf{x}_k - \\mathbf{x}_* â€–\", (0, 8), [-1, 0, 8]),\n legend = :topright,\n)\nplot!(ifun(xgs), efun(xgs), color=:green, label = \"GD\")\nplot!(ifun(xns), efun(xns), color=:blue, label = \"Nesterov FGM\" * extra)\nplot!(ifun(x1s), efun(x1s), color=:red, label = \"OGM1\" * extra)\nif do_restart\n    scatter!(re_nest, efun(xns[:, re_nest .+ 1]), color=:blue)\n    scatter!(re_ogm1, efun(x1s[:, re_ogm1 .+ 1]), color=:red)\nend\nplot(pic)\n\nprompt()\n\n# savefig demo_fgm1_ogm1c # restart\n# savefig demo_fgm1_ogm1b # no restart\n\n# todo: compare with LBFGS","category":"section"},{"location":"generated/demos/09/logistic1/#Reproducibility","page":"Logistic regression","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/08/spectral-cluster/#spectral-cluster","page":"Spectral clustering","title":"Spectral clustering","text":"This example illustrates spectral clustering via normalized graph Laplacian applied to hand-written digits.\n\nThis page comes from a single Julia file: spectral-cluster.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: spectral-cluster.ipynb, or open it in binder here: spectral-cluster.ipynb.","category":"section"},{"location":"generated/demos/08/spectral-cluster/#Setup","page":"Spectral clustering","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"Clustering\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"MLDatasets\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing Clustering: kmeans\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings # pretty plot labels\nusing LinearAlgebra: I, norm, Diagonal, eigen\nusing MIRTjim: jim, prompt\nusing MLDatasets: MNIST\nusing Plots: default, gui, plot, scatter, plot!, scatter!\nusing Random: seed!, randperm\nusing StatsBase: mean\ndefault(); default(markersize=5, markerstrokecolor=:auto, label=\"\")\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/demos/08/spectral-cluster/#Load-data","page":"Spectral clustering","title":"Load data","text":"Read the MNIST data for some handwritten digits. This code will automatically download the data from web if needed and put it in a folder like: ~/.julia/datadeps/MNIST/.\n\nif !@isdefined(data)\n    digitn = (0, 1, 3) # which digits to use\n    isinteractive() || (ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true) # avoid prompt\n    dataset = MNIST(Float32, :train)\n    nrep = 30\n    # function to extract the 1st 1000 examples of digit n:\n    data = n -> dataset.features[:,:,findall(==(n), dataset.targets)[1:nrep]]\n    data = cat(dims=4, data.(digitn)...)\n    labels = vcat([fill(d, nrep) for d in digitn]...) # to check later\n    nx, ny, nrep, ndigit = size(data)\n    data = data[:,2:ny,:,:] # make images non-square to force debug\n    ny = size(data,2)\n    data = reshape(data, nx, ny, :)\n    seed!(0)\n    tmp = randperm(nrep * ndigit)\n    data = data[:,:,tmp]\n    labels = labels[tmp]\n    size(data) # (nx, ny, nrep*ndigit)\nend\n\nLook at \"unlabeled\" image data for unsupervised clustering\n\njim(data)\n# savefig(\"spectral-cluster-data.pdf\")\n\nChoose similarity function\n\nÏƒ = 2^-2 # tuning parameter\nsfun(x,z) = exp(-norm(x-z)^2/nx/ny/Ïƒ^2)\n\nWeight matrix\n\nslices = eachslice(data, dims=3)\nW = [sfun(x,z) for x in slices, z in slices]\npw = jim(W, \"weight matrix W\")\n\nDegree matrix\n\nD = Diagonal(vec(sum(W; dims=2)))","category":"section"},{"location":"generated/demos/08/spectral-cluster/#Normalized-graph-Laplacian","page":"Spectral clustering","title":"Normalized graph Laplacian","text":"L = I - inv(D) * W\njim(L, \"Normalized graph Laplacian L\")\n\nEigendecomposition and eigenvalues\n\neig = eigen(L)\npe = scatter(eig.values, xlabel = L\"k\", ylabel=\"Eigenvalues\")\n\nprompt()","category":"section"},{"location":"generated/demos/08/spectral-cluster/#Apply-k-means-to-eigenvectors","page":"Spectral clustering","title":"Apply k-means++ to eigenvectors","text":"K = length(digitn) # try using the known number of digits\nY = eig.vectors[:,1:K]'\nr3 = kmeans(Y, K)\n\nConfusion matrix using class assignments from kmeans++\n\nlabel_list = unique(labels)\n\nresult = zeros(Int, K, length(label_list))\nfor k in 1:K # each cluster\n    rck = r3.assignments .== k\n    for (j,l) in enumerate(label_list)\n        result[k,j] = count(rck .& (l .== labels))\n    end\nend\nresult\n\nVisualize the clustered digits\n\np3 = jim(\n [jim(data[:,:,r3.assignments .== k], \"Class $k\"; prompt=false) for k in 1:K]...\n)\n\nThe clustering here seems only so-so, at least from the digit classification point of view. Each of these digits lives reasonably close to a manifold, and apparently the simply Gaussian similarity function used here does not adequately capture within-manifold similarities.\n\nHowever, there is no reason to think that it is optimal to use the same number of classes as digits. Let's try again using more classes (larger K).\n\nK = 9\nY = eig.vectors[:,1:K]'\nr9 = kmeans(Y, K)\np9 = jim(\n [jim(data[:,:,r9.assignments .== k], \"Class $k\"; prompt=false) for k in 1:K]...\n)\n\nNow there is somewhat more consistency between images in the same class,","category":"section"},{"location":"generated/demos/08/spectral-cluster/#Reproducibility","page":"Spectral clustering","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/09/precon1/#precon1","page":"Preconditioning","title":"Preconditioning","text":"This example illustrates the effects of preconditioning matrices for gradient descent (GD) for least squares (LS) problems, using the Julia language.\n\n2019-11-19 Created by Steven Whitaker\n2023-05-30 Julia 1.9 by Jeff Fessler\n\nThis page comes from a single Julia file: precon1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: precon1.ipynb, or open it in binder here: precon1.ipynb.","category":"section"},{"location":"generated/demos/09/precon1/#Setup","page":"Preconditioning","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: svd, norm, svdvals, eigvals, Diagonal, I\nusing MIRTjim: prompt\nusing Plots: contour, default, gui, plot, plot!, savefig, scatter!\nusing Random: seed!\ndefault(); default(markerstrokecolor=:auto, label = \"\", markersize=6,\n tickfontsize=12, labelfontsize=18, legendfontsize=18)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/09/precon1/#Background","page":"Preconditioning","title":"Background","text":"The cost function to minimize for least squares problems is f(x) = frac12  A x - y _2^2 and its gradient is  f(x) = A (A x - y)\n\nPreconditioned GD with positive definite preconditioner P has the following update:\n\nx_k+1 = x_k - P A (A x_k - y)\n\nFor preconditioned GD to converge from any starting point, the following must be satisfied:\n\n-1  mathrmeigG  1, where G = I - P^12 A A P^12\n\nFurthermore, the closer the eigenvalues of G are to zero, the faster preconditioned GD converges.","category":"section"},{"location":"generated/demos/09/precon1/#Setup-2","page":"Preconditioning","title":"Setup","text":"This notebook creates a matrix A in mathbbR^3 times 2 with specified singular values, and uses y = 0 in mathbbR^3 for simplicity of plots.\n\nCreate random 3 Ã— 2 matrix with singular values 10 and 3\n\nseed!(0)\n(U, _, V) = svd(randn(3,2))\nA = U * [10 0; 0 3] * V';\nnothing #hide\n\nSet up LS cost function and its gradient\n\nf(x) = 0.5 * norm(A * x)^2\nâˆ‡f(x) = A' * (A * x)\n\nFunction for generating the matrix G from a given preconditioner matrix\n\nG(sqrtP) = I - sqrtP' * (A' * A) * sqrtP;\nnothing #hide","category":"section"},{"location":"generated/demos/09/precon1/#Gradient-Descent","page":"Preconditioning","title":"Gradient Descent","text":"First consider regular GD, i.e., preconditioned GD with P = alpha I where Î± is the step size.\n\nWe use the optimal step size Î± = frac2Ïƒ_1^2(A) + Ïƒ_N^2(A)\n\nPick step size (preconditioner P = Î±I)\n\nÎ± = 2 / (sum(svdvals(A' * A)[[1, end]])) # Optimal step size\neigvals(G(sqrt(Î±))) # Eigenvalues of G govern rate of convergence\n\nPlot cost function\n\nx1 = -10:0.1:10\nx2 = -10:0.1:10\nxidx = [[x1[i], x2[j]] for i in 1:length(x1), j in 1:length(x2)]\nscale = 1/1000 # simplify clim\npu = contour(x1, x2, scale * f.(xidx)', annotate = (1, 6, \"Unpreconditioned\"),\n xaxis = (L\"x_1\", (-1,1).*10),\n yaxis = (L\"x_2\", (-1,1).*10),\n size = (500,400),\n);\n\nx0 = [5.0, -8.0] # initial guess\nscatter!(pu, [x0[1]], [x0[2]], color=:green, label = L\"x_0\");\nnothing #hide\n\nRun GD\n\nniter = 100\nx = Vector{Vector{Float64}}(undef, niter + 1)\nx[1] = x0\nfor k in 1:niter\n    x[k+1] = x[k] - Î± * âˆ‡f(x[k])\nend\n\nDisplay iterates\n\nplot!(pu, [x[k][1] for k in 1:niter], [x[k][2] for k in 1:niter],\n    marker=:star, color=:blue, label = L\"x_k\");\nnothing #hide\n\nMark the minimum of the cost function\n\nscatter!(pu, [0], [0], label = L\"\\hat{x}\", color=:red,\n    aspect_ratio = :equal, marker = :x)\n\n# savefig(pu, \"precon1-pu.pdf\")\n\nprompt()\n\nThe contours of our cost function f(x) are ellipses. The ratio of the singular values of A determines the eccentricity, or how oblong (non-circular) the ellipse is. In our case, the singular values are 10 and 3, so the major axis of the contour ellipse is 10/3 as long as the minor axis.","category":"section"},{"location":"generated/demos/09/precon1/#Preconditioned-Gradient-Descent","page":"Preconditioning","title":"Preconditioned Gradient Descent","text":"Now let's see how adding a preconditioner matrix changes things.\n\nManipulating the preconditioned GD step for LS problems leads to the following update:\n\nx_k+1 = x_k - P A (A x_k - y)\n\nP^-12 x_k+1 = P^-12 x_k - P^12 A (A x_k - y)\n\nP^-12 x_k+1 = P^-12 x_k - P^12 A (A P^12 P^-12 x_k - y)\n\nz_k+1 = z_k - P^12 A (A P^12 z_k - y), where z_k = P^-12 x_k\n\nz_k+1 = z_k - tildeA (tildeA z_k - y), where tildeA = A P^12, and we used the fact that P^12 is Hermitian symmetric.\n\nThis last equation is the normal (not preconditioned) GD step (with step size 1) for a LS problem with cost function tildef(z) = frac12  tildeA z - y _2^2.","category":"section"},{"location":"generated/demos/09/precon1/#Clicker-Question","page":"Preconditioning","title":"Clicker Question","text":"The preconditioned LS cost function tildef relates to the non-preconditioned LS cost function f via the relation tildef(z) = f(g(z)) for what function g?\n\nA. g(z) = P^-12 z\nB. g(z) = P^12 z\nC. g(z) = z\nD. g(z) = tildeA z\nE. g(z) = tildeA z","category":"section"},{"location":"generated/demos/09/precon1/#Ideal-Preconditioner","page":"Preconditioning","title":"Ideal Preconditioner","text":"We first consider the ideal preconditioner P = (A A)^-1.\n\nCompute ideal preconditioner\n\nsqrtPideal = sqrt(inv(A' * A))\neigvals(G(sqrtPideal))\n\nSet up preconditioned cost function and its gradient\n\nfÌƒideal(z) = f(sqrtPideal * z)\nâˆ‡fÌƒideal(z) = sqrtPideal' * âˆ‡f(sqrtPideal * z);\nnothing #hide\n\nPlot preconditioned cost function\n\nz1 = -40:40\nz2 = -40:40\nzidx = [[z1[i], z2[j]] for i in 1:length(z1), j in 1:length(z2)]\nscale = 1/250 # simplify clim\nph = contour(z1, z2, scale * fÌƒideal.(zidx)',\n annotate = (9, 24, \"Ideal preconditioner\"),\n xaxis = (L\"z_1\", (-1,1).*40),\n yaxis = (L\"z_2\", (-1,1).*40),\n size = (500,400),\n);\nnothing #hide\n\nTransform initial x guess into z coordinates and plot\n\nz0 = sqrtPideal \\ x0\nscatter!(ph, [z0[1]], [z0[2]], color=:green, label = L\"z_0\");\nnothing #hide\n\nRun GD\n\nzk = z0 - âˆ‡fÌƒideal(z0)\n\nDisplay iterates\n\nplot!(ph, [z0[1],zk[1]], [z0[2],zk[2]], marker=:star, color=:blue, label = L\"z_k\");\nnothing #hide\n\nMark the minimum of the preconditioned cost function\n\nscatter!(ph, [0], [0], label = L\"\\hat{z}\", color=:red,\n    aspect_ratio = :equal, marker = :x)\n\n# savefig(ph, \"precon1-ph.pdf\")\n\nprompt()\n\nUsing the ideal preconditioner caused a coordinate change in which the contours of our cost function are circles. In this new coordinate system, the negative gradient of our cost function points towards the minimizer. Furthermore, with the ideal preconditioner GD converged in just one step, which agrees with the fact that the eigenvalues of G for this preconditioner are 0 (ignoring numerical precision issues). Unfortunately, computing the ideal preconditioner is expensive.","category":"section"},{"location":"generated/demos/09/precon1/#Diagonal-Preconditioner","page":"Preconditioning","title":"Diagonal Preconditioner","text":"A less expensive preconditioner is the diagonal preconditioner P = alpha  mathrmdiagA A 1_N^-1. For convergence, we must have 0  alpha  2. We use an empirically chosen value for Î± in that range.\n\nPick step size and compute diagonal preconditioner\n\nÎ± = 1.71 # Chosen empirically\nsqrtPdiag = sqrt(Î± * inv(Diagonal(abs.(A' * A) * ones(size(A, 2)))))\neigvals(G(sqrtPdiag))\n\nSet up preconditioned cost function and its gradient\n\nfÌƒdiag(z) = f(sqrtPdiag * z)\nâˆ‡fÌƒdiag(z) = sqrtPdiag' * âˆ‡f(sqrtPdiag * z);\nnothing #hide\n\nPlot preconditioned cost function\n\nz1 = -50:50\nz2 = -50:50\nzidx = [[z1[i], z2[j]] for i in 1:length(z1), j in 1:length(z2)]\nscale = 1/500 # simplify clim\npd = contour(z1, z2, scale * fÌƒdiag.(zidx)',\n annotate = (12, 30, \"Diagonal preconditioner\"),\n xaxis = (L\"z_1\", (-1,1).*50),\n yaxis = (L\"z_2\", (-1,1).*50),\n size = (500,400),\n);\nnothing #hide\n\nTransform initial x guess into z coordinates and plot\n\nz0 = sqrtPdiag \\ x0\nscatter!(pd, [z0[1]], [z0[2]], color=:green, label = L\"z_0\");\nnothing #hide\n\nRun GD\n\nniter = 100\nz = Array{Array{Float64,1},1}(undef, niter + 1)\nz[1] = z0\nfor k in 1:niter\n    z[k+1] = z[k] - âˆ‡fÌƒdiag(z[k])\nend;\nnothing #hide\n\nDisplay iterates\n\nplot!(pd, [z[k][1] for k in 1:niter], [z[k][2] for k in 1:niter],\n    marker=:star, color=:blue, label = L\"z_k\");\nnothing #hide\n\nMark the minimum of the preconditioned cost function\n\nscatter!(pd, [0], [0], label = L\"\\hat{z}\", color=:red,\n    aspect_ratio = :equal, marker = :x)\n\n# savefig(pd, \"precon1-pd.pdf\")\n\nprompt()\n\nUsing the diagonal preconditioner did cause a coordinate change, but one less dramatic than did the ideal preconditioner. The contours in this new coordinate system are still ellipses, but they are slightly more circular. Using the diagonal preconditioner also resulted in eigenvalues of G that are smaller than when using (non-preconditioned) GD with optimal step size, and one can see that using the diagonal preconditioner appears to converge more quickly.\n\nThe following reports the ratio of the singular values of the three different A (or tildeA) matrices used here. A value of 1 corresponds to circular cost function contours, and higher values correspond to more elliptical contours.\n\n\"Ratio of singular values of A, A * sqrtPideal A * sqrtPdiag:\"\n\n[\n/(svdvals(A)...)\n/(svdvals(A * sqrtPideal)...)\n/(svdvals(A * sqrtPdiag)...)\n]\n\nHere are the three plots displayed next to each other.\n\npp = plot(\n    plot!(pu, title = \"GD\"),\n    plot!(ph, title = \"Ideal\"),\n    plot!(pd, title = \"Diagonal\"),\n    size = (1900,470),\n    layout=(1,3),\n)\n\n# savefig(pp, \"precon1-pp.pdf\")","category":"section"},{"location":"generated/demos/09/precon1/#Reproducibility","page":"Preconditioning","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/02/dot/#dot","page":"Vector dot product","title":"Vector dot product","text":"This example illustrates different ways of computing vector dot products using the Julia language.\n\nThis page comes from a single Julia file: dot.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: dot.ipynb, or open it in binder here: dot.ipynb.","category":"section"},{"location":"generated/demos/02/dot/#Setup","page":"Vector dot product","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"BenchmarkTools\"\n        \"InteractiveUtils\"\n        \"LazyGrids\"\n        \"LinearAlgebra\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing BenchmarkTools: @benchmark\nusing InteractiveUtils: versioninfo\nusing LazyGrids: btime\nusing LinearAlgebra: dot","category":"section"},{"location":"generated/demos/02/dot/#Overview-of-dot-products","page":"Vector dot product","title":"Overview of dot products","text":"The dot product between two vectors is such a basic method in linear algebra that, of course, Julia has a function dot built-in for it.\n\nIn practice one should simply call that dot method.\n\nThis demo explores other ways of coding the dot product, to illustrate, in a simple setting, techniques for writing efficient code.\n\nWe write each method as a function because the most reliable way to benchmark different methods is to use functions.","category":"section"},{"location":"generated/demos/02/dot/#The-built-in-dot-method:","page":"Vector dot product","title":"The built-in dot method:","text":"f1(x,y) = dot(y,x);\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#An-equivalent-method-using-the-adjoint-'","page":"Vector dot product","title":"An equivalent method using the adjoint '","text":"It can be written y' * x or *(y', x). By checking @which *(y', x) one can verify that these all call dot.\n\nf2(x,y) = y'x;\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#Using-sum-with-vector-conjugate","page":"Vector dot product","title":"Using sum with vector conjugate","text":"This is suboptimal because it must allocate memory for conj(y)\n\nf3(x,y) = sum(conj(y) .* x); # must allocate \"conj(y)\"\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#Using-zip-and-sum-with-a-function-argument","page":"Vector dot product","title":"Using zip and sum with a function argument","text":"This approach avoids the needless allocation.\n\nf4(x,y) = sum(z -> z[1] * conj(z[2]), zip(x,y));\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#A-basic-for-loop-like-one-would-write-in-a-low-level-language","page":"Vector dot product","title":"A basic for loop like one would write in a low-level language","text":"function f5(x,y)\n    accum = zero(promote_type(eltype(x), eltype(y)))\n    for i in 1:length(x)\n        accum += x[i] * conj(y[i])\n    end\n    return accum\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#An-advanced-for-loop-that-uses-bounds-checking-and-SIMD-operations","page":"Vector dot product","title":"An advanced for loop that uses bounds checking and SIMD operations","text":"function f6(x,y)\n    accum = zero(promote_type(eltype(x), eltype(y)))\n    @boundscheck length(x) == length(y) || throw(\"incompatible\")\n    @simd for i in 1:length(x)\n        @inbounds accum += x[i] * conj(y[i])\n    end\n    return accum\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#The-Julia-fallback-method-(from-source-code-as-of-v1.8.1)","page":"Vector dot product","title":"The Julia fallback method (from source code as of v1.8.1)","text":"This code is what is used for general AbstractArray types.\n\nfunction f7(x,y)\n    accum = zero(promote_type(eltype(x), eltype(y)))\n    @boundscheck length(x) == length(y) || throw(\"incompatible\")\n    for (ix,iy) in zip(eachindex(x), eachindex(y))\n        @inbounds accum += x[ix] * conj(y[iy]) # same as dot(y[iy], x[ix])\n    end\n    return accum\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/02/dot/#Data-for-timing-tests","page":"Vector dot product","title":"Data for timing tests","text":"N = 2^16; x = rand(ComplexF32, N); y = rand(ComplexF32, N)\n\nVerify the methods are equivalent\n\n@assert f1(x,y) == f2(x,y) â‰ˆ f3(x,y) â‰ˆ f4(x,y) â‰ˆ f5(x,y) â‰ˆ f6(x,y) â‰ˆ f7(x,y)","category":"section"},{"location":"generated/demos/02/dot/#Benchmark-the-methods","page":"Vector dot product","title":"Benchmark the methods","text":"The results will depend on the computer used, of course.\n\ny'x\n\nt = @benchmark f1($x,$y)\ntimeu = t -> btime(t, unit=:Î¼s)\ntimeu(t)\n\ndot(y,x)\n\nt = @benchmark f2($x,$y)\ntimeu(t)\n\nsum with conj()\n\nt = @benchmark f3($x,$y)\ntimeu(t)\n\nzip sum\n\nt = @benchmark f4($x,$y)\ntimeu(t)\n\nbasic loop\n\nt = @benchmark f5($x,$y)\ntimeu(t)\n\nfancy loop with @inbounds & @simd\n\nt = @benchmark f6($x,$y)\ntimeu(t)\n\nzip accum loop\n\nt = @benchmark f7($x,$y)\ntimeu(t)","category":"section"},{"location":"generated/demos/02/dot/#Remarks","page":"Vector dot product","title":"Remarks","text":"The built-in dot method is the fastest. Behind the scenes it calls BLAS.dot which is highly optimized because it uses cpu specific assembly code based on Single instruction, multiple data (SIMD) to perform, say, 4 multiplies in a single instruction. Thus the basic loop is several times slower than dot().\n\nSometimes we can speed up code by promising the Julia compiler that array indexing operations like x[i] are valid, by adding the @inbounds macro.\n\nDepending on the CPU, using @simd and @inbounds can lead to speeds close to that of dot.\n\nThe promote_type function ensures that the accumulator uses the better precision of the two arguments.","category":"section"},{"location":"generated/demos/02/dot/#Reproducibility","page":"Vector dot product","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/08/ssc/#sparse-spectral-cluster","page":"Sparse spectral clustering (SSC)","title":"Sparse spectral clustering (SSC)","text":"This example illustrates sparse spectral clustering using POGM applied to simulated data and (todo) hand-written digits.\n\nOriginal version by Javier Salazar Cavazos.\n\nThis page comes from a single Julia file: ssc.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: ssc.ipynb, or open it in binder here: ssc.ipynb.","category":"section"},{"location":"generated/demos/08/ssc/#Setup","page":"Sparse spectral clustering (SSC)","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"Clustering\"\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRT\"\n        \"MIRTjim\"\n        \"MLDatasets\"\n        \"Plots\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing Clustering: kmeans\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, eigen, I, opnorm\nusing MIRT: pogm_restart\nusing MIRTjim: jim, prompt\nusing Plots: default, gui, palette, plot, plot!, scatter, scatter!\nusing Random: randperm, seed!\ndefault(); default(markersize=5, markerstrokecolor=:auto, label=\"\")\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/demos/08/ssc/#Synthetic-data","page":"Sparse spectral clustering (SSC)","title":"Synthetic data","text":"Generate synthetic data points in â„Â² that lie along K = 2 subspaces in the span of (11) and (1-1).\n\nseed!(3) # fix random generation for better debugging\n\nxval = -9.5:1:9.5 # x locations before adding noise\n\nN = length(xval)\nÏƒ = 0.5\nx1 = xval + Ïƒ * randn(N) # noisy data that lies on union of subspaces\nx2 = xval + Ïƒ * randn(N)\ny1 = 1 * x1 .+ Ïƒ * randn(N) # y=x and y=-x are the 2 subspaces\ny2 = -1 * x2 .+ Ïƒ * randn(N)\n\ndata = [ [x1';y1'] [x2';y2'] ] # gathering data to one matrix\nclusters = [1*ones(Int,20); 2*ones(Int,20)]; # ground-truth clusters\n\nif true # permute data points\n    permuteOrder = randperm(40)\n    data = data[:,permuteOrder]\n    clusters = clusters[permuteOrder]\nend\nreord = invperm(permuteOrder)\n\nplot subspaces and data points\n\np0 = plot(aspect_ratio = 1, size = (550, 500), xlabel=L\"x_1\", ylabel=L\"x_2\")\nplot!(p0, xval, 1 .* xval)\nplot!(p0, xval, -1 .* xval)\npd = deepcopy(p0)\nscatter!(pd, x1, y1, color=1)\nscatter!(pd, x2, y2, color=2)\nplot!(pd, title = \"Data and Subspaces\")","category":"section"},{"location":"generated/demos/08/ssc/#POGM-for-SSC","page":"Sparse spectral clustering (SSC)","title":"POGM for SSC","text":"Solve the SSC problem with the self-representation cost function\n\nargmin_C (12)  Y (M  C) - Y _mathrmFÂ² + Î»  C _11\n\nwhere M is a mask matrix that is unity everywhere except 0 along the diagonal that forces each column of Y to be represented as a (sparse) linear combination of other columns of Y. The regularizer encourages sparsity of C.\n\nPOGM is an optimal accelerated optimization method for convex composite cost functions.\n\nhttps://doi.org/10.1007/s10957-018-1287-4\nhttps://doi.org/10.1137/16m108104x\n\nÎ» = 0.001 # regularization parameter for sparsity term\nLf = opnorm(data,2)^2 # Lipschitz constant for âˆ‡f: smooth term of obj function\nnpoint = size(data,2) # total # of points\nx0 = zeros(npoint, npoint) # initialize solution\nM = 1 .- I(npoint); # mask to force diag(C)=0\n\ngrad(x) = M .* (data' * (data*x - data)) # âˆ‡f: gradient of smooth term\nsoft(x,t) = sign(x) * max(abs(x) - t, 0) # soft threshold at t\ng_prox(z,c) = soft.(z, c * Î») # proximal operator for c*b*|x|_1\n\nniter = 1000\nA, _ = pogm_restart(x0, x->0, grad, Lf ; g_prox, niter) # POGM method\njim(A[reord,reord], \"A\")","category":"section"},{"location":"generated/demos/08/ssc/#Spectral-clustering","page":"Sparse spectral clustering (SSC)","title":"Spectral clustering","text":"Cluster via a spectral method; see:\n\nhttps://doi.org/10.1109/JSTSP.2018.2867446\nhttps://doi.org/10.1109/TPAMI.2013.57\n\nW = transpose(abs.(A)) + abs.(A) # Weight matrix, force Hermitian\njim(W[reord,reord], \"W\")\n\nD = vec(sum(W, dims=2)) # degree matrix of graph\nD = D .^ (-1/2)\nD = Diagonal(D) # normalized symmetric Laplacian formula\nL = I - D * W * D\njim(L[reord,reord], \"L\")\n\nFor K=2 subspaces we pick the bottom K eigenvectors (smallest Î»)\n\nK = 2\nE = eigen(L) # eigen value decomposition, really only need vectors\neigenVectors = E.vectors[:, 1:K];\n\nseriescolor = palette([:orange, :skyblue], 2)\np4 = scatter(eigenVectors[:,1], eigenVectors[:,2],\n title=\"Spectral Embedding Plot\",\n marker_z = clusters;\n seriescolor,\n colorbar = nothing,\n)\n\nSince there are K subspaces, we look for K clusters in rows of eigenvectors using kmeans\n\nresults = kmeans(eigenVectors', K)\nassign = results.assignments; # store assignments\nnothing #hide\n\nPlot truth (on the left) and SSC results (on the right)\n\np1 = deepcopy(p0)\nscatter!(p1, data[1,:], data[2,:];\n aspect_ratio = 1, size = (550, 450),\n xlims = (-11,11), ylims = (-11,11),\n marker_z = clusters,\n seriescolor,\n colorbar = nothing,\n title = \"Truth\",\n);\n\np2 = deepcopy(p0)\nscatter!(p2, data[1,:], data[2,:];\n aspect_ratio = 1, size = (550, 450),\n xlims = (-11,11), ylims = (-11,11),\n marker_z = assign,\n seriescolor,\n colorbar = nothing,\n title = \"SSC (POGM)\",\n)\np12 = plot(p1, p2, layout = (1, 2), size=(1100, 450))","category":"section"},{"location":"generated/demos/08/ssc/#Reproducibility","page":"Sparse spectral clustering (SSC)","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/07/photometric3/#photometric3","page":"Photometric stereo","title":"Photometric stereo","text":"This example illustrates photometric stereo for Lambertian surfaces using the Julia language.\n\nThis method determines the surface normals of an object from 3 or more pictures of the object taken with different lighting directions.\n\nThis demo follows the \"uncalibrated\" approach of Hayakawa, JOSA, 1994 that treats the lighting directions as being unknown, unlike the original least-squares approach of Woodham, 1980.\n\nThis page comes from a single Julia file: photometric3.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: photometric3.ipynb, or open it in binder here: photometric3.ipynb.","category":"section"},{"location":"generated/demos/07/photometric3/#Setup","page":"Photometric stereo","title":"Setup","text":"Add the Julia packages used in this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"Downloads\"\n        \"InteractiveUtils\"\n        \"LinearAlgebra\"\n        \"LaTeXStrings\"\n        \"MIRTjim\"\n        \"NPZ\"\n        \"Plots\"\n        \"Printf\"\n        \"Random\"\n    ])\nend\n\nTell Julia to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing Downloads: download\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, svd, svdvals, rank, norm, pinv\nusing MIRTjim: jim, prompt\nusing NPZ: npzread\nusing Plots: gui, plot, scatter, scatter!, ylims!, cgrad, default, RGB, savefig\nusing Plots.PlotMeasures: px\nusing Printf: @sprintf\nusing Random: seed!\ndefault(); default(label=\"\", markerstrokecolor=:auto)\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/07/photometric3/#Ground-truth-surface-normals","page":"Photometric stereo","title":"Ground-truth surface normals","text":"Load ground truth surface normal vectors of \"bunny\" data used in 2012 CVPR paper by Ikehata et al..\n\nif !@isdefined(gt_normal_bunny)\n    url = \"https://github.com/yasumat/RobustPhotometricStereo/raw/master/data/bunny/gt_normal.npy\"\n    tmp = npzread(download(url))\n    i1, i2 = 32:256-25, 33:212 # crop to reduce compute\n    tmp = permutedims(tmp, [2, 1, 3]) # \"transpose\"\n    gt_normal_bunny = tmp[i1, i2, :] # crop\nend;\nnothing #hide\n\nCreate hemisphere to augment the bunny data.\n\nfunction hemi_normal(x, y ; # surface normal of a hemi-ellipsoid\n    xh = 20f0, yh = xh, zh = xh,\n)\n    tmp = (x/xh)^2 + (y/yh)^2\n    if tmp < 1\n        z = zh * sqrt(1 - tmp)\n        tmp = [x/xh^2, y/yh^2, z/zh^2]\n        return tmp / norm(tmp)\n    end\n    return [0, 0, 0]\nend;\n\nif true\n    rh = 20\n    x = -rh:rh\n    y = x\n    tmp = hemi_normal.(x, y'; xh=rh)\n    tmp = reduce(hcat, tmp)\n    tmp = reshape(tmp', 2rh+1, 2rh+1, 3)\nend;\nnothing #hide\n\nDefine ground truth normals as combination of bunny normals and hemisphere normals.\n\nif !@isdefined(gt_normal)\n    gt_normal = copy(gt_normal_bunny)\n    gt_normal[176 .+ x, 25 .+ y, :] = tmp\n    nx, ny = size(gt_normal)[1:2]\n    shape2 = x -> reshape(x, prod(size(x)[1:2]), :)\n    shape3 = x -> reshape(x, nx, ny, :)\nend;\nnothing #hide\n\nThe three images are the x, y, and z components\n\npn_gt = jim(gt_normal; title=\"Ground-truth normals\", nrow=1,\n    xticks = false, yticks = false, labelfontsize = 16, tickfontsize=12,\n    left_margin = 20px, right_margin = 30px,\n#  xaxis=L\"x\", yaxis=L\"y\",\n    size=(600,200), clim=(-1,1), colorbar_ticks=-1:1,\n)\n# savefig(pn_gt, \"photometric3_gt.pdf\")\n\nSurface normals are meaningful only where the object is present, so we determine an object \"mask\".\n\nmask = dropdims(sum(abs, gt_normal, dims=3), dims=3) .> eps(Float32)\npm = jim(mask, \"Mask\"; cticks=0:1)\n\nVerify that the surface normals are unit norm (within the mask).\n\n@assert maximum(abs, sum(abs2, gt_normal, dims=3)[vec(mask)] .- 1) < 1e-12\n\nView angle of surface normal w.r.t. z-axis\n\nif true\n    tmp = sqrt.(sum(abs2, gt_normal[:,:,1:2]; dims=3))\n    tmp = rad2deg.(atan.(tmp, gt_normal[:,:,3]))\n    pza = jim(tmp; title=\"Angle of surface normal w.r.t. z axis\",\n        ctitle=\"degrees\", cticks=0:30:90)\nend","category":"section"},{"location":"generated/demos/07/photometric3/#Lighting-directions","page":"Photometric stereo","title":"Lighting directions","text":"Define lighting directions for simulated object views.\n\nfunction light_vector( ;\n    Î¸ = rand() * 2Ï€,\n    r = 1/sqrt(2),\n    xoffset = 0.2,\n    yoffset = 0.2,\n    x = r * cos(Î¸) + xoffset,\n    y = r * sin(Î¸) + yoffset,\n)\n    z = sqrt(1 - x^2 - y^2)\n    return [x, y, z]\nend;\nnothing #hide\n\nDeliberately asymmetric directions to aid testing\n\nif !@isdefined(Ltrue)\n    nlight = 12 # number of lighting directions\n    Ltrue = [light_vector(; Î¸=il/nlight*1.7Ï€, r=0.5-0.1*il/nlight) for il in 1:nlight]\n    Ltrue = reduce(hcat, Ltrue) # (3, nlight)\n    @assert maximum(abs, sum(abs2, Ltrue; dims=1) .- 1) < 9eps()\n    @show extrema(Ltrue[3,:])\nend;\nnothing #hide\n\nPlot lighting directions\n\ntmp = range(0, 2Ï€, 361)\nplot(cos.(tmp), sin.(tmp); aspect_ratio=1, color=:black,\n    xaxis = (L\"x\", (-1,1)),\n    yaxis = (L\"y\", (-1,1)),\n    title = \"Lighting directions\",\n)\npl_gt = scatter!(Ltrue[1,:], Ltrue[2,:]; label = \"True\", color=:red)\n\nprompt()","category":"section"},{"location":"generated/demos/07/photometric3/#Synthesize-images","page":"Photometric stereo","title":"Synthesize images","text":"Synthesize image data using Lambertian reflectance model.\n\nFor the Lambertian model, each pixel value is proportional to the inner product of the lighting direction with the corresponding surface normal.\n\nIf that inner product is zero for some pixels, then the image contains \"shadows\" at those pixels. Wu et al., ACCV, 2011 describe the max(â‹…,0) operation below as \"attached shadows.\"\n\nif !@isdefined(images)\n    images_ideal = shape2(gt_normal) * Ltrue # hypothetical Lambertian\n    images_ideal ./= maximum(images_ideal) # normalize\n    svdval_ideal = svdvals(images_ideal)\n    images = max.(images_ideal, 0) # \"shadows\" if lighting is â‰¥ 90Â° from normal\n    svdval_images = svdvals(images)\n    images = shape3(images)\nend;\nnothing #hide\n\nNote the different shadings in the different images. Obviously the bunny cannot \"jump around\" during the imaging...\n\npd = jim(images; title=\"Images for $nlight different lighting directions\",\n#  xticks = false, yticks = false, tickfontsize=12, # book\n    caxis=(\"Intensity\", (0,1), 0:1),\n)\n\n# savefig(pd, \"photometric3_data.pdf\")","category":"section"},{"location":"generated/demos/07/photometric3/#Low-rank-structure","page":"Photometric stereo","title":"Low-rank structure","text":"Examine the singular values. Ideally (i.e., ignoring shadows), there would be (at most) 3 nonzero singular values, because images_ideal is the product of (npixel Ã— 3) object normal matrix with a (3 Ã— nlight) lighting direction matrix, so its rank is at most 3.\n\nWith self-shadow effects, e.g., Barsky & Petrou, 2003, T-PAMI, there are 3 dominant singular values and additional small but non-negligible values. The images matrix rank is not exactly 3 because of the shadow effects.\n\nif !@isdefined(ps)\n    ps1 = scatter(svdval_ideal, label=\"Ideal\", color=:blue,\n       xaxis=(L\"k\", (1,nlight), [1,3,4,nlight]),\n       yaxis=(L\"Ïƒ_k\",), marker=:x, widen=true,\n       title=\"Singular values\")\n    scatter!(ps1, svdval_images, label=\"Realistic\", color=:red)\n\n    good = all(>(0), images, dims=3)\n    images_good = shape2(images)[vec(good),:]\n    svdval_good = svdvals(images_good)\n    scatter!(ps1, svdval_good, label=\"Good pixels\", color=:green)\n    ps4 = deepcopy(ps1)\n    ylims!(ps4, (0,5); title=\"zoom\")\n    ps = plot(ps1, ps4)\nend\n\nprompt()\n\n# savefig(ps, \"photometric3_svdvals.pdf\")","category":"section"},{"location":"generated/demos/07/photometric3/#Rank-3-approximation","page":"Photometric stereo","title":"Rank-3 approximation","text":"To make a low-rank approximation, collect image data into a npixel Ã— nlight matrix and use a SVD.\n\nEstimate the lighting directions using only the pixels with no shadows.\n\ntmp = svd(images_good)\nlight1 = tmp.Vt[1:3,:] # right singular vectors\nnormal1 = tmp.U[:,1:3] * Diagonal(tmp.S[1:3])\n@assert norm(images_good - normal1 * light1) / norm(images_good) < 9eps()","category":"section"},{"location":"generated/demos/07/photometric3/#Estimate-lighting-directions","page":"Photometric stereo","title":"Estimate lighting directions","text":"Apply method of Hayakawa, JOSA, 1994 to resolve non-uniqueness issue, under the simplifying assumption (satisfied here) that the light intensity is the same for all lighting directions.\n\nThat method does not explicitly exploit the fact that AA is positive semi-definite. Challenge: develop method that does use that property.\n\nAbig = reduce(vcat, map(c -> kron(c', c'), eachcol(light1)))\ntmp = Abig \\ ones(nlight)\nB = reshape(tmp, 3, 3) # B = A'A\n@assert B â‰ˆ B' # (symmetry check)\nA = sqrt(B)\n@assert A'A â‰ˆ B\nlight2 = A * light1\n@assert maximum(abs, sum(abs2, light2, dims=1) .- 1) < 30eps()\n\nnormal2 = normal1 * inv(A)\n@assert norm(images_good - normal2 * light2) / norm(images_good) < 10eps()\n@assert maximum(abs, norm.(eachrow(normal2)) .- 1) < 9e-6 # already unit norm!\n\nAs described in Hayakawa, JOSA, 1994, the estimated lighting and surface normals are in an arbitrary 3D coordinate system. To display them in a useful way, we use the Procrustes method to align the coordinate system with that of the original lighting.\n\nif true\n    tmp = Ltrue[:,1:3] * light2[:,1:3]' # use just the first 3 sources\n    tmp = svd(tmp)\n    tmp = tmp.U * tmp.Vt\n    light3 = tmp * light2\n    normal3 = normal2 * tmp'\n    @assert norm(images_good - normal3 * light3) / norm(images_good) < 10eps()\nend\n\nPlot estimated lighting directions (after coordinate system alignment)\n\ntmp = deepcopy(pl_gt)\npl = scatter!(tmp, light3[1,:], light3[2,:],\n    marker = :x, label = \"Estimated\", color=:blue)\n\nprompt()","category":"section"},{"location":"generated/demos/07/photometric3/#Estimate-surface-normals.","page":"Photometric stereo","title":"Estimate surface normals.","text":"Having estimated the lighting directions, return to estimate the surface normals for all pixels, not just the \"good\" pixels.\n\nnormal3 = shape3(shape2(images) * pinv(light3));\nnothing #hide\n\nExamine the estimated surface normals. The accuracy is very good, except in the shadow regions.\n\npn_hat = jim(normal3; nrow=1, title=\"Estimated normals\",\n    xticks = false, yticks = false, labelfontsize = 16, tickfontsize=12,\n    left_margin = 20px, right_margin = 30px,\n#  xaxis=L\"x\", yaxis=L\"y\",\n    size=(600,200), clim=(-1,1), colorbar_ticks=-1:1,\n)\n# savefig(pn_hat, \"photometric3_hat.pdf\")\nRGB255(args...) = RGB((args ./ 255)...)\ncolor = cgrad([RGB255(230, 80, 65), :black, RGB255(23, 120, 232)])\npn_d = jim(normal3 - gt_normal; nrow=1, title=\"Difference\", color,\n    xticks = false, yticks = false, labelfontsize = 16, tickfontsize=12,\n    left_margin = 20px, right_margin = 30px,\n    xaxis=L\"x\", yaxis=L\"y\", size=(600,200), clim=(-1,1), colorbar_ticks=-1:1,\n)\npn = jim(\n pn_gt,\n pn_hat,\n pn_d,\n layout=(3,1),\n size=(550, 600),\n)\n\n# savefig(pn, \"photometric3_pn1.pdf\")\n\nThis demo illustrates the utility of the SVD and low-rank matrix approximation.\n\nMore advanced methods handle shadows by allowing sparse errors, e.g.,\n\nWu et al., ACCV 2011\nIkehata et al., CVPR 2012,\n\nor handle more general lighting conditions, e.g.,\n\nBasri & Jacobs, CVPR 2001.","category":"section"},{"location":"generated/demos/07/photometric3/#Exercise","page":"Photometric stereo","title":"Exercise","text":"Apply the method described above to the bunny data used in Ikehata et al., CVPR 2012. This is a set of 50 images under various lighting directions.\n\nAs a starting point, here we load that data.\n\nif !@isdefined(images_bunny)\n    url0 = \"https://github.com/yasumat/RobustPhotometricStereo/raw/master/data/bunny/bunny_lambert/image000.npy\"\n\n    index_bunny = 0:5:45 # just load 10 of the 50\n    nlight_bunny = length(index_bunny)\n    tmp = download(url0)\n    x = npzread(tmp)\n\n    dim = size(x)[1:2]\n    images_bunny = zeros(Float32, dim..., nlight_bunny)\n    images_bunny[:,:,1] = x[:,:,1]'\n\n    for (iz, index) in enumerate(index_bunny[2:end])\n        id3 = @sprintf(\"%03d\", index)\n        @show id3\n        url1 = replace(url0, \"000\" => id3)\n        xtmp = npzread(download(url1))\n        images_bunny[:,:,iz+1] = xtmp[:,:,1]'\n    end\n    images_bunny ./= maximum(images_bunny) # normalize\nend\npb = jim(images_bunny; title=\"Images for different lighting directions\")\n\nFor reference, here are the ground truth lighting directions.\n\nif !@isdefined(gt_lights)\n    url = \"https://github.com/yasumat/RobustPhotometricStereo/raw/master/data/bunny/lights.npy\"\n    gt_lights = npzread(download(url))\n    gt_lights = gt_lights'[:,index_bunny .+ 1]\nend\npl_gtb = scatter(eachrow(gt_lights)...,\n   xaxis = (L\"x\", (-0.8, 0.8), (-1:1)*0.8),\n   yaxis = (L\"y\", (-0.8, 0.8), (-1:1)*0.8),\n   zaxis = (L\"z\", (0.4, 1.0), [0.4, 0.69, 0.96]),\n   title = \"True lighting directions\",\n)\n\nprompt()\n\nIf needed, here is the url for the mask:\n\nhttps://raw.githubusercontent.com/yasumat/RobustPhotometricStereo/master/data/bunny/mask.png","category":"section"},{"location":"generated/demos/07/photometric3/#Reproducibility","page":"Photometric stereo","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/demos/12/complete1/#rmt-complete1","page":"RMT and matrix completion","title":"RMT and matrix completion","text":"This example examines noisy matrix completion (estimating a low-rank matrix from noisy data with missing measurements) through the lens of random matrix theory, using the Julia language.\n\nThis page comes from a single Julia file: complete1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: complete1.ipynb, or open it in binder here: complete1.ipynb.\n\nAdd the Julia packages that are need for this demo. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"LaTeXStrings\"\n        \"LinearAlgebra\"\n        \"MIRTjim\"\n        \"Plots\"\n        \"Random\"\n        \"StatsBase\"\n    ])\nend\n\nTell Julia to use the following packages for this example. Run Pkg.add() in the preceding code block first, if needed.\n\nusing InteractiveUtils: versioninfo\nusing LaTeXStrings\nusing LinearAlgebra: Diagonal, dot, norm, rank, svd, svdvals\nusing MIRTjim: prompt, jim\nusing Plots: default, gui, plot, plot!, scatter!, savefig\nusing Plots.PlotMeasures: px\nusing Random: seed!\nusing StatsBase: mean, var\ndefault(markerstrokecolor=:auto, label=\"\", widen=true, markersize = 6,\n labelfontsize = 24, legendfontsize = 18, tickfontsize = 14, linewidth = 3,\n)\nseed!(0)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/demos/12/complete1/#Helper-functions","page":"RMT and matrix completion","title":"Helper functions","text":"Generate random data for one trial:\n\nfunction gen1(\n    Î¸::Real = 3,\n    M::Int = 100,\n    N::Int = 2M,\n    p_obs::Real = 1, # probability an element is observed\n    T::Type{<:Real} = Float32,\n)\n    mask = rand(M, N) .<= p_obs\n    u = rand((-1,+1), M) / T(sqrt(M)) # Bernoulli just for variety\n    v = rand((-1,+1), N) / T(sqrt(N))\n    # u = randn(T, M) / T(sqrt(M))\n    # v = randn(T, N) / T(sqrt(N))\n    X = Î¸ * u * v' # theoretically rank-1 matrix\n    Z = randn(T, M, N) / T(sqrt(N)) # gaussian noise\n    Y = mask .* (X + Z) # missing entries set to zero\n    return Y, u, v, Î¸, p_obs\nend;\nnothing #hide\n\nSVD results for 1 trial:\n\nfunction trial1(args...)\n    Y, u, v, Î¸, p_obs = gen1(args...)\n    fac = svd(Y)\n    Ïƒ1 = fac.S[1]\n    u1 = fac.U[:,1]\n    v1 = fac.Vt[1,:]\n    return [Ïƒ1, abs2(dot(u1, u)), abs2(dot(v1, v))]\nend;\nnothing #hide\n\nAverage nrep trials:\n\ntrial2(nrep::Int, args...) = mean((_) -> trial1(args...), 1:nrep);\nnothing #hide\n\nSVD for each of multiple trials, for different SNRs and matrix sizes:\n\nif !@isdefined(vgrid)\n\n    # Simulation parameters\n    T = Float32\n    p_obs = 0.49\n    Mlist = [30, 300]\n    Î¸max = 4\n    nÎ¸ = Î¸max * 4 + 1\n    nrep = 100\n    Î¸list = T.(range(0, Î¸max, nÎ¸));\n    labels = map(n -> latexstring(\"\\$M = $n\\$\"), Mlist)\n\n    c = 0.7 # non-square matrix to test\n    c4 = c^0.25\n    tmp = ((Î¸, M) -> trial2(nrep, Î¸, M, ceil(Int, M/c) #= N =#, p_obs)).(Î¸list, Mlist')\n    Ïƒgrid = map(x -> x[1], tmp)\n    ugrid = map(x -> x[2], tmp)\n    vgrid = map(x -> x[3], tmp)\nend;\nnothing #hide","category":"section"},{"location":"generated/demos/12/complete1/#Results","page":"RMT and matrix completion","title":"Results","text":"Compare theory predictions and empirical results. There is again notable agreement between theory and empirical results here.\n\nÏƒ1 plot\n\ncolors = [:orange, :red]\nÎ¸fine = range(0, 2Î¸max, 60Î¸max + 1)\nÎ¸mod = Î¸fine .* sqrt(p_obs) # key modification from RMT!\nsbg(Î¸) = Î¸ > c4 ? sqrt((1 + Î¸^2) * (c + Î¸^2)) / Î¸ : 1 + âˆš(c)\nstheory = sbg.(Î¸mod) * sqrt(p_obs) # note modification!\nbm = s -> \"\\\\mathbf{\\\\mathit{$s}}\"\nylabel = latexstring(\"\\$Ïƒ_1($(bm(:Y)))\\$ (Avg)\")\nps = plot(Î¸fine, stheory, color=:blue, label=\"theory\",\n    aspect_ratio = 1, legend = :topleft,\n    xaxis = (L\"Î¸\", (0,Î¸max), 0:Î¸max),\n    yaxis = (ylabel, (1,Î¸max), 1:Î¸max),\n    annotate = (3.1, 3.6, latexstring(\"c = $c\"), :left),\n)\nscatter!(Î¸list, Ïƒgrid[:,1], marker=:square, color=colors[1], label = labels[1])\nscatter!(Î¸list, Ïƒgrid[:,2], marker=:circle, color=colors[2], label = labels[2])\nplot!(Î¸list, Î¸list * p_obs, label=L\"p \\; Î¸\", color=:black, linewidth=2,\n    annotate = (3.1, 3.3, latexstring(\"p = $p_obs\"), :left))\n\nprompt()\n\nu1 plot\n\nubg(Î¸) = (Î¸ > c4) ? 1 - c * (1 + Î¸^2) / (Î¸^2 * (Î¸^2 + c)) : 0\nutheory = ubg.(Î¸mod)\nylabel = latexstring(\"\\$|âŸ¨\\\\hat{$(bm(:u))}, $(bm(:u))âŸ©|^2\\$ (Avg)\")\npu = plot(Î¸fine, utheory, color=:blue, label=\"theory\",\n    left_margin = 10px, legend = :bottomright,\n    xaxis = (L\"Î¸\", (0,Î¸max), 0:Î¸max),\n    yaxis = (ylabel, (0,1), 0:0.5:1),\n)\nscatter!(Î¸list, ugrid[:,1], marker=:square, color=colors[1], label = labels[1])\nscatter!(Î¸list, ugrid[:,2], marker=:circle, color=colors[2], label = labels[2])\n\nprompt()\n\nv1 plot\n\nvbg(Î¸) = (Î¸ > c^0.25) ? 1 - (c + Î¸^2) / (Î¸^2 * (Î¸^2 + 1)) : 0\nvtheory = vbg.(Î¸mod)\nylabel = latexstring(\"\\$|âŸ¨\\\\hat{$(bm(:v))}, $(bm(:v))âŸ©|^2\\$ (Avg)\")\npv = plot(Î¸fine, vtheory, color=:blue, label=\"theory\",\n    left_margin = 10px, legend = :bottomright,\n    xaxis = (L\"Î¸\", (0,Î¸max), 0:Î¸max),\n    yaxis = (ylabel, (0,1), 0:0.5:1),\n)\nscatter!(Î¸list, vgrid[:,1], marker=:square, color=colors[1], label = labels[1])\nscatter!(Î¸list, vgrid[:,2], marker=:circle, color=colors[2], label = labels[2])\n\nprompt()\n\n\nif false\n savefig(ps, \"complete1-s.pdf\")\n savefig(pu, \"complete1-u.pdf\")\n savefig(pv, \"complete1-v.pdf\")\n pp = plot(ps, pu, pv; layout=(3,1), size=(600, 900))\nend","category":"section"},{"location":"generated/demos/12/complete1/#Image-example","page":"RMT and matrix completion","title":"Image example","text":"Apply an SVD-based matrix completion approach to some noisy and incomplete image data.","category":"section"},{"location":"generated/demos/12/complete1/#Latent-matrix","page":"RMT and matrix completion","title":"Latent matrix","text":"Make a matrix that has low rank:\n\ntmp = [\n    zeros(1,20);\n    0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0;\n    0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0;\n    0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0;\n    0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0;\n    zeros(1,20)\n]';\nrank(tmp)\n\nTurn it into an image:\n\nXtrue = kron(10 .+ 80*tmp, ones(9,9))\nrtrue = rank(Xtrue)\n\nplots with consistent size\n\njim1 = (X ; kwargs...) -> jim(X; size = (700,300),\n leftmargin = 10px, rightmargin = 10px, kwargs...);\nnothing #hide\n\nand consistent display range\n\njimc = (X ; kwargs...) -> jim1(X; clim=(0,99), kwargs...);\nnothing #hide\n\nand with NRMSE label\n\nnrmse = (Xh) -> round(norm(Xh - Xtrue) / norm(Xtrue) * 100, digits=1)\nargs = (xaxis = false, yaxis = false, colorbar = :none) # book\nargs = (;) # web\njime = (X; kwargs...) -> jimc(X; xlabel = \"NRMSE = $(nrmse(X)) %\",\n args..., kwargs...,\n)\ntitle = latexstring(\"\\$$(bm(:X))\\$ : Latent image\")\npt = jimc(Xtrue; title, xlabel = \" \", args...)","category":"section"},{"location":"generated/demos/12/complete1/#Noisy-/-incomplete-data","page":"RMT and matrix completion","title":"Noisy / incomplete data","text":"seed!(0)\np_see = 0.8\nmask = rand(Float32, size(Xtrue)) .<= p_see\nsigZ = 6\nMx,Nx = sort(collect(size(Xtrue)))\nZ = sigZ * randn(size(Xtrue)) # AWGN\nY = mask .* (Xtrue + Z);\n\ntitle = latexstring(\"\\$$(bm(:Y))\\$ : Corrupted image matrix\\n(missing pixels set to 0)\")\npy = jime(Y ; title)\n\nShow mask; count proportion of missing entries\n\nfrac_nonzero = count(mask) / length(mask)\ntitle = latexstring(\"\\$$(bm(:M))\\$ : Locations of observed entries\")\npm = jim1(mask; title, args...,\n    xlabel = \"sampled fraction = $(round(frac_nonzero * 100, digits=1))%\")","category":"section"},{"location":"generated/demos/12/complete1/#Singular-values.","page":"RMT and matrix completion","title":"Singular values.","text":"The first 3 singular values of Y are well above the \"noise floor\" caused by masking, but, relative to those of X they are scaled down by a factor of p as expected.\n\nWe also show the critical value of Ïƒ where the phase transition occurs. Ïƒâ‚„(X) is just barely above the threshold, and Ïƒâ‚…(X) below the threshold, so we cannot expect a simple SVD approach to recover them.\n\nc_4 = (Mx / Nx)^(1/4)\nÏƒcrit = sigZ^2 * sqrt(Nx) * c_4 / sqrt(p_see) # from RMT\n\npg = plot([1, Nx], [1, 1] * Ïƒcrit, color=:cyan,\n title=\"singular values\",\n xaxis=(L\"k\", (1, Mx), [1, 3, 6, Mx]),\n yaxis=(L\"Ïƒ_k\",),\n leftmargin = 15px, bottommargin = 20px, size = (600,350), widen = true,\n)\nsv_x = svdvals(Xtrue)\nsv_y = svdvals(Y)\nscatter!(pg, sv_x, color=:blue, label=\"Xtrue\", marker=:utriangle)\nscatter!(pg, sv_y, color=:red, label=\"Y (data)\", marker=:dtriangle)\nscatter!(pg, sv_y[1:3] / p_see, color=:green, label=\"Y/p\", marker=:hex, alpha=0.8)\n\nprompt()","category":"section"},{"location":"generated/demos/12/complete1/#Low-rank-estimate","page":"RMT and matrix completion","title":"Low-rank estimate","text":"A simple low-rank estimate of X from the first few SVD components of Y works just so-so here. A simple SVD approach recovers the first 3 components well, but cannot estimate the 4th and 5th components.\n\nr = 3\nU,s,V = svd(Y)\ns ./= p_see # correction for masking effect\nXr = U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]'\ntitle = latexstring(\"Rank $r approximation of data \\$$(bm(:Y))\\$\")\npr = jime(Xr ; title)\n\nHow well do the singular vectors match? The first 3 components match quite well:\n\n[sum(svd(Xr).U[:,1:r] .* svd(Xtrue).U[:,1:r], dims=1).^2;\n sum(svd(Xr).V[:,1:r] .* svd(Xtrue).V[:,1:r], dims=1).^2]\n\nThe next 2 components match very poorly, as predicted:\n\n[sum(svd(Y).U[:,4:5]/p_see .* svd(Xtrue).U[:,4:5], dims=1).^2;\n sum(svd(Y).V[:,4:5]/p_see .* svd(Xtrue).V[:,4:5], dims=1).^2]","category":"section"},{"location":"generated/demos/12/complete1/#Reproducibility","page":"RMT and matrix completion","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
