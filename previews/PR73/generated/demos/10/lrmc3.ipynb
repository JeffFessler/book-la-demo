{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Low-rank matrix completion: ADMM\n",
    "\n",
    "This example illustrates\n",
    "low-rank matrix completion\n",
    "using the Julia language.\n",
    "\n",
    "History:\n",
    "- 2017-11-07 Greg Ongie, University of Michigan, original version\n",
    "- 2017-11-12 Jeff Fessler, minor modifications\n",
    "- 2021-08-23 Julia 1.6.2\n",
    "- 2023-04-11 Literate version for Julia 1.8"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Add the Julia packages used in this demo.\n",
    "Change `false` to `true` in the following code block\n",
    "if you are using any of the following packages for the first time."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if false\n",
    "    import Pkg\n",
    "    Pkg.add([\n",
    "        \"DelimitedFiles\"\n",
    "        \"Downloads\"\n",
    "        \"InteractiveUtils\"\n",
    "        \"LaTeXStrings\"\n",
    "        \"LinearAlgebra\"\n",
    "        \"MIRT\"\n",
    "        \"MIRTjim\"\n",
    "        \"Plots\"\n",
    "    ])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tell Julia to use the following packages.\n",
    "Run `Pkg.add()` in the preceding code block first, if needed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using DelimitedFiles: readdlm\n",
    "using Downloads: download\n",
    "using InteractiveUtils: versioninfo\n",
    "using LaTeXStrings\n",
    "using LinearAlgebra: svd, svdvals, Diagonal, norm\n",
    "using MIRT: pogm_restart\n",
    "using MIRTjim: jim, prompt\n",
    "using Plots: plot, scatter, scatter!, savefig, default\n",
    "default(); default(markerstrokecolor=:auto, label = \"\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line is helpful when running this jl-file as a script;\n",
    "this way it will prompt user to hit a key after each image is displayed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isinteractive() && prompt(:prompt);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `TOP SECRET`\n",
    "\n",
    "On 2017-11-06 10:34:12 GMT,\n",
    "Agent 556 obtained a photo of the illegal arms dealer,\n",
    "code name **Professor X-Ray**.\n",
    "However, Agent 556 spilled soup on their lapel-camera,\n",
    "shorting out several CCD sensors.\n",
    "The image matrix has several missing entries;\n",
    "we were only able to recover 25% of data!\n",
    "\n",
    "Agent 551: Your mission, should you choose to accept it,\n",
    "is to recover the missing entries and uncover the true identity\n",
    "of **Professor X-Ray**.\n",
    "\n",
    "Read in data with missing pixels set to zero"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(Y)\n",
    "    tmp = homedir() * \"/web/course/551/julia/demo/professorxray.txt\" # jf\n",
    "    if !isfile(tmp)\n",
    "        url = \"https://github.com/JeffFessler/book-la-demo/raw/refs/heads/main/data/professorxray.txt\"\n",
    "        tmp = download(url)\n",
    "    end\n",
    "    Y = collect(readdlm(tmp)')\n",
    "    py = jim(Y, \"Y: Corrupted image matrix of Professor X-Ray\\n (missing pixels set to 0)\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create binary mask $Ω$ (true=observed, false=unobserved)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Ω = Y .!= 0\n",
    "percent_nonzero = sum(Ω) / length(Ω) # count proportion of missing entries"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show mask"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pm = jim(Ω, \"Ω: Locations of observed entries\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Low-rank approximation\n",
    "A simple low-rank approximation works poorly\n",
    "for this much missing data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = 20\n",
    "U,s,V = svd(Y)\n",
    "Xr = U[:,1:r] * Diagonal(s[1:r]) * V[:,1:r]'\n",
    "pr = jim(Xr, \"Low-rank approximation for r=$r\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Low-rank matrix completion\n",
    "\n",
    "Instead, we will try to uncover the identity of\n",
    "**Professor X-Ray** using\n",
    "low-rank matrix completion.\n",
    "\n",
    "The optimization problem we will solve is:\n",
    "$$\n",
    "\\min_{\\mathbf X}\n",
    "\\frac{1}{2} ‖ P_Ω(\\mathbf X) - P_Ω(\\mathbf Y) ‖_2^2\n",
    "+ β ‖ \\mathbf X ‖_*\n",
    "\\quad\\quad\\text{(NN-min)}\n",
    "$$\n",
    "where $\\mathbf Y$ is the zero-filled input data matrix,\n",
    "and $P_Ω$ is the operator\n",
    "that extracts a vector of entries belonging to the index set $Ω$.\n",
    "\n",
    "Define cost function for optimization problem:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nucnorm = (X) -> sum(svdvals(X))\n",
    "costfun = (X, beta) -> 0.5 * norm(X[Ω] - Y[Ω])^2 + beta * nucnorm(X);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define singular value soft thresholding (SVST) function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function SVST(X, beta)\n",
    "    U,s,V = svd(X)\n",
    "    sthresh = @. max(s - beta, 0)\n",
    "    return U * Diagonal(sthresh) * V'\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iterative Soft-Thresholding Algorithm (ISTA)\n",
    "\n",
    "ISTA is an extension of gradient descent to convex cost functions\n",
    "that look like\n",
    "$\\min_x f(x) + g(x)$\n",
    "where $f(x)$ is smooth and $g(x)$ is non-smooth.\n",
    "Also known as a\n",
    "[proximal gradient method](https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning).\n",
    "\n",
    "**ISTA algorithm for solving (NN-min):**\n",
    "- initialize $\\mathbf X_0 = \\mathbf Y$ (zero-fill missing entries)\n",
    "- `for` $k=0,1,2,…$\n",
    "  - $[\\hat{\\mathbf X}_k]_{i,j} = \\begin{cases}[\\mathbf X_k]_{i,j} & (i,j) ∉ Ω \\\\ [\\mathbf Y]_{i,j} & (i,j) ∈ Ω \\end{cases}$\n",
    "    (Put back in known entries)\n",
    "\n",
    "  - $\\mathbf X_{k+1} = \\text{SVST}(\\hat{\\mathbf X}_k,β)$\n",
    "    (Singular value soft-thresholding)\n",
    "- `end`\n",
    "\n",
    "Apply ISTA:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "niter = 400\n",
    "beta = 0.01 # chosen by trial-and-error here\n",
    "function lrmc_ista(Y)\n",
    "    X = copy(Y)\n",
    "    Xold = copy(X)\n",
    "    cost_ista = zeros(niter+1)\n",
    "    cost_ista[1] = costfun(X,beta)\n",
    "    for k in 1:niter\n",
    "        X[Ω] = Y[Ω]\n",
    "        X = SVST(X,beta)\n",
    "        cost_ista[k+1] = costfun(X,beta)\n",
    "    end\n",
    "    return X, cost_ista\n",
    "end;\n",
    "\n",
    "if !@isdefined(Xista)\n",
    "    Xista, cost_ista = lrmc_ista(Y)\n",
    "    pj_ista = jim(Xista, \"ISTA result at $niter iterations\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "What went wrong? Let's investigate.\n",
    "First, let's see if the above solution is actually low-rank."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "s_ista = svdvals(Xista)\n",
    "s0 = svdvals(Y)\n",
    "plot(title = \"singular values\",\n",
    "    xtick = [1, sum(s .> 20*eps()), minimum(size(Y))])\n",
    "scatter!(s0, color=:black, label=\"Y (initialization)\")\n",
    "scatter!(s_ista, color=:red, label=\"X (ISTA)\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's check the cost function descent:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "scatter(cost_ista, color=:red,\n",
    "    title = \"cost vs. iteration\",\n",
    "    xlabel = \"iteration\",\n",
    "    ylabel = \"cost function value\",\n",
    "    label = \"ISTA\",\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fast Iterative Soft-Thresholding Algorithm (FISTA)\n",
    "\n",
    "Modification of ISTA\n",
    "that includes Nesterov acceleration for faster convergence.\n",
    "\n",
    "Reference:\n",
    "- Beck, A. and Teboulle, M., 2009.\n",
    "  [A fast iterative shrinkage-thresholding algorithm for linear inverse problems.](https://doi.org/10.1137/080716542)\n",
    "  SIAM J. on Imaging Sciences, 2(1), pp.183-202.\n",
    "\n",
    "**FISTA algorithm for solving (NN-min)**\n",
    "- initialize matrices $\\mathbf Z_0 = \\mathbf X_0 = \\mathbf Y$\n",
    "- `for` $k=0,1,2,…$\n",
    "  - $[\\hat{\\mathbf Z}_k]_{i,j} = \\begin{cases}[\\mathbf Z_k]_{i,j} & (i,j) ∉ Ω \\\\ [\\mathbf Y]_{i,j} & (i,j) ∈ Ω \\end{cases}$\n",
    "    (Put back in known entries)\n",
    "\n",
    "  - $\\mathbf X_{k+1} = \\text{SVST}(\\hat{\\mathbf Z}_k,\\beta)$\n",
    "\n",
    "  - $t_{k+1} = \\frac{1 + \\sqrt{1+4t_k^2}}{2}$ (Nesterov step-size)\n",
    "\n",
    "  - $\\mathbf Z_{k+1} = \\mathbf X_{k+1} + \\frac{t_k-1}{t_{k+1}}(\\mathbf X_{k+1}-\\mathbf X_{k})$ (Momentum update)\n",
    "- `end`\n",
    "\n",
    "Run FISTA:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "niter = 200\n",
    "function lrmc_fista(Y)\n",
    "    X = copy(Y)\n",
    "    Z = copy(X)\n",
    "    Xold = copy(X)\n",
    "    told = 1\n",
    "    cost_fista = zeros(niter+1)\n",
    "    cost_fista[1] = costfun(X,beta)\n",
    "    for k in 1:niter\n",
    "        Z[Ω] = Y[Ω]\n",
    "        X = SVST(Z,beta)\n",
    "        t = (1 + sqrt(1+4*told^2))/2\n",
    "        Z = X + ((told-1)/t)*(X-Xold)\n",
    "        Xold = X\n",
    "        told = t\n",
    "        cost_fista[k+1] = costfun(X,beta) # comment out to speed-up\n",
    "    end\n",
    "    return X, cost_fista\n",
    "end;\n",
    "\n",
    "if !@isdefined(Xfista)\n",
    "    Xfista, cost_fista = lrmc_fista(Y)\n",
    "    pj_fista = jim(Xfista, \"FISTA result at $niter iterations\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(title = \"cost vs. iteration\",\n",
    "    xlabel=\"iteration\", ylabel = \"cost function value\")\n",
    "scatter!(cost_ista, label=\"ISTA\", color=:red)\n",
    "scatter!(cost_fista, label=\"FISTA\", color=:blue)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "See if the FISTA result is \"low rank\""
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "s_fista = svdvals(Xfista)\n",
    "effective_rank = count(>(0.01*s_fista[1]), s_fista)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = plot(title=\"singular values\",\n",
    "    xtick = [1, effective_rank, count(>(20*eps()), s_fista), minimum(size(Y))])\n",
    "scatter!(s0, label=\"Y (initial)\", color=:black)\n",
    "scatter!(s_fista, label=\"X (FISTA)\", color=:blue)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercise: think about why $σ_1(X) > σ_1(Y)$ !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alternating directions method of multipliers (ADMM)\n",
    "\n",
    "ADMM is another approach that uses SVST as a sub-routine,\n",
    "closely related to proximal gradient descent.\n",
    "\n",
    "It is faster than FISTA,\n",
    "but the algorithm requires a tuning parameter $μ$.\n",
    "(Here we use $μ = β$).\n",
    "\n",
    "References:\n",
    "- Cai, J.F., Candès, E.J. and Shen, Z., 2010.\n",
    "  [A singular value thresholding algorithm for matrix completion.](https://doi.org/10.1137/080738970)\n",
    "  SIAM J. Optimization, 20(4), pp. 1956-1982.\n",
    "- Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J., 2011.\n",
    "  [Distributed optimization and statistical learning via the alternating direction method of multipliers.](https://doi.org/10.1561/2200000016)\n",
    "  Foundations and Trends in Machine Learning, 3(1), pp. 1-122.\n",
    "\n",
    "Run alternating directions method of multipliers (ADMM) algorithm:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "niter = 50"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choice of parameter $μ$ can greatly affect convergence rate"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function lrmc_admm(Y; mu::Real = beta)\n",
    "    X = copy(Y)\n",
    "    Z = zeros(size(X))\n",
    "    L = zeros(size(X))\n",
    "    cost_admm = zeros(niter+1)\n",
    "    cost_admm[1] = costfun(X,beta)\n",
    "    for k in 1:niter\n",
    "        Z = SVST(X + L, beta / mu)\n",
    "        X = (Y + mu * (Z - L)) ./ (mu .+ Ω)\n",
    "        L = L + X - Z\n",
    "        cost_admm[k+1] = costfun(X,beta) # comment out to speed-up\n",
    "    end\n",
    "    return X, cost_admm\n",
    "end;\n",
    "\n",
    "if !@isdefined(Xadmm)\n",
    "    Xadmm, cost_admm = lrmc_admm(Y)\n",
    "    pj_admm = jim(Xadmm, \"ADMM result at $niter iterations\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pc = plot(title = \"cost vs. iteration\",\n",
    "    xtick = [0, 50, 200, 400],\n",
    "    xlabel = \"iteration\", ylabel = \"cost function value\")\n",
    "scatter!(0:400, cost_ista, label=\"ISTA\", color=:red)\n",
    "scatter!(0:200, cost_fista, label=\"FISTA\", color=:blue)\n",
    "scatter!(0:niter, cost_admm, label=\"ADMM\", color=:magenta)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "All singular values"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "s_admm = svdvals(Xadmm)\n",
    "scatter!(ps, s_admm, label=\"X (ADMM)\", color=:magenta, marker=:square)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a suitable choice of $μ$, ADMM converges faster than FISTA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proximal optimized gradient method (POGM)\n",
    "\n",
    "The\n",
    "[proximal optimized gradient method (POGM)](https://doi.org/10.1137/16m108104x)\n",
    "with\n",
    "[adaptive restart](https://doi.org/10.1007/s10957-018-1287-4)\n",
    "is faster than FISTA\n",
    "with very similar computation per iteration.\n",
    "Unlike ADMM,\n",
    "POGM does not require any algorithm tuning parameter $μ$,\n",
    "making it easier to use in many practical composite optimization problems."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(Xpogm)\n",
    "    Fcost = X -> costfun(X, beta)\n",
    "    f_grad = X -> Ω .* (X - Y) # gradient of smooth term\n",
    "    f_L = 1 # Lipschitz constant of f_grad\n",
    "    g_prox = (X, c) -> SVST(X, c * beta)\n",
    "    fun = (iter, xk, yk, is_restart) -> (xk, Fcost(xk), is_restart)\n",
    "    niter = 150\n",
    "    Xpogm, out = pogm_restart(Y, Fcost, f_grad, f_L; g_prox, fun, niter)\n",
    "    cost_pogm = [o[2] for o in out]\n",
    "    pj_pogm = jim(Xpogm, \"POGM result at $niter iterations\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "scatter!(pc, 0:niter, cost_pogm, label=\"POGM\", color=:green)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  },
  "kernelspec": {
   "name": "julia-1.12",
   "display_name": "Julia 1.12.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}