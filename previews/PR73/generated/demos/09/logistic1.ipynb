{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression\n",
    "\n",
    "Binary classification via logistic regression\n",
    "in Julia."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Add the Julia packages used in this demo.\n",
    "Change `false` to `true` in the following code block\n",
    "if you are using any of the following packages for the first time."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if false\n",
    "    import Pkg\n",
    "    Pkg.add([\n",
    "        \"InteractiveUtils\"\n",
    "        \"LaTeXStrings\"\n",
    "        \"LinearAlgebra\"\n",
    "        \"MIRTjim\"\n",
    "        \"Plots\"\n",
    "        \"Random\"\n",
    "        \"StatsBase\"\n",
    "    ])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tell Julia to use the following packages.\n",
    "Run `Pkg.add()` in the preceding code block first, if needed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using InteractiveUtils: versioninfo\n",
    "using LaTeXStrings\n",
    "using LinearAlgebra: dot, eigvals\n",
    "using MIRTjim: prompt\n",
    "using Plots: default, gui, savefig\n",
    "using Plots: plot, plot!, scatter, scatter!\n",
    "using Random: seed!\n",
    "using StatsBase: mean\n",
    "default(); default(markersize=6, linewidth=2, markerstrokecolor=:auto, label=\"\",\n",
    " tickfontsize=12, labelfontsize=18, legendfontsize=18, titlefontsize=18)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line is helpful when running this file as a script;\n",
    "this way it will prompt user to hit a key after each figure is displayed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isinteractive() ? prompt(:prompt) : prompt(:draw)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Generate synthetic data from two classes"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(yy)\n",
    "    seed!(0)\n",
    "    n0 = 60\n",
    "    n1 = 50\n",
    "    mu0 = [-1, 1]\n",
    "    mu1 = [1, -1]\n",
    "    v0 = mu0 .+ randn(2,n0) # class -1\n",
    "    v1 = mu1 .+ randn(2,n1) # class 1\n",
    "    nex = 0\n",
    "    if true # 2017-01-18\n",
    "        nex = 4 # extra dim (beyond the 2 shown) to make \"larger scale\"\n",
    "        v0 = [v0; rand(nex,n0)] # (2+nex, n0)\n",
    "        v1 = [v1; rand(nex,n1)] # (2+nex, n1)\n",
    "    end\n",
    "    M = n0 + n1 # how many samples\n",
    "    yy = [-ones(n0); ones(n1)] # (M) labels\n",
    "    vv = [[v0 v1]; ones(1,n0+n1)] # (npar, M) training data\n",
    "    npar = 3 + nex # unknown parameters\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "scatter plot and initial decision boundary"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(ps)\n",
    "    x0 = [-1; 3; rand(nex); 5]\n",
    "    v1p = range(-1,1,101) * 4\n",
    "    v2p_fun = x -> @. (-x[end] - x[1] * v1p) / x[2]\n",
    "\n",
    "    ps = plot(aspect_ratio = 1, size = (550, 500), legend=:topright,\n",
    "     xaxis = (L\"v_1\", (-4, 4), [-4 -1 0 1 4]),\n",
    "     yaxis = (L\"v_2\", (-4, 4), [-4 -1 0 1 4]),\n",
    "    )\n",
    "    plot!(v1p, v2p_fun(x0), color=:red, label=\"initial\")\n",
    "    plot!(v1p, v1p, color=:yellow, label=\"ideal\")\n",
    "    scatter!(v0[1,:], v0[2,:], color=:green, alpha=0.7)\n",
    "    scatter!(v1[1,:], v1[2,:], color=:blue, marker=:square, alpha=0.7)\n",
    "    # savefig(ps, \"demo_fgm1_ogm1_s0.pdf\")\n",
    "end\n",
    "plot(ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cost function\n",
    "\n",
    "Logistic regression with Tikhonov regularization:\n",
    "$$\n",
    "f(x) = 1_M' h.(A x) + β/2 ‖ x ‖_2^2\n",
    "$$\n",
    "where\n",
    "$h(z) = log(1 + e^{-z})$\n",
    "is the logistic loss function.\n",
    "\n",
    "Its gradient is\n",
    "$∇ f(x) = A' \\dot{h}.(A x) + β x$,\n",
    "and its Lipschitz constant\n",
    "is $‖A‖_2^2 / 4 + β$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(kost)\n",
    "    pot = (t) -> log(1 + exp(-t)) # logistic\n",
    "    dpot = (t) -> -1 / (exp(t) + 1)\n",
    "    tmp = vv * vv' # (npar, npar) covariance\n",
    "    tmp = eigvals(tmp)\n",
    "    @show maximum(tmp) / minimum(tmp)\n",
    "    pLip = maximum(tmp) / 4 # 1/4 comes from logistic curvature\n",
    "\n",
    "    reg = 2^0\n",
    "    Lip = pLip + reg # Lipschitz constant\n",
    "\n",
    "    A = yy .* vv'\n",
    "    gfun = x -> A' * dpot.(A * x) + reg * x # gradient\n",
    "    if false\n",
    "        tmp = gfun(x0)\n",
    "        @show size(tmp)\n",
    "    end\n",
    "\n",
    "    kost = x -> sum(pot, A * x, dims=1) .+ reg/2 * sum(abs2, x, dims=1)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GD\n",
    "Iterate GD"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tol = 1e-6\n",
    "tol_gd = tol\n",
    "tol_n1 = tol\n",
    "tol_o1 = tol\n",
    "\n",
    "function gd(x0, gfun::Function, niter::Int)\n",
    "    xg = copy(x0)\n",
    "    xgs = copy(xg)\n",
    "    for n in 1:niter\n",
    "        xold = xg\n",
    "        xg -= 1/Lip * gfun(xg)\n",
    "        if false && (norm(xg - xold, Inf) / norm(x0, Inf) < tol_gd)\n",
    "            @show n\n",
    "            break\n",
    "        end\n",
    "        any(isnan, xg) && throw(\"nan\")\n",
    "        # projected GD ??\n",
    "        # xg = xg / norm(xg) # decision boundary unaffected by norm!\n",
    "        xgs = [xgs xg] # archive\n",
    "    end\n",
    "    return xgs\n",
    "end\n",
    "\n",
    "if !@isdefined(xgs)\n",
    "    niter_gd = 300\n",
    "    xgs = gd(x0, gfun, niter_gd)\n",
    "    pgs = plot(xgs', xlabel=\"Iteration\", title = \"GD\")\n",
    "end\n",
    "plot(pgs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nesterov FGM"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "do_restart = true;\n",
    "\n",
    "function fgm(x0, grad, Lip::Real, niter::Int)\n",
    "    re_nest = Int[]\n",
    "    told = 1\n",
    "    x = copy(x0)\n",
    "    zold = copy(x0)\n",
    "    xns = copy(x)\n",
    "    for n in 1:niter\n",
    "        xold = copy(x)\n",
    "        grad = gfun(x)\n",
    "        znew = x - 1/Lip * grad\n",
    "        zdiff = znew - zold # dk - for restart\n",
    "\n",
    "        tnew = 1/2 * (1 + sqrt(1 + 4 * told^2))\n",
    "\n",
    "        x = znew + (told - 1) / tnew * (znew - zold)\n",
    "        zold = copy(znew)\n",
    "        told = tnew\n",
    "        if false && (norm(x - xold, Inf) / norm(x0, Inf) < tol_n1)\n",
    "            @show n\n",
    "            break\n",
    "        end\n",
    "        any(isnan, x) && throw(\"nan\")\n",
    "\n",
    "        if do_restart && (dot(grad, zdiff) > 0) # dk new version\n",
    "            told = 1\n",
    "            x = copy(znew) # check\n",
    "            zold = copy(x)\n",
    "            @show \"nest. restart\", n\n",
    "            push!(re_nest, n)\n",
    "        end\n",
    "        xns = [xns x] # archive\n",
    "    end\n",
    "    return xns, re_nest\n",
    "end\n",
    "\n",
    "if !@isdefined(xns)\n",
    "    niter_n1 = 300\n",
    "    niter_n1 = 500 # nex\n",
    "    xns, re_nest = fgm(x0, gfun, Lip, niter_n1)\n",
    "    pns = plot(xns', xlabel = \"Iteration\", title = \"FGM\")\n",
    "end\n",
    "plot(pns)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OGM\n",
    "Optimized gradient method"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function ogm(\n",
    "    x0,\n",
    "    gfun,\n",
    "    Lip::Real,\n",
    "    niter::Int,\n",
    ")\n",
    "    re_ogm1 = Int[]\n",
    "    told = 1\n",
    "    x = copy(x0)\n",
    "    zold = copy(x0)\n",
    "    x1s = copy(x)\n",
    "    for n in 1:niter\n",
    "        xold = x\n",
    "        grad = gfun(x)\n",
    "        znew = x - 1/Lip * grad\n",
    "\n",
    "        tnew = 1/2 * (1 + sqrt(1 + 4 * told^2))\n",
    "\n",
    "        x = znew + (told - 1) / tnew * (znew - zold) + told/tnew * (znew - xold)\n",
    "        zdiff = znew - zold # dk - for restart\n",
    "        zold = znew\n",
    "        told = tnew\n",
    "        if false && (norm(x - xold, Inf) / norm(x0, Inf) < tol_n1)\n",
    "            @show n\n",
    "            break\n",
    "        end\n",
    "        any(isnan, x) && throw(\"nan\")\n",
    "\n",
    "        if do_restart && (dot(grad, zdiff) > 0) # dk new version\n",
    "            push!(re_ogm1, n)\n",
    "            told = 1\n",
    "            x = znew # check\n",
    "            zold = x # dk fixed from x0\n",
    "            @info \"ogm1 restart $n\"\n",
    "        end\n",
    "        x1s = [x1s x] # archive\n",
    "    end\n",
    "    return x1s, re_ogm1\n",
    "end\n",
    "\n",
    "\n",
    "if !@isdefined(x1s)\n",
    "    niter_o1 = 300\n",
    "    niter_o1 = 500 # nex\n",
    "    x1s, re_ogm1 = ogm(x0, gfun, Lip, niter_o1)\n",
    "    po1 = plot(x1s', xlabel = \"Iteration\", title = \"OGM\")\n",
    "end\n",
    "plot(po1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "impartial version of x ͚"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xh_tmp = [xgs[:,end] xns[:,end] x1s[:,end]]\n",
    "xh = vec(mean(xh_tmp[:,2:3], dims=2)); # GD too slow to include"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "plot cost"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "extra = do_restart ? \" (restart)\" : \"\"\n",
    "pc = plot(xaxis=(\"iteration\", (0,10)), yaxis=(\"Cost function\",))\n",
    "plot!(0:niter_gd, vec(kost(xgs)) .- kost(xh), label = \"GD\" * extra)\n",
    "plot!(0:niter_n1, vec(kost(xns)) .- kost(xh), label = \"FGM\" * extra)\n",
    "plot!(0:niter_o1, vec(kost(x1s)) .- kost(xh), label = \"OGM1\" * extra)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot decision boundaries"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if true\n",
    "    psh = deepcopy(ps)\n",
    "    v2p = @. (-xh[end] - xh[1] * v1p) / xh[2]\n",
    "    plot!(psh, v1p, v2p, color = :magenta, label=\"final\")\n",
    "# savefig(psh, \"demo-fgm1-fgm1a.pdf\")\n",
    "end\n",
    "plot(psh)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot iterate convergence"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "efun1 = (x) -> vec(sqrt.(sum(abs2, x .- xh, dims=1)))\n",
    "efun = (x) -> do_restart ? log10.(efun1(x)) : efun1(x)\n",
    "ifun = (x) -> 0:(size(x,2)-1);\n",
    "\n",
    "pic = plot(\n",
    " xaxis = (\"Iteration\", (0, 40+10*nex), 0:20:80),\n",
    " yaxis = do_restart ?\n",
    "  (L\"\\log_{10}(‖ \\mathbf{x}_k - \\mathbf{x}_* ‖)\", (-3, 1), -3:1) :\n",
    "  (L\"‖ \\mathbf{x}_k - \\mathbf{x}_* ‖\", (0, 8), [-1, 0, 8]),\n",
    " legend = :topright,\n",
    ")\n",
    "plot!(ifun(xgs), efun(xgs), color=:green, label = \"GD\")\n",
    "plot!(ifun(xns), efun(xns), color=:blue, label = \"Nesterov FGM\" * extra)\n",
    "plot!(ifun(x1s), efun(x1s), color=:red, label = \"OGM1\" * extra)\n",
    "if do_restart\n",
    "    scatter!(re_nest, efun(xns[:, re_nest .+ 1]), color=:blue)\n",
    "    scatter!(re_ogm1, efun(x1s[:, re_ogm1 .+ 1]), color=:red)\n",
    "end\n",
    "plot(pic)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()\n",
    "\n",
    "# savefig demo_fgm1_ogm1c # restart\n",
    "# savefig demo_fgm1_ogm1b # no restart\n",
    "\n",
    "# todo: compare with LBFGS"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  },
  "kernelspec": {
   "name": "julia-1.12",
   "display_name": "Julia 1.12.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}