{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression - QN\n",
    "\n",
    "Binary classification via logistic regression\n",
    "using Quasi-Newton optimizer\n",
    "in Julia."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Add the Julia packages used in this demo.\n",
    "Change `false` to `true` in the following code block\n",
    "if you are using any of the following packages for the first time."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if false\n",
    "    import Pkg\n",
    "    Pkg.add([\n",
    "        \"InteractiveUtils\"\n",
    "        \"LaTeXStrings\"\n",
    "        \"LinearAlgebra\"\n",
    "        \"MIRTjim\"\n",
    "        \"Optim\"\n",
    "        \"Plots\"\n",
    "        \"Random\"\n",
    "        \"Statistics\"\n",
    "    ])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tell Julia to use the following packages.\n",
    "Run `Pkg.add()` in the preceding code block first, if needed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using InteractiveUtils: versioninfo\n",
    "using LaTeXStrings\n",
    "using LinearAlgebra: dot, eigvals\n",
    "using MIRTjim: prompt\n",
    "using Optim: optimize\n",
    "import Optim # Options\n",
    "using Plots: default, gui, savefig\n",
    "using Plots: histogram!, plot, plot!, scatter, scatter!\n",
    "using Random: seed!\n",
    "using Statistics: mean\n",
    "default(); default(markersize=6, linewidth=2, markerstrokecolor=:auto, label=\"\",\n",
    " tickfontsize=12, labelfontsize=18, legendfontsize=18, titlefontsize=18)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line is helpful when running this file as a script;\n",
    "this way it will prompt user to hit a key after each figure is displayed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isinteractive() ? prompt(:prompt) : prompt(:draw)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Generate synthetic data from two classes"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(yy)\n",
    "    seed!(0)\n",
    "    n0 = 60\n",
    "    n1 = 50\n",
    "    mu0 = [-1, 1]\n",
    "    mu1 = [1, -1]\n",
    "    v0 = mu0 .+ randn(2,n0) # class -1\n",
    "    v1 = mu1 .+ randn(2,n1) # class 1\n",
    "    nex = 0\n",
    "    if false\n",
    "        nex = 4 # extra dim (beyond the 2 shown) to make \"larger scale\"\n",
    "        v0 = [v0; rand(nex,n0)] # (2+nex, n0)\n",
    "        v1 = [v1; rand(nex,n1)] # (2+nex, n1)\n",
    "    end\n",
    "    M = n0 + n1 # how many samples\n",
    "    yy = [-ones(Int, n0); ones(Int, n1)] # (M) labels\n",
    "    vv = [[v0 v1]; ones(1,M)] # (npar, M) training data - with bias/offset\n",
    "    npar = 3 + nex # unknown parameters\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "scatter plot and initial decision boundary"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(ps)\n",
    "    x0 = [-1; 3; rand(nex); 5]\n",
    "    v1p = range(-1,1,101) * 4\n",
    "    v2p_fun = x -> @. (-x[end] - x[1] * v1p) / x[2]\n",
    "\n",
    "    ps = plot(aspect_ratio = 1, size = (550, 500), legend=:topright,\n",
    "     xaxis = (L\"v_1\", (-4, 4), [-4 -1 0 1 4]),\n",
    "     yaxis = (L\"v_2\", (-4, 4), [-4 -1 0 1 4]),\n",
    "    )\n",
    "    plot!(v1p, v2p_fun(x0), color=:red, label=\"initial\")\n",
    "    plot!(v1p, v1p, color=:yellow, label=\"ideal\")\n",
    "    scatter!(v0[1,:], v0[2,:], color=:green, alpha=0.7)\n",
    "    scatter!(v1[1,:], v1[2,:], color=:blue, marker=:square, alpha=0.7)\n",
    "end\n",
    "plot(ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cost function\n",
    "\n",
    "Logistic regression with Tikhonov regularization:\n",
    "$$\n",
    "f(x) = 1_M' h.(A x) + β/2 ‖ x ‖_2^2\n",
    "$$\n",
    "where\n",
    "$h(z) = log(1 + e^{-z})$\n",
    "is the logistic loss function.\n",
    "\n",
    "Its gradient is\n",
    "$∇ f(x) = A' \\dot{h}.(A x) + β x$,\n",
    "and its Lipschitz constant\n",
    "is $‖A‖_2^2 / 4 + β$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(cost)\n",
    "    pot(t) = log(1 + exp(-t)) # logistic\n",
    "    dpot(t) = -1 / (exp(t) + 1)\n",
    "    tmp = vv * vv' # (npar, npar) covariance\n",
    "    tmp = eigvals(tmp)\n",
    "    @show maximum(tmp) / minimum(tmp)\n",
    "    pLip = maximum(tmp) / 4 # 1/4 comes from logistic curvature\n",
    "\n",
    "    reg = 0 # no regularization because N ≪ M here\n",
    "    Lip = pLip + reg # Lipschitz constant\n",
    "\n",
    "    A = yy .* vv'\n",
    "    gfun = x -> A' * dpot.(A * x) + reg * x # gradient\n",
    "    if false\n",
    "        tmp = gfun(x0)\n",
    "        @show size(tmp)\n",
    "    end\n",
    "\n",
    "    cost(x::AbstractVector) = sum(pot, A * x) + reg/2 * sum(abs2, x)\n",
    "    cost(x::AbstractMatrix) = cost.(eachcol(x)) ## to handle arrays\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## L-BFGS optimizer"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "opt = Optim.Options(\n",
    " store_trace = true,\n",
    " show_warnings = false,\n",
    " extended_trace = true, # for trace of x\n",
    ")\n",
    "outq = optimize(cost, gfun, x0, opt; inplace=false)\n",
    "xqs = hcat(Optim.x_trace(outq)...)\n",
    "xq = outq.minimizer\n",
    "\n",
    "\n",
    "xh = xqs[:,end] # final estimate"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot cost"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ifun = xs -> 0:(size(xs,2)-1)\n",
    "pc = plot(xaxis=(\"iteration\", (0,16), 0:4:16), yaxis=(\"Cost function\",))\n",
    "plot!(ifun(xqs), cost(xqs) .- cost(xh), label = \"QN\", marker=:o)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot decision boundaries"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if true\n",
    "    psh = deepcopy(ps)\n",
    "    v2p = @. (-xh[end] - xh[1] * v1p) / xh[2]\n",
    "    plot!(psh, v1p, v2p, color = :magenta, label=\"final\")\n",
    "end\n",
    "plot(psh)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot iterate convergence"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "efun1 = (x) -> vec(sqrt.(sum(abs2, x .- xh, dims=1)))\n",
    "efun = (x) -> log10.(efun1(x))\n",
    "pic = plot(\n",
    "xaxis = (\"Iteration\", (0, 16), 0:2:16),\n",
    " yaxis = (L\"\\log_{10}(‖ \\mathbf{x}_k - \\mathbf{x}_* ‖)\", (-9, 3), -9:3),\n",
    " legend = :topright,\n",
    ")\n",
    "plot!(ifun(xqs), efun(xqs), label = \"QN\", marker = :o)\n",
    "plot(pic)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot 1D separation"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "inprod0 = [v0; ones(1,n0)]' * xh\n",
    "inprod1 = [v1; ones(1,n1)]' * xh\n",
    "\n",
    "accuracy0 = round(count(<(0), inprod0) / n0 * 100, digits=1)\n",
    "accuracy1 = round(count(>(0), inprod1) / n1 * 100, digits=1)\n",
    "\n",
    "plot(xaxis=(\"⟨x,v⟩\",))\n",
    "bins = -15:15\n",
    "histogram!(inprod0, alpha=0.5; bins, color=:green, linecolor = :green,\n",
    " label=\"class 0: $accuracy0%\")\n",
    "histogram!(inprod1, alpha=0.5; bins, color=:blue, linecolor = :blue,\n",
    " label=\"class 1: $accuracy1%\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method\n",
    "Stand-alone function for (regularized) logistic regression"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "   xh = logistic(data, label, reg)\n",
    "\n",
    "Perform regularized logistic regression for binary `label`s\n",
    "by minimizing\n",
    "$f(x) = 1_M' h.(A x) + β/2 ‖ x ‖_2^2$\n",
    "where\n",
    "$h(z) = log(1 + e^{-z})$\n",
    "is the logistic loss function.\n",
    "\n",
    "In:\n",
    "- `data` `N × M` where `N` is number of features (including offset)\n",
    "- `label` vector of `M` labels ±1\n",
    "- `reg` regularization parameter\n",
    "\n",
    "Out:\n",
    "- `xh` minimizer of $f$\n",
    "\"\"\"\n",
    "function logistic(data::AbstractMatrix, labels::AbstractVector, reg::Real)\n",
    "    any(x -> ∉(x, (-1,1)), labels) && throw(\"labels must be ±1\")\n",
    "    pot(t) = log(1 + exp(-t)) # logistic\n",
    "    dpot(t) = -1 / (exp(t) + 1) # derivative\n",
    "    tmp = data * data' # (N, N) covariance\n",
    "    tmp = eigvals(tmp)\n",
    "    pLip = maximum(tmp) / 4 # 1/4 comes from logistic curvature\n",
    "    Lip = pLip + reg # Lipschitz constant\n",
    "\n",
    "    A = labels .* data'\n",
    "    cost(x) = sum(pot, A * x) + reg/2 * sum(abs2, x)\n",
    "    gfun(x) = A' * dpot.(A * x) + reg * x # gradient\n",
    "\n",
    "    x0 = zeros(size(data,1))\n",
    "    outq = optimize(cost, gfun, x0; inplace=false)\n",
    "    return outq.minimizer\n",
    "end;\n",
    "\n",
    "xl = logistic(vv, yy, reg)\n",
    "@assert xl ≈ xh"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  },
  "kernelspec": {
   "name": "julia-1.12",
   "display_name": "Julia 1.12.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}